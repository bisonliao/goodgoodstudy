# 利用深度学习进行图片相似性比较

## 1、问题提出

有些场景需要对两张图片做相似性比较：

1. 人脸验证：输入人脸图片和人的ID，输出图片和ID是否匹配，实际上是比对数据库里的该ID的人脸图片和输入的图片是否相似。相比人脸识别的优势：
   1. 新加入人ID，不需要重新训练模型；
   2. 因为是1:1的人脸识别，不是1：n的，对准确率要求比较低；
   3. 对训练数据的数量要求较少，1两张图片就可以满足
2. 图片/视频的指纹库，防盗版。用于两张图片相似性比较的孪生网络，其隐含层包含了图片的特征摘要，利用这些摘要做倒排，可以建立图片、视频版权库，通过搜索比对特征摘要，达到防盗版、视频片段检索的目的。



一种常见的解决方案是孪生网络（siamese network）。之所以叫孪生，因为网络由多个类似的小网络组成，他们之间共享相同的参数。

孪生网络主要有两种形态：

1. 两输入的孪生网络：输入两张图片，生成特征摘要，特征摘要的距离大小满足对图片相似度的分类：相似or不相似
2. 三输入的三胞胎网络（triplet network）：输入三张图片，分别是anchor（参照图）、positive（与参照图相似的图）、negtive（与参照图不相似的图），输出对应的三个特征摘要f(A)/f(P)/f(N)，满足distance(f(A), f(P)) 比distance(f(A), f(N)) 小一定的阈值α。

还是借用吴恩达老师的图来示意：

![](img/siamese_net/siamese1.jpg)

## 2、试验一：手写数字比对

caffe源代码的目录下，有个examples/siamese例子，可以对手写数字进行两两比对，判断他们是不是同一个数字。

网络拓扑如下：

![](img/siamese_net/siamese2.jpg)

这个网络有如下特点：

1. 数据输入方面，把两张图片合并为一张2通道的图片，每张图片各使用一个通道。label还是一个整数（1表示相似，0表示不相似），存储在Datum这个message的label字段里。经过slice层将两个通道切割开，即slice层输入的blob是[batchsz,  2, 28, 28]，输入是两个blob：[batchsz,  1, 28,  28]
2. 参数共享：通过相同的参数名实现对应的层共享，例如conv2和conv2_p是共享参数的
3. 最后的特征摘要层是feat和feat_p，blob大小为[batchsz,  2]，即两个浮点数。
4. 损失函数层，我第一次见到：ContrastiveLoss类型

### 2.1 关于ContrastiveLoss

根据网友的blog：

![](img/siamese_net/siamese3.jpg)

下图是距离d和损失函数的关系，比较直观好理解：

![](img/siamese_net/siamese4.jpg)

### 2.2 训练与测试结果

数据用caffe提供的脚本，比较方便就准备好了。然后开始训练：

```
E:\DeepLearning\siamese\siamese>caffe.exe train --solver=.\mnist_siamese_solver.prototxt

I0430 20:44:57.489703 12096 solver.cpp:399]     Test net output #0: loss = 0.209038 (* 1 = 0.209038 loss)
I0430 20:44:57.524310 12096 solver.cpp:220] Iteration 0 (-2.00567e-35 iter/s, 0.684607s/1000 iters), loss = 0.234112
I0430 20:44:57.525387 12096 solver.cpp:239]     Train net output #0: loss = 0.234112 (* 1 = 0.234112 loss)
I0430 20:44:57.525387 12096 sgd_solver.cpp:105] Iteration 0, lr = 0.01, solver type:SGD
I0430 20:45:00.639927 12096 solver.cpp:332] Iteration 500, Testing net (#0)
I0430 20:45:00.809927 18424 data_layer.cpp:73] Restarting data prefetching from start.
I0430 20:45:00.819928 12096 solver.cpp:399]     Test net output #0: loss = 0.0346499 (* 1 = 0.0346499 loss)
I0430 20:45:03.242072  6792 data_layer.cpp:73] Restarting data prefetching from start.
I0430 20:45:03.555248 12096 solver.cpp:332] Iteration 1000, Testing net (#0)
I0430 20:45:03.727337 18424 data_layer.cpp:73] Restarting data prefetching from start.
I0430 20:45:03.727337 12096 solver.cpp:399]     Test net output #0: loss = 0.0273838 (* 1 = 0.0273838 loss)
I0430 20:45:03.737339 12096 solver.cpp:220] Iteration 1000 (161.173 iter/s, 6.20451s/1000 iters), loss = 0.0230628
I0430 20:45:03.737339 12096 solver.cpp:239]     Train net output #0: loss = 0.0230628 (* 1 = 0.0230628 loss)
...
I0430 20:49:16.570897 18424 data_layer.cpp:73] Restarting data prefetching from start.
I0430 20:49:16.580899 12096 solver.cpp:399]     Test net output #0: loss = 0.0188294 (* 1 = 0.0188294 loss)
I0430 20:49:17.464105  6792 data_layer.cpp:73] Restarting data prefetching from start.
I0430 20:49:18.990659 12096 solver.cpp:449] Snapshotting to binary proto file snapshot/mnist_siamese_iter_50000.caffemodel
I0430 20:49:19.010659 12096 sgd_solver.cpp:274] Snapshotting solver state to binary proto file snapshot/mnist_siamese_iter_50000.solverstate
I0430 20:49:19.020689 12096 solver.cpp:312] Iteration 50000, loss = 0.000393342
I0430 20:49:19.020689 12096 solver.cpp:332] Iteration 50000, Testing net (#0)
I0430 20:49:19.195806 18424 data_layer.cpp:73] Restarting data prefetching from start.
I0430 20:49:19.205806 12096 solver.cpp:399]     Test net output #0: loss = 0.0191147 (* 1 = 0.0191147 loss)
I0430 20:49:19.205806 12096 solver.cpp:317] Optimization Done.
I0430 20:49:19.205806 12096 caffe.cpp:260] Optimization Done.
```
从训练环节来看，损失函数收敛不错，但从测试环节来看，损失函数收敛的不是很好。

写个代码调用模型来感受一下：

![](img/siamese_net/siamese5.jpg)

直观上感觉模型的准确率不太高。

测试1万对图片，量化的准确率为78.34%。

注意，deploy.prototxt的最后输出层是两个特征摘要的欧式距离：

```
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "feat"
  bottom: "feat_p"
  top: "loss"
}
```

[详细的代码见这里](code/siamese_net/UseTrainedModel.cpp)

[网络定义文件在这里](code/siamese_net/)

摘取部分代码如下：
```c
int classify(boost::shared_ptr<Net<float> > net, const Mat & img1, const Mat & img2)
{
	static float data_input[2][input_size][input_size];
	Mat image1, image2;
	
	if (img1.channels() != 1)
	{
		cvtColor(img1, image1, CV_BGR2GRAY);
	}
	else
	{
		image1 = img1;
	}
	if (img2.channels() != 1)
	{
		cvtColor(img2, image2, CV_BGR2GRAY);
	}
	else
	{
		image2 = img2;
	}
	Mat resized = image1.clone();
	cv::resize(resized, image1, cv::Size(input_size, input_size));
	resized = image2.clone();
	cv::resize(resized, image2, cv::Size(input_size, input_size));

	int width, height, chn;
	for (height = 0; height < input_size; ++height)
	{
		for (width = 0; width < input_size; ++width)
		{
			uchar v = image1.at<uchar>(height, width);
			data_input[0][height][width] = v / 256.0;
	
			v = image2.at<uchar>(height, width);
			data_input[1][height][width] = v / 256.0;
	
		}
	}
	
	Blob<float>* input_blobs = net->input_blobs()[0];
	//printf("inpupt blob x count:%d\n", input_blobs->count());
	switch (Caffe::mode())
	{
	case Caffe::CPU:
		memcpy(input_blobs->mutable_cpu_data(), data_input,
			sizeof(float) * input_blobs->count());
		break;
	case Caffe::GPU:
	
		cudaMemcpy(input_blobs->mutable_gpu_data(), data_input,
			sizeof(float) * input_blobs->count(), cudaMemcpyHostToDevice);

		break;
	default:
		LOG(FATAL) << "Unknown Caffe mode.";
	}
	net->Forward();
	
	int index = get_blob_index(net, "loss");
	boost::shared_ptr<Blob<float> > blob = net->blobs()[index];
	//printf("output blob index:%d,  y count:%d\n", index, blob->count());
	const float *blob_ptr = (const float *)blob->cpu_data();
	float y = *blob_ptr;
	
	if (y > 1)
	{
		return 0;
	}
	else
	{
		return 1;
	}
}
```

### 2.3 阈值的选择

因为网络训练过程中用到的损失函数层的margin值为1， 所以我的代码将特征摘要的欧式距离大于1的两张图片认为是不同的数字，反之认为是相同的数字。测试得到的准确率不到80%。

好奇心驱使，我把测试的欧式距离和标签输出来，截取部分如下：
```
0.306053 0
0.777429 0
0.008755 1
0.022663 1
0.028572 1
0.286447 0
0.274477 0
0.530489 0
0.977609 0
0.809711 0
0.105800 0
0.063307 1
```

可以看到，如果判断条件改为：

“特征摘要的欧式距离大于0.1的两张图片认为是不同的数字，反之认为是相同的数字”

准确率应该会提升很多。试一下，得到准确率为95%！

![](img/siamese_net/siamese6.jpg)

下面是根据输出的距离、标签等信息绘制上图的mathematica代码：

```
a={
{11.819243, 0}, {9.927095, 0}, {11.293344, 0}, {10.085804, 0}, \
{10.793452, 0}, {11.157048, 0}, {10.287151, 0}, {12.037912, 0}, \
{13.185759, 0}, {12.594630, 0}, {4.197765, 0}, {4.608857, 0}, \
{4.816608, 0}
};

accu[th_] := Module[{len, i, y, right},
   len = Length[a];
   right = 0;
   For[i = 1, i <= len, i = i + 1,
    If[a[[i, 1]] > th, y = 0, y = 1];
    If[y == a[[i, 2]], right = right + 1, null];
    ];
   N[right / len]

   ];
l = Table[{x, accu[x]}, {x, 0.05, 0.2, 0.01}]
ListLinePlot[l]
```

那训练阶段的ContrastiveLoss层的margin要不要改为0.1呢？  

试验发现不能改，改了的话，deploy的距离阈值使用0.1准确率很低，要进一步缩小为0.001，准确率才能上到85%。

需要注意的是，这里没有分开考虑正负样本的准确率。阈值的选择只考虑了整体的准确率最高。但由于不平衡样本问题，对于正样本（两张图片类似的情况）肯定有比较大的影响。后面的试验分开考虑了。

## 3、试验二：人脸比对

### 3.1 高度标准化的数据

使用一组经高度标准化处理（相同的光照、文件大小相同，眼睛位置相同）的日本女性脸部照片作为训练数据，使用的网络同上面手写体比对的网络，非常简单的网络。

![](img/siamese_net/japanface1.jpg)

该数据集有10个不同的人，每人20张左右的照片。

每个人取3张照片，共30张照片，两两交叉形成训练样本。剩下的照片作为测试集合。

测试用距离阈值为0.2的时候，训练获得的模型可以达到92.7%的准确率。

[人脸数据](http://www.kasrl.org/jaffe.html)

[生成训练和测试数据的代码](code/siamese_net/GeneLevelDB.cpp)


```
  距离     实际标签
>>0.777017 0
>>0.727108 0
>>0.749180 0
>>0.028116 1
>>0.000000 1
>>0.079996 1
>>7.217347 0
...
>>1.503664 0
>>1.011323 0
>>1.993794 0
>>1.772121 0
>>1.928236 0
>>1.070983 0
>>0.963893 0
>>1.019184 0
>>0.061835 1
>>0.079996 1
>>0.000000 1
>>accuracy:0.926667
```

严谨一点，正样本和负样本分开统计准确率：0.582418（正样本）, 0.970774（负样本）。

为了保证正样本和负样本都有比较高的准确率，使用下面的mathematica代码分析可知：

距离的阈值取0.5是合适的，这时候的准确率为：91%(正样本), 90%(负样本)

```
a={{0.7,1},{3.4,0}};(*这里填充真实数据*)
posAccu[th_] := Module[{len, i, y, right, cnt},
   len = Length[a];
   right = 0;
   cnt = 0;
   For[i = 1, i <= len, i = i + 1,
    If[a[[i, 2]] == 0, Continue[], null];
    If[a[[i, 1]] > th, y = 0, y = 1];
    If[y == a[[i, 2]], right = right + 1, null];
    cnt = cnt + 1;
    ];
   N[right / cnt]

   ];
negAccu[th_] := Module[{len, i, y, right, cnt},
   len = Length[a];
   right = 0;
   cnt = 0;
   For[i = 1, i <= len, i = i + 1,
    If[a[[i, 2]] == 1, Continue[], null];
    If[a[[i, 1]] > th, y = 0, y = 1];
    If[y == a[[i, 2]], right = right + 1, null];
    cnt = cnt + 1;
    ];
   N[right / cnt]

   ];
l1 = Table[{x, posAccu[x]}, {x, 0.1, 0.8, 0.01}]
l2 = Table[{x, negAccu[x]}, {x, 0.1, 0.8, 0.01}]
ListLinePlot[{l1, l2}]
```



### 3.2 加入了噪声的数据

每张图片都随机的调整一下亮度：
```
void read_image(const string & filename, unsigned char* pixels) {
	...
	float r = rand() / (RAND_MAX*1.0) * 0.5;
	int i, j;
	for (i = 0; i < g_rows; i++)
	{
		for (j = 0; j < g_cols; j++)
		{
			uchar v = mat.at<uchar>(i, j);
			v = v + (255 - v)*r;
			mat.at<uchar>(i, j) =  v;
			pixels[i*g_cols + j] = v;
		}
	}
	...
}
```
还是使用上面的mathematica代码分析最佳阈值：

![](img/siamese_net/siamese7.jpg)

阈值取0.26，准确率为86%（正样本）， 85.9%（负样本），总体的准确率为：85.9%。相比标准化的数据，准确率有些下降。

