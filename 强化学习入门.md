

## 强化学习入门

### 1. 基本原理

推荐王树森老师的这个[教学视频](https://www.bilibili.com/video/BV12o4y197US?spm_id_from=333.788.videopod.episodes&vd_source=2173cb93b451f2278a1c87becf3ef529)

![image-20250324221405752](img/RL/image-20250324221405752.png)

![image-20250330085209214](img/RL/image-20250330085209214.png)

#### 1.1 两个容易混淆的价值函数：

在深度强化学习中，**action-value函数**和**state-value函数**是评估策略性能的两个关键函数，它们的区别和联系如下：

**1. 定义**

- **State-Value Function (V函数)**:
  
  - 表示在状态 \( s \) 下，遵循策略 \( \pi \) 的预期回报。
  
  - 公式：
  $$
    V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t R_{t+1} \mid S_t = s \right] 
  $$
  
  
  - 其中，
    $$
    \gamma  是折扣因子，( R_{t+1} ) 是时刻 ( t+1 ) 的奖励。
    $$
  
- **Action-Value Function (Q函数)**:
  
  - 表示在状态 \( s \) 下采取动作 \( a \)，然后遵循策略 Pi 的预期回报。
  
  - 公式：
    $$
    Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t R_{t+1} \mid S_t = s, A_t = a \right] 
    $$
    

**2. 区别**

- **输入**:
  
  - V 函数只依赖于状态 \( s \)。
  - Q 函数依赖于状态 \( s \) 和动作 \( a \)。
  
- **用途**:
  - V 函数 用于评估策略在状态 \( s \) 下的表现。
  - Q 函数用于评估在状态 \( s \) 下采取动作 \( a \) 的效果。
  





### 2.方式一：基于价值的学习（Value based RL）

#### 2.1 原理

又叫Q-Learning。

Q(s, a)函数，即action-value函数，能返回状态s下 分别采取各种行动a的未来总的奖励期望值。agent可以根据Q(s, a)函数，在每个状态下采取奖励期望值最大的行动，持续进行下去。

DQN就是要训练出一个深度神经网络，拟合Q(s, a)函数。准确的说，是输入s，输出各种a下的奖励期望。



DQN（Deep Q-Network）是强化学习的一种方法，结合了 Q-Learning 和深度学习。其核心原理如下：

1. **Q-Learning**：使用 Q 函数 Q(s,a)Q(s, a) 估计状态 ss 下采取动作 aa 的长期回报。更新规则：
   $$
   Q(s,a)←Q(s,a)+α[r+γmax⁡Q(s′,a′)−Q(s,a)]
   $$

   $$
   其中 r 是奖励，γ 是折扣因子，s′ 是下一个状态。
   $$

2. **深度神经网络**：用 CNN/MLP 逼近 Q 函数，输入状态 ss，输出所有动作的 Q 值。

3. **经验回放（Experience Replay）**：收集游戏经历，存入缓冲区，**随机采样**训练，减少数据相关性，提高稳定性。

4. **目标网络（Target Network）**：用一个 **滞后的 Q 网络**计算目标 Q 值，减少训练不稳定性。

5. **ε-greedy 策略**：开始时以一定概率随机探索（选择随机动作），逐渐减少探索，更多利用学习到的策略（选择 Q 值最大的动作）。



下图是李宏毅老师的课件笔记：

![image-20250329153218963](img/RL/image-20250329153218963.png)



我的理解：对于状态和动作空间离散且有限的场景（甚至连续的也可以？），还可以用策略迭代更新 + 蒙特卡洛方法，从而得到最优策略：

![image-20250401192434621](img/RL/image-20250401192434621.png)

#### 2.2 实操1 举高高

OpenAI Gym 提供了许多强化学习环境，可以用来训练 DQN（Deep Q-Network）。下面使用 **CartPole-v1**，它的状态空间小，动作空间离散，训练速度快，适合作为 DQN 的入门案例。

**CartPole 任务简介**

- **目标**：控制一个小车，使其上的杆子保持平衡
- **状态空间**：4 维（小车位置、速度、杆子角度、杆子角速度）
- **动作空间**：2 维（向左推、向右推）
- **奖励**：每个时间步 +1，直到杆子倒下或小车超出边界



action只有向左或向右移动小车两个动作，策略看起来是简单的依据当前杆子的角度，杆子往哪边倒就往哪边推车子，这是一个贪心算法，但并不work。

因为车子的状态还包括车子的速度、杆子的角速度，所以杆子的角度并不表示一定像某一侧倾倒，杆子同时也在转动，且有惯性，车子也在移动和有加速度。

```python
import random
import gym
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from collections import deque
import argparse
import os

# 设备选择
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 超参数
gamma = 0.99  # 折扣因子
epsilon = 1.0  # 初始探索率
epsilon_min = 0.01  # 最低探索率
epsilon_decay = 0.995  # 探索率衰减
learning_rate = 1e-3  # 学习率
batch_size = 64  # 经验回放的批量大小
memory_size = 10000  # 经验池大小
target_update_freq = 10  # 目标网络更新频率

env = gym.make("CartPole-v1", render_mode="human")
n_state = env.observation_space.shape[0]  # 状态维度
n_action = env.action_space.n  # 动作数量


# DQN 网络定义
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)


# 初始化网络
model = DQN(n_state, n_action).to(device)
target_model = DQN(n_state, n_action).to(device)
target_model.load_state_dict(model.state_dict())
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
memory = deque(maxlen=memory_size)


def select_action(state, epsilon):
    """基于 ε-greedy 选择动作"""
    if random.random() < epsilon:
        return random.randint(0, n_action - 1)  # 随机选择
    else:
        state = torch.FloatTensor(state).unsqueeze(0).to(device)  # 变换前：[4] -> 变换后：[1, 4]
        return model(state).argmax(1).item()  # 选取 Q 值最大的动作


def train():
    if len(memory) < batch_size:
        return 9999.0  # 经验池数据不足时不训练

    batch = random.sample(memory, batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)

    states = torch.FloatTensor(states).to(device)  # (batch_size, 4)
    actions = torch.LongTensor(actions).unsqueeze(1).to(device)  # (batch_size,) -> (batch_size, 1)
    rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)  # (batch_size,) -> (batch_size, 1)
    next_states = torch.FloatTensor(next_states).to(device)  # (batch_size, 4)
    dones = torch.FloatTensor(dones).unsqueeze(1).to(device)  # (batch_size,) -> (batch_size, 1)

    # 计算当前 Q 值
    q_values = model(states).gather(1, actions)  # 从 Q(s, a) 选取执行的动作 Q 值

    # 计算目标 Q 值
    next_q_values = target_model(next_states).max(1, keepdim=True)[0]  # 选取 Q(s', a') 的最大值
    target_q_values = rewards + gamma * next_q_values * (1 - dones)  # TD 目标

    # 计算损失
    loss = F.mse_loss(q_values, target_q_values.detach())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()


def save_checkpoint(id):
    path=f"dqn_checkpoint_{id}.pth"
    torch.save(model.state_dict(), path)
    print(f"Checkpoint saved to {path}")


def load_checkpoint(path):
    if os.path.exists(path):
        model.load_state_dict(torch.load(path, map_location=device))
        print(f"Checkpoint loaded from {path}")
    else:
        print("No checkpoint found, starting from scratch.")


def main(mode):
    global epsilon

    if mode == "train":
        episodes = 500
        for episode in range(episodes):
            state = env.reset()
            state = state[0]  # 适配 Gym v26
            total_reward = 0

            while True:
                action = select_action(state, epsilon)
                next_state, reward, done, _, _ = env.step(action)

                # 经验回放缓存
                memory.append((state, action, reward, next_state, done))
                state = next_state
                total_reward += reward

                # 训练 DQN
                loss = train()

                if done:
                    break

            # 逐步降低 epsilon，减少随机探索，提高利用率
            epsilon = max(epsilon_min, epsilon * epsilon_decay)

            # 定期更新目标网络，提高稳定性
            if episode % target_update_freq == 0:
                target_model.load_state_dict(model.state_dict())

            # 定期保存模型
            #if episode % 50 == 0:
            if total_reward > 1000:
                save_checkpoint(total_reward)

            print(f"Episode {episode}, Reward: {total_reward}, Epsilon: {epsilon:.3f}, loss:{loss}")

    elif mode == "infer":
        load_checkpoint("./dqn_checkpoint.pth")
        state = env.reset()
        state = state[0]
        total_reward = 0

        while True:
            env.render()
            action = select_action(state, 0)  # 纯利用，epsilon=0
            state, reward, done, _, _ = env.step(action)
            total_reward += reward

            if done:
                break

        print(f"Inference finished. Total reward: {total_reward}")


if __name__ == "__main__":
    main("train")
```

训练效果：

```shell
#一开始杆子竖立不了多久 （看total reward值）
Episode 3, Reward: 24.0, Epsilon: 0.980, loss:0.08007372915744781
Episode 4, Reward: 20.0, Epsilon: 0.975, loss:0.028971994295716286

# 400个episode的时候，杆子可以竖立很久，图片上可以看到不怎么晃
Episode 383, Reward: 8844.0, Epsilon: 0.146, loss:0.049317866563797
Episode 384, Reward: 266.0, Epsilon: 0.145, loss:0.011158960871398449
Episode 385, Reward: 2882.0, Epsilon: 0.144, loss:0.006994911935180426
Episode 386, Reward: 19383.0, Epsilon: 0.144, loss:0.02331690490245819
Episode 387, Reward: 13857.0, Epsilon: 0.143, loss:0.007248120382428169
Episode 388, Reward: 12.0, Epsilon: 0.142, loss:0.27341461181640625
Episode 389, Reward: 60.0, Epsilon: 0.142, loss:0.019126372411847115
Episode 390, Reward: 2144.0, Epsilon: 0.141, loss:0.01851404272019863
Episode 391, Reward: 5288.0, Epsilon: 0.140, loss:0.009122185409069061
Episode 392, Reward: 100.0, Epsilon: 0.139, loss:0.0026022980455309153
Episode 393, Reward: 1567.0, Epsilon: 0.139, loss:0.006415518932044506
Episode 394, Reward: 3256.0, Epsilon: 0.138, loss:1.4469681978225708
#中间效果变差过
Episode 489, Reward: 287.0, Epsilon: 0.086, loss:0.053130991756916046
Episode 490, Reward: 222.0, Epsilon: 0.085, loss:0.09821200370788574
Episode 491, Reward: 1530.0, Epsilon: 0.085, loss:0.4331984519958496
Episode 492, Reward: 12340.0, Epsilon: 0.084, loss:0.03685907647013664
#到这里的时候，已经过去两个小时没有输出了，杆子屹立不倒...
```

![image-20250324122259027](img/RL/image-20250324122259027.png)

### 3. 方式二：基于策略的学习（Policy based RL）

#### 3.1 原理

相比基于价值的强化学习，基于策略的强化学习（如Policy Gradient方法）的一个重要优势是：**它不需要每一步（step-wise）都有明确的即时奖励信号，只要在整局游戏（episode）结束时能获得一个累积奖励（或最终胜负结果），就可以直接训练策略网络**。这是与基于值函数的方法（如DQN）的关键区别之一。

这样我就可以开发一个五子棋小游戏，不需要每一步落子都显式编程给出奖励值，只需要在棋局终局给胜者1负者-1的奖励即可。降低了设计的难度。

但它明显的劣势是梯度倍乘的Return值变化太大，不稳定。

![image-20250324202948777](img/RL/image-20250324202948777.png)

![image-20250324203037983](img/RL/image-20250324203037983.png)

我没有想明白的是：如果能够近似计算Q(s,a)，那似乎也就回到了DQN，不需要做策略学习了？

AI这样说上面训练过程不需要拟合Q函数，只需要计算一次从s开始采取a的游戏局的return就可以。两个方法的区别：

![image-20250324203820771](img/RL/image-20250324203820771.png)



下面两个图是李宏毅老师课程的笔记：

![image-20250327120345688](img/RL/image-20250327120345688.png)



![image-20250329103037264](img/RL/image-20250329103037264.png)

#### 3.2 实操1 举高高

还是平衡车小游戏：

```python
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import argparse
import os
import matplotlib.pyplot as plt

# 设置随机种子
torch.manual_seed(42)
np.random.seed(42)

# 检测设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


# ----------------------------
# 1. 策略网络定义
# ----------------------------
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, action_dim)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        if len(x.shape) == 1:
            x = x.unsqueeze(0)  # [state_dim] -> [1, state_dim]
        x = torch.relu(self.fc1(x))  # [batch_size, hidden_dim]
        logits = self.fc2(x)  # [batch_size, action_dim]
        probs = self.softmax(logits)  # [batch_size, action_dim]
        return probs


# ----------------------------
# 2. Checkpoint 保存与加载
# ----------------------------
def save_checkpoint(policy, optimizer, episode, reward, path="checkpoint.pth"):
    torch.save({
        'policy_state_dict': policy.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'episode': episode,
        'reward': reward
    }, path)
    print(f"Checkpoint saved to {path} (Reward: {reward:.2f})")


def load_checkpoint(policy, optimizer, path="checkpoint.pth"):
    if os.path.exists(path):
        checkpoint = torch.load(path)
        policy.load_state_dict(checkpoint['policy_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print(f"Loaded checkpoint from {path} (Episode: {checkpoint['episode']}, Reward: {checkpoint['reward']:.2f})")
        return checkpoint['episode'], checkpoint['reward']
    else:
        print(f"No checkpoint found at {path}")
        return 0, 0


# ----------------------------
# 3. 训练函数（带Checkpoint和可视化）
# ----------------------------
def train(env_name="CartPole-v1", hidden_dim=128, lr=1e-2,
          gamma=0.99, max_episodes=1000, print_interval=2):
    env = gym.make(env_name, render_mode="human")  # 开启可视化
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)
    optimizer = optim.Adam(policy.parameters(), lr=lr)

    # 尝试加载Checkpoint
    checkpoint_path = "./pbrl_checkpoint_150_2955xx.0.pth"
    start_episode, _ = load_checkpoint(policy, optimizer, checkpoint_path)
    episode_rewards = []

    for episode in range(start_episode, max_episodes):
        state = env.reset()
        state = state[0]
        states, actions, rewards = [], [], []

        while True:
            state_tensor = torch.FloatTensor(state).to(device)
            state_tensor = state_tensor.unsqueeze(0)
            probs = policy(state_tensor)
            m = Categorical(probs) #根据各种action的概率值probs创建一个离散的概率分布
            action = m.sample() #使用该概率分布进行抽样，得到一个具体的action

            next_state, reward, done, _, _ = env.step(action.item())

            states.append(state_tensor.squeeze(0))
            actions.append(action.squeeze(0))
            rewards.append(reward)

            state = next_state
            if done:
                break
            '''到后面模型能力强了，游戏一把玩好久也不死。就导致两次训练的时间间隔很长
            粗暴的截断会有两个问题：
            1、可能导致游戏后面才会出现的states，从来没有出现在训练集里，使得模型失去泛化能力
            2、compute_returns不准确了，这个可能对于密集奖励型游戏还好，毕竟到了后面状态，伽马的n次方接近0，后面的项影响有限
            '''
            '''if  len(rewards) > 1000:
                print('brutally cut')
                break'''

        # 计算回报
        total_reward = sum(rewards)
        episode_rewards.append(total_reward)

        returns = compute_returns(rewards, gamma)
        returns = (returns - returns.mean()) / (returns.std() + 1e-9)

        # 更新策略
        loss = train_policy_network(policy, optimizer, states, actions, returns)

        # 保存Checkpoint（如果回报>1000）
        if total_reward > 1000:
            checkpoint_path = f"./pbrl_checkpoint_{episode}_{total_reward}.pth"
            save_checkpoint(policy, optimizer, episode, total_reward, checkpoint_path)

        # 打印进度
        if (episode + 1) % print_interval == 0:
            avg_reward = np.mean(episode_rewards[-print_interval:])
            print(f"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}, Loss: {loss:.4f}")

        # 提前终止
        if len(episode_rewards) >= 100 and np.mean(episode_rewards[-100:]) >= 2000:
            print(f"Solved at Episode {episode + 1}!")
            break

    env.close()
    return episode_rewards


# ----------------------------
# 4. 推理函数（演示训练好的模型）
# ----------------------------
def test(env_name="CartPole-v1", hidden_dim=128, checkpoint_path="checkpoint.pth"):
    env = gym.make(env_name, render_mode="human")  # 开启可视化
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)
    optimizer = optim.Adam(policy.parameters(), lr=0.001)  # 仅占位，实际不用于测试

    # 加载Checkpoint
    load_checkpoint(policy, optimizer, checkpoint_path)

    print("Starting inference...")
    while True:  # 无限运行直到手动停止
        state = env.reset()[0]
        total_reward = 0

        while True:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).to(device)
                probs = policy(state_tensor)
                action = torch.argmax(probs).item()  # 直接选最优动作

            next_state, reward, done, _, _ = env.step(action)
            total_reward += reward
            state = next_state

            if done:
                print(f"Inference Reward: {total_reward}")
                break


# ----------------------------
# 5. 辅助函数， 用于策略梯度更新（替代Q(s,a) 的蒙特卡洛估计）
# ----------------------------
def compute_returns(rewards, gamma=0.99):
    returns = []
    R = 0
    for r in reversed(rewards):
        R = r + gamma * R
        returns.insert(0, R)
    return torch.tensor(returns, device=device)


def train_policy_network(policy, optimizer, states, actions, returns):
    # 将列表中的状态/动作/回报堆叠成张量， 假设一把游戏玩下来的状态个数是T
    states = torch.stack(states)  # [T, 4]
    actions = torch.stack(actions) # [T]
    returns = returns  # [T]
    # 1. 通过策略网络计算动作概率
    probs = policy(states)  # [T, action_dim]
    # 2. 创建分类分布（用于采样和计算对数概率）
    m = Categorical(probs)
    # 3. 计算所选动作的对数概率
    log_probs = m.log_prob(actions)  # [T,] 
    # 4. 因为基于策略的强化学习要使用梯度上升使得state-value函数的期望最大化，所以损失函数是期望值的负数
    # returns已经在函数外面进行了带折扣的汇总运算，也就是已经是U了，不是每一步的r
    loss = -(log_probs * returns).mean()
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss.item()


# ----------------------------
# 6. 主函数（命令行参数解析）
# ----------------------------
def main(mode):
    if mode == "train":
        rewards = train()
        plt.plot(rewards)
        plt.xlabel("Episode")
        plt.ylabel("Total Reward")
        plt.title("Policy Gradient Training")
        plt.show()
    elif mode == "test":
        test(checkpoint_path="./pbrl_checkpoint_1001.0.pth")


if __name__ == "__main__":
    main("train")
```

效果类似基于价值的强化学习，150个episode后，一根棍子可以举很久不倒。

上面的代码，是严格按照王树森老师的使V(s)最大化的思路来实现的，如果按照李宏毅老师的交叉熵的思路，train函数改为如下：

```python
def train_policy_network(policy, optimizer, states, actions, returns):
    # 将列表中的状态/动作/回报堆叠成张量， 假设一把游戏玩下来的状态个数是T
    states = torch.stack(states)  # [T, 4]
    actions = torch.stack(actions) # [T]
    returns = returns  # [T]
    # 1. 通过策略网络计算动作概率
    probs = policy(states)  # [T, action_dim]
    # 2. 计算交叉熵，因为probs是softmax处理过后的，所以用nll_loss函数
    loss = (torch.nn.functional.nll_loss(torch.log(probs), actions, reduction='none')  * returns).mean()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss.item()
```

这样修改后，也能够快速收敛，把棍子举得久久的。

#### 3.3 实操2 左右博弈五子棋 v1

具体的： 

1. 先让AI帮忙写一个简单的五子棋小游戏，提供reset()  step()等接口，供RL程序调用
2. 定义一个策略CNN网络，输入是9x9的棋局，也就是状态，返回9x9个位置上落黑子的概率。在获取概率每个位置的概率的时候，把当期棋局已经落子的位置剔除掉不考虑
3.  把上面的CNN策略网络创建两个实例，一个叫A，一个叫B。他们是同一个类型网络的左右手对弈。在调用B获得下一步的action的时候，需要把棋局的黑白对调一下后作为输入状态传给B，返回下一步在每个坐标下黑子的概率。因为他们是同一个网络的两个实例，我们训练的网络是执黑的。 
4. 每隔10个epsode就把A的参数赋值给B 
5. 五子棋程序只有在分出胜负的时候才返回非0的reward 



一开始如上面所示的去做，怎么训练都没有起色。后来简化了问题：

1. 棋盘大小改为5x5
2. 连成3个就算赢
3. 让RL程序执黑，和人肉智能对弈（ black_step()函数背后会有白子执行人肉智能 ）， 每128局统计一下黑方的赢率

简化后训练会有效果，慢慢的黑棋的赢率会达到53%，最高到90%，但不能稳住在一个高水平，经常在30%+附近。

人类与之对弈，会明显发现训练后的模型会比训练前聪明。

下面是简化后的代码

```python
import random
import time
import numpy as np
import pygame
import sys
import torch
from exceptiongroup import catch

# 定义颜色
WHITE = (255, 255, 255)
BLACK = (0, 0, 0)
GRAY = (200, 200, 200)
GREEN = (0, 255, 0)

# 棋盘大小
BOARD_SIZE = 5
CELL_SIZE = 50
WINDOW_SIZE = BOARD_SIZE * CELL_SIZE


class Gomoku:
    def __init__(self):
        """ 初始化五子棋环境 """
        pygame.init()
        self.screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))
        pygame.display.set_caption("五子棋")
        self.font = pygame.font.Font(None, 36)
        self.reset()

    def reset(self):
        """ 重新初始化棋局，在中心落一个黑子 """
        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)
        self.last_move = None
        # 随机初始位置
        '''x = random.randint(0, BOARD_SIZE - 1)
        y = random.randint(0, BOARD_SIZE - 1)
        self.board[x, y] = 1  # 黑棋先手
        self.last_move = (x, y)'''
        self.game_over = False
        self.winner = None
        return self.board.copy()

    def step(self, role, x, y):
        """ 执行落子，并计算局部奖励 """
        if self.game_over or self.board[x, y] != 0:
            return self.board.copy(), 0, self.game_over  # 非法落子

        self.board[x, y] = role
        self.last_move = (x, y)

        # 检查胜负
        if self.check_win(x, y, role):
            self.game_over = True
            self.winner = role
            reward = 1 if role == 1 else -1
        else:
            reward = 0
        self.render()
        return self.board.copy(), reward, self.game_over

    def black_step(self, x, y):
        """ 黑方落子，然后白方智能应对 """
        draw = False
        # 黑方落子
        state, reward, done = self.step(1, x, y)
        if done:
            return state, reward, done, draw, (x,y)

        # 白方智能落子
        if not done:
            white_x, white_y = self.find_best_move(-1)
            if white_x is None:
                draw = True
                return None, None, None, draw, (x, y)
            state, reward, done = self.step(-1, white_x, white_y)

        return state, reward, done, draw, (white_x, white_y)

    def ai_play_episode(self, slow=False):
        """ 自动对弈一局，返回黑方的状态、动作和奖励序列 """
        states = []
        actions = []
        rewards = []


        self.reset()
        states.append(torch.tensor(self.board.copy()).unsqueeze(0))

        draw = False
        lastcoord = None
        while not self.game_over:
            # 黑方智能落子
            black_x, black_y = self.find_best_move(1)
            if black_x is None:
                draw = True
                break
            actions.append(torch.tensor([black_x, black_y]))
            state, reward, done, draw, lastcoord = self.black_step(black_x, black_y)
            if draw:
                break
            if slow:
                time.sleep(2)
            states.append( torch.tensor(state.copy()).unsqueeze(0))
            rewards.append(reward)
            if done :
                break


            # 处理游戏结束事件
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    sys.exit()
        if slow:
            time.sleep(2)
        return states[:-1], actions, rewards, draw, lastcoord  # 最后一个状态不需要

    def find_best_move(self, role):
        """ 智能寻找最佳落子位置 """
        opponent = -role

        # 1. 检查自己是否能直接获胜
        for x in range(BOARD_SIZE):
            for y in range(BOARD_SIZE):
                if self.board[x, y] == 0:
                    self.board[x, y] = role
                    if self.check_win(x, y, role):
                        self.board[x, y] = 0  # 恢复
                        return x, y
                    self.board[x, y] = 0  # 恢复

        # 2. 检查对手是否能直接获胜，需要防守
        for x in range(BOARD_SIZE):
            for y in range(BOARD_SIZE):
                if self.board[x, y] == 0:
                    self.board[x, y] = opponent
                    if self.check_win(x, y, opponent):
                        self.board[x, y] = 0  # 恢复
                        return x, y
                    self.board[x, y] = 0  # 恢复

        # 3. 评估每个空位的得分
        best_score = -1
        best_moves = []
        for x in range(BOARD_SIZE):
            for y in range(BOARD_SIZE):
                if self.board[x, y] == 0:
                    score = self.evaluate_position(x, y, role)
                    if score > best_score:
                        best_score = score
                        best_moves = [(x, y)]
                    elif score == best_score:
                        best_moves.append((x, y))

        # 从最佳候选位置中随机选择一个
        if len(best_moves) == 0:
            return None,None
        else:
            return random.choice(best_moves)

    def evaluate_position(self, x, y, role):
        """ 评估在(x,y)落子的价值 """
        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]
        total_score = 0

        for dx, dy in directions:
            # 计算这个方向上的棋型
            line = []
            for d in [-4, -3, -2, -1, 0, 1, 2, 3, 4]:  # 检查9个位置
                nx, ny = x + d * dx, y + d * dy
                if 0 <= nx < BOARD_SIZE and 0 <= ny < BOARD_SIZE:
                    line.append(self.board[nx, ny])
                else:
                    line.append(2)  # 边界外

            # 只关注以(x,y)为中心的5个位置
            center = BOARD_SIZE // 2  # 因为前面检查了9个位置，(x,y)在中间
            segment = line[center - BOARD_SIZE // 2:center + BOARD_SIZE // 2+1]

            # 计算这个方向的得分
            total_score += self.evaluate_segment(segment, role)

        return total_score

    def evaluate_segment(self, segment, role):
        """ 评估一个5连位置的得分 """
        opponent = -role
        count_role = segment.count(role)
        count_opponent = segment.count(opponent)

        # 如果有对手的棋子，这个位置价值降低
        if count_opponent > 0:
            return 0

        # 根据连子数给分
        if count_role == 4: return 10000  # 活四
        if count_role == 3: return 1000  # 活三
        if count_role == 2: return 100  # 活二
        if count_role == 1: return 10  # 活一
        return 1  # 空位

    def check_win(self, x, y, role):
        """ 判断当前落子是否形成五连胜 """
        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]
        for dx, dy in directions:
            count = 1
            for d in [-1, 1]:  # 计算两个方向
                nx, ny = x, y
                while True:
                    nx += d * dx
                    ny += d * dy
                    if 0 <= nx < BOARD_SIZE and 0 <= ny < BOARD_SIZE and self.board[nx, ny] == role:
                        count += 1
                    else:
                        break
            if count >= 3:
                return True
        return False

    def render(self):
        """ 渲染棋盘 """
        self.screen.fill(WHITE)

        # 画网格
        for i in range(BOARD_SIZE):
            pygame.draw.line(self.screen, GRAY, (i * CELL_SIZE + CELL_SIZE // 2, CELL_SIZE // 2),
                             (i * CELL_SIZE + CELL_SIZE // 2, WINDOW_SIZE - CELL_SIZE // 2), 2)
            pygame.draw.line(self.screen, GRAY, (CELL_SIZE // 2, i * CELL_SIZE + CELL_SIZE // 2),
                             (WINDOW_SIZE - CELL_SIZE // 2, i * CELL_SIZE + CELL_SIZE // 2), 2)

        # 画棋子
        for x in range(BOARD_SIZE):
            for y in range(BOARD_SIZE):
                if self.board[x, y] == 1:  # 黑子
                    pygame.draw.circle(self.screen, BLACK, (y * CELL_SIZE + CELL_SIZE // 2,
                                                            x * CELL_SIZE + CELL_SIZE // 2), CELL_SIZE // 3)
                elif self.board[x, y] == -1:  # 白子
                    pygame.draw.circle(self.screen, WHITE, (y * CELL_SIZE + CELL_SIZE // 2,
                                                            x * CELL_SIZE + CELL_SIZE // 2), CELL_SIZE // 3)
                    pygame.draw.circle(self.screen, BLACK, (y * CELL_SIZE + CELL_SIZE // 2,
                                                            x * CELL_SIZE + CELL_SIZE // 2), CELL_SIZE // 3, 2)

        # 标记最后一手棋
        if self.last_move:
            lx, ly = self.last_move
            pygame.draw.circle(self.screen, GREEN, (ly * CELL_SIZE + CELL_SIZE // 2,
                                                    lx * CELL_SIZE + CELL_SIZE // 2), CELL_SIZE // 3 + 3, 2)

        # 游戏结束显示赢家
        if self.game_over:
            msg = "black wins." if self.winner == 1 else "white wins."
            text = self.font.render(msg, True, BLACK)
            self.screen.blit(text, (WINDOW_SIZE // 3, WINDOW_SIZE // 2))

        pygame.display.flip()

    def play_human(self):
        """ 让人类玩家交互式落子 """
        running = True
        role = 1
        while running:
            self.render()
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    running = False
                    pygame.quit()
                    sys.exit()
                elif event.type == pygame.MOUSEBUTTONDOWN and not self.game_over:
                    x, y = event.pos[1] // CELL_SIZE, event.pos[0] // CELL_SIZE
                    _, r, done = self.step(role, x, y)
                    print(f"r={r}")
                    if not done:
                        role *= -1  # 轮到另一方落子
        pygame.quit()

    def human_play(self, role):
        """ 让人类玩家落子，阻塞直到有效输入，并返回新状态 """
        self.render()

        while True:  # 阻塞等待有效输入
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    exit()  # 退出程序

                elif event.type == pygame.MOUSEBUTTONDOWN and not self.game_over:
                    x, y = event.pos[1] // CELL_SIZE, event.pos[0] // CELL_SIZE

                    # 确保落子位置未被占据
                    if self.board[x, y] == 0:
                        state, reward, done,  = self.step(role,  x, y)
                        return state, reward, done  # 返回新状态、奖励、是否终局


def main():
    go = Gomoku()
    print(5//2)
    go.ai_play_episode(True)








if __name__ == "__main__":
    main()
```

然后是训练的代码：

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import time
import pygame
import numpy as np
from collections import Counter
import os
from gomoku import Gomoku  # 假设 gomoku.py 里定义了五子棋环境
from gomoku import BOARD_SIZE
from torch.distributions import Categorical

# 设备初始化
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



# 定义策略网络
class PolicyNet(nn.Module):
    def __init__(self):
        super(PolicyNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.fc = nn.Linear(128 * BOARD_SIZE * BOARD_SIZE, BOARD_SIZE * BOARD_SIZE)

    def forward(self, state):
        batch_size = state.size(0)  # 获取 batch 大小

        # 计算合法落子 mask（state=0 的地方可以落子）
        mask = (state.view(batch_size, BOARD_SIZE, BOARD_SIZE) == 0).float()  # [batch, BOARD_SIZE, BOARD_SIZE]，可落子=1，不可落子=0
        mask = mask.view(batch_size, BOARD_SIZE*BOARD_SIZE)  # [batch, BOARD_SIZE*BOARD_SIZE]

        # 通过 CNN 提取特征
        x = F.relu(self.conv1(state))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))

        # 展平成 [batch, 128 * BOARD_SIZE * BOARD_SIZE]
        x = x.view(batch_size, -1)
        x = self.fc(x)

        # 对不可落子的位置赋极小值 (-inf)
        x = x.masked_fill(mask == 0, float('-inf'))  # 屏蔽不可落子的位置

        # 计算 softmax 仅在合法位置
        x = F.softmax(x, dim=1)

        return x.view(batch_size, BOARD_SIZE, BOARD_SIZE)  # 变回 [batch, BOARD_SIZE, BOARD_SIZE] 的棋盘格式

# 选择动作
def select_action(policy_net, state):
    """
    选择动作，避免已落子点，并处理棋盘已满的情况。

    :param policy_net: 策略网络
    :param state: 当前棋盘状态 (9x9)
    :return: 选择的动作坐标 (x, y) 或 None（表示平局）
    """
    state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)
    policy_net.eval()
    with torch.no_grad():
        action_probs = policy_net(state_tensor).squeeze().detach().cpu().numpy()  # 转到CPU计算
    policy_net.train()


    # 不能选择已落子点
    action_probs[state != 0] = 0

    total_prob = action_probs.sum()
    if total_prob == 0:
        return None  # 棋盘已满，平局

    action_probs /= total_prob  # 归一化
    choice = np.random.choice(BOARD_SIZE * BOARD_SIZE, p=action_probs.flatten())
    return divmod(choice, BOARD_SIZE)  # 返回 (x, y) 坐标

def entropy(coords):
    if not coords:
        return 0  # 空列表的熵定义为0

    counter = Counter(coords)  # 统计每个坐标的出现次数
    total = len(coords)  # 总数
    probs = np.array([count / total for count in counter.values()])  # 计算概率分布
    #print(f"{len(coords)}, {len(probs)}")
    return -np.sum(probs * np.log2(probs))  # 计算熵（以2为底）

# 运行完整一局游戏，收集经验
def play_one_episode(policy_net, win_coords):
    env = Gomoku()
    state = env.reset()
    states, actions, rewards = [], [], []

    blackwin = False


    while True:
        action = select_action(policy_net , state )
        if action is None:  # 平局
            print(f"Draw detected. Ending episode. rewards len:{len(rewards)}")
            break
        #print(f"on {action[0], action[1]}")
        new_state, reward, done, draw, (x,y) = env.black_step(*action)
        if draw:
            continue;


        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)
        states.append(state_tensor.squeeze(0))
        actions.append(torch.tensor(action, dtype=torch.long, device=device))
        rewards.append(reward)


        if done:
            if reward > 0:
                blackwin = True
            win_coords.append((x,y))
            break

        state = new_state


    return states, actions, rewards, blackwin


# 计算折扣回报（G_t）
def compute_returns(rewards, gamma=0.99):
    returns = []
    G = 0
    for r in reversed(rewards):
        G = r + gamma * G
        returns.insert(0, G)  # 按时间步正序排列
    return torch.tensor(returns, dtype=torch.float32, device=device)


def train_policy_network(policy, optimizer, states, actions, returns):
    """
    policy: 策略网络
    optimizer: 优化器
    states:  [T, BOARD_SIZE, BOARD_SIZE]，每一步的棋盘状态
    actions: [T, 2]，每一步 (x, y) 位置的动作
    returns: [T]，每一步的折扣回报
    """
    states = torch.stack(states)  # [T, 1, BOARD_SIZE, BOARD_SIZE]
    actions = torch.stack(actions)  # [T, 2]，包含 x 和 y 坐标

    # **Step 1: 计算 state_mask**
    state_mask = (states != 0).float().squeeze(1)  # [T, BOARD_SIZE, BOARD_SIZE]，非 0（已落子）为 1，空位为 0

    # **Step 2: 计算策略网络输出**
    probs = policy(states)  # [T, BOARD_SIZE, BOARD_SIZE]

    # **Step 3: 过滤已落子点**
    masked_probs = probs * (1 - state_mask)  # 已落子的位置概率设为 0
    masked_probs = masked_probs / (masked_probs.sum(dim=(1, 2), keepdim=True) + 1e-9)  # 重新归一化

    # **Step 4: 计算 log 概率**
    # 从 masked_probs 这个 [T, BOARD_SIZE, BOARD_SIZE] 的张量中，提取每一步行动 actions[x, y] 处的概率, 并求log()
    log_probs = torch.log( masked_probs[torch.arange(actions.shape[0]), actions[:, 0], actions[:, 1]] + 1e-9)

    # **Step 5: 归一化 returns**
    returns = (returns - returns.mean()) / (returns.std() + 1e-9)

    # **Step 6: 计算损失**
    loss = -(log_probs * returns).mean()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()

# 主训练循环
def train(policy_net, episodes=50000, checkpoint_path='policy.pth', gamma=0.99):
    optimizer = optim.Adam(policy_net.parameters(), lr=0.00001)

    # 加载已有的模型参数
    if os.path.exists(checkpoint_path):
        policy_net.load_state_dict(torch.load(checkpoint_path, map_location=device))


    policy_net = policy_net.to(device)

    win_cnt = 0

    win_coords = []
    for episode in range(episodes):
        # 运行一局游戏，收集 (states, actions, rewards), win_coords收集一段时间的赢的坐标，用于监控是不是棋局总是在重复
        states, actions, rewards, blackwin = play_one_episode(policy_net,  win_coords)
        if blackwin:
            win_cnt += 1

        # 计算折扣回报
        returns = compute_returns(rewards, gamma)

        # 更新策略网络
        loss = train_policy_network(policy_net, optimizer, states, actions, returns)


        # 每 128 轮更新对手策略，
        update_interval = 128
        if episode % update_interval == 0:
            path = checkpoint_path
            if win_cnt / update_interval > 0.4:
                path = f"simple_gomoku_{int(win_cnt*100 / update_interval)}.pth"
            torch.save(policy_net.state_dict(), path)
            entro = entropy(win_coords)
            print(f'Episode {episode}: Model updated, checkpoint saved, Loss: {loss:.4f}, win_ratio:{win_cnt / update_interval:.2f},entropy:{entro:.2f}')
            win_cnt = 0
            win_coords = []

def play_with_human(checkpoint_path='policy.pth'):
    env = Gomoku()

    policy_net = PolicyNet().to(device)
    policy_net.load_state_dict(torch.load(checkpoint_path))
    state = env.reset()
    turn = 1  # AI 执黑，人类执白
    done = False

    while not done:
        if turn == 1:
            action = select_action(policy_net, state)
            state, _, done = env.step(1, *action)  # AI 落子
        else:
            state, _, done = env.human_play(-1)  # 人类落子

        turn *= -1  # 轮流落子


if __name__ == "__main__":
    import argparse
    policy = PolicyNet().to(device)
    arg = "test"
    if arg == "train":
        train(policy)
    else:
        play_with_human('./simple_gomoku_53.pth')

```



#### 3.4 实操3 左右博弈五子棋 v2

参考AlphaGo的训练方式，修改一下：

1. 状态的表示，用10个通道，而不是只有一个通道。分别表示黑子和白子过去5步的棋子布局
3. 网络结构也调整一下，加了残差网络、归一化、1x1卷积等
4. 棋盘缩小到5x5，连成3个即为胜出
5. 五子棋类Gomoku增加了基于规则搜索的人肉智能：
   1. ai_play_episode()两个人肉智能对弈一局，记录下棋谱可以供RL训练；
   2. black_step()用于RL作为黑方下了后，人肉智能作为对手会再落子并返回新的状态

五子棋代码：

```python
import random
import time
import numpy as np
import pygame
import sys
import torch
from exceptiongroup import catch

# 定义颜色
WHITE = (255, 255, 255)
BLACK = (0, 0, 0)
GRAY = (200, 200, 200)
GREEN = (0, 255, 0)

# 棋盘大小
BOARD_SIZE = 5
CELL_SIZE = 50
WINDOW_SIZE = BOARD_SIZE * CELL_SIZE


class Gomoku:
    def __init__(self):
        """ 初始化五子棋环境 """
        pygame.init()
        self.screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))
        pygame.display.set_caption("五子棋")
        self.font = pygame.font.Font(None, 36)
        self.reset()

    def reset(self):
        """ 重新初始化棋局，在中心落一个黑子 """
        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)
        self.last_move = None
        # 随机初始位置
        '''x = random.randint(0, BOARD_SIZE - 1)
        y = random.randint(0, BOARD_SIZE - 1)
        self.board[x, y] = 1  # 黑棋先手
        self.last_move = (x, y)'''
        self.game_over = False
        self.winner = None
        return self.board.copy()

    def step(self, role, x, y):
        """ 执行落子，并计算局部奖励 """
        if self.game_over or self.board[x, y] != 0:
            return self.board.copy(), 0, self.game_over  # 非法落子

        self.board[x, y] = role
        self.last_move = (x, y)

        # 检查胜负
        if self.check_win(x, y, role):
            self.game_over = True
            self.winner = role
            reward = 1 if role == 1 else -1
        else:
            reward = 0
        self.render()
        return self.board.copy(), reward, self.game_over

    def black_step(self, x, y):
        """ 黑方落子，然后白方智能应对 """
        draw = False
        # 黑方落子
        state, reward, done = self.step(1, x, y)
        if done:
            return state, reward, done, draw, (x,y)

        # 白方智能落子
        if not done:
            white_x, white_y = self.find_best_move(-1)
            if white_x is None:
                draw = True
                return None, None, None, draw, (x, y)
            state, reward, done = self.step(-1, white_x, white_y)

        return state, reward, done, draw, (white_x, white_y)

    def ai_play_episode(self, slow=False):
        """ 自动对弈一局，返回黑方的状态、动作和奖励序列 """
        states = []
        actions = []
        rewards = []


        self.reset()
        states.append(torch.tensor(self.board.copy()).unsqueeze(0))

        draw = False
        lastcoord = None
        while not self.game_over:
            # 黑方智能落子
            black_x, black_y = self.find_best_move(1)
            if black_x is None:
                draw = True
                break
            actions.append(torch.tensor([black_x, black_y]))
            state, reward, done, draw, lastcoord = self.black_step(black_x, black_y)
            if draw:
                break
            if slow:
                time.sleep(2)
            states.append( torch.tensor(state.copy()).unsqueeze(0))
            rewards.append(reward)
            if done :
                break


            # 处理游戏结束事件
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    sys.exit()
        if slow:
            time.sleep(2)
        return states[:-1], actions, rewards, draw, lastcoord  # 最后一个状态不需要

    def find_best_move(self, role):
        """ 智能寻找最佳落子位置 """
        opponent = -role

        # 1. 检查自己是否能直接获胜
        for x in range(BOARD_SIZE):
            for y in range(BOARD_SIZE):
                if self.board[x, y] == 0:
                    self.board[x, y] = role
                    if self.check_win(x, y, role):
                        self.board[x, y] = 0  # 恢复
                        return x, y
                    self.board[x, y] = 0  # 恢复

        # 2. 检查对手是否能直接获胜，需要防守
        for x in range(BOARD_SIZE):
            for y in range(BOARD_SIZE):
                if self.board[x, y] == 0:
                    self.board[x, y] = opponent
                    if self.check_win(x, y, opponent):
                        self.board[x, y] = 0  # 恢复
                        return x, y
                    self.board[x, y] = 0  # 恢复

        # 3. 评估每个空位的得分
        best_score = -1
        best_moves = []
        for x in range(BOARD_SIZE):
            for y in range(BOARD_SIZE):
                if self.board[x, y] == 0:
                    score = self.evaluate_position(x, y, role)
                    if score > best_score:
                        best_score = score
                        best_moves = [(x, y)]
                    elif score == best_score:
                        best_moves.append((x, y))

        # 从最佳候选位置中随机选择一个
        if len(best_moves) == 0:
            return None,None
        else:
            return random.choice(best_moves)

    def evaluate_position(self, x, y, role):
        """ 评估在(x,y)落子的价值 """
        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]
        total_score = 0

        for dx, dy in directions:
            # 计算这个方向上的棋型
            line = []
            for d in [-4, -3, -2, -1, 0, 1, 2, 3, 4]:  # 检查9个位置
                nx, ny = x + d * dx, y + d * dy
                if 0 <= nx < BOARD_SIZE and 0 <= ny < BOARD_SIZE:
                    line.append(self.board[nx, ny])
                else:
                    line.append(2)  # 边界外

            # 只关注以(x,y)为中心的5个位置
            center = BOARD_SIZE // 2  # 因为前面检查了9个位置，(x,y)在中间
            segment = line[center - BOARD_SIZE // 2:center + BOARD_SIZE // 2+1]

            # 计算这个方向的得分
            total_score += self.evaluate_segment(segment, role)

        return total_score

    def evaluate_segment(self, segment, role):
        """ 评估一个5连位置的得分 """
        opponent = -role
        count_role = segment.count(role)
        count_opponent = segment.count(opponent)

        # 如果有对手的棋子，这个位置价值降低
        if count_opponent > 0:
            return 0

        # 根据连子数给分
        if count_role == 4: return 10000  # 活四
        if count_role == 3: return 1000  # 活三
        if count_role == 2: return 100  # 活二
        if count_role == 1: return 10  # 活一
        return 1  # 空位

    def check_win(self, x, y, role):
        """ 判断当前落子是否形成五连胜 """
        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]
        for dx, dy in directions:
            count = 1
            for d in [-1, 1]:  # 计算两个方向
                nx, ny = x, y
                while True:
                    nx += d * dx
                    ny += d * dy
                    if 0 <= nx < BOARD_SIZE and 0 <= ny < BOARD_SIZE and self.board[nx, ny] == role:
                        count += 1
                    else:
                        break
            if count >= 3:
                return True
        return False

    def render(self):
        """ 渲染棋盘 """
        self.screen.fill(WHITE)

        # 画网格
        for i in range(BOARD_SIZE):
            pygame.draw.line(self.screen, GRAY, (i * CELL_SIZE + CELL_SIZE // 2, CELL_SIZE // 2),
                             (i * CELL_SIZE + CELL_SIZE // 2, WINDOW_SIZE - CELL_SIZE // 2), 2)
            pygame.draw.line(self.screen, GRAY, (CELL_SIZE // 2, i * CELL_SIZE + CELL_SIZE // 2),
                             (WINDOW_SIZE - CELL_SIZE // 2, i * CELL_SIZE + CELL_SIZE // 2), 2)

        # 画棋子
        for x in range(BOARD_SIZE):
            for y in range(BOARD_SIZE):
                if self.board[x, y] == 1:  # 黑子
                    pygame.draw.circle(self.screen, BLACK, (y * CELL_SIZE + CELL_SIZE // 2,
                                                            x * CELL_SIZE + CELL_SIZE // 2), CELL_SIZE // 3)
                elif self.board[x, y] == -1:  # 白子
                    pygame.draw.circle(self.screen, WHITE, (y * CELL_SIZE + CELL_SIZE // 2,
                                                            x * CELL_SIZE + CELL_SIZE // 2), CELL_SIZE // 3)
                    pygame.draw.circle(self.screen, BLACK, (y * CELL_SIZE + CELL_SIZE // 2,
                                                            x * CELL_SIZE + CELL_SIZE // 2), CELL_SIZE // 3, 2)

        # 标记最后一手棋
        if self.last_move:
            lx, ly = self.last_move
            pygame.draw.circle(self.screen, GREEN, (ly * CELL_SIZE + CELL_SIZE // 2,
                                                    lx * CELL_SIZE + CELL_SIZE // 2), CELL_SIZE // 3 + 3, 2)

        # 游戏结束显示赢家
        if self.game_over:
            msg = "black wins." if self.winner == 1 else "white wins."
            text = self.font.render(msg, True, BLACK)
            self.screen.blit(text, (WINDOW_SIZE // 3, WINDOW_SIZE // 2))

        pygame.display.flip()

    def play_human(self):
        """ 让人类玩家交互式落子 """
        running = True
        role = 1
        while running:
            self.render()
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    running = False
                    pygame.quit()
                    sys.exit()
                elif event.type == pygame.MOUSEBUTTONDOWN and not self.game_over:
                    x, y = event.pos[1] // CELL_SIZE, event.pos[0] // CELL_SIZE
                    _, r, done = self.step(role, x, y)
                    print(f"r={r}")
                    if not done:
                        role *= -1  # 轮到另一方落子
        pygame.quit()

    def human_play(self, role):
        """ 让人类玩家落子，阻塞直到有效输入，并返回新状态 """
        self.render()

        while True:  # 阻塞等待有效输入
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    exit()  # 退出程序

                elif event.type == pygame.MOUSEBUTTONDOWN and not self.game_over:
                    x, y = event.pos[1] // CELL_SIZE, event.pos[0] // CELL_SIZE

                    # 确保落子位置未被占据
                    if self.board[x, y] == 0:
                        state, reward, done,  = self.step(role,  x, y)
                        return state, reward, done  # 返回新状态、奖励、是否终局


def main():
    go = Gomoku()
    print(5//2)
    go.ai_play_episode(True)








if __name__ == "__main__":
    main()
```

训练的代码：

```python
import time

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import numpy as np
from collections import Counter
import os
from gomoku import BOARD_SIZE

from gomoku import Gomoku  # 假设 gomoku.py 里定义了五子棋环境

# 设备初始化
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
'''
'''
HISTORY_LEN=5


# 定义一个残差块
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)

    def forward(self, x):
        residual = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += residual  # 残差连接
        return F.relu(out)


# 定义策略网络
class PolicyNet(nn.Module):
    def __init__(self):
        super(PolicyNet, self).__init__()
        # 初始卷积层 + BN
        self.conv1 = nn.Conv2d(HISTORY_LEN * 2, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)

        # 添加两个残差块
        self.res_block1 = ResidualBlock(128)
        self.res_block2 = ResidualBlock(128)

        # 策略头：使用 1x1 卷积替代全连接层
        self.policy_conv = nn.Conv2d(128, 4, kernel_size=1)
        self.policy_bn = nn.BatchNorm2d(4)
        self.policy_out = nn.Conv2d(4, 1, kernel_size=1)

    def forward(self, state):
        batch_size = state.size(0)

        # 计算合法落子 mask（state=0 的地方可以落子）
        # 这里假设通道0和通道5代表双方棋子
        mask = state[:, 0, :, :] + state[:, 5, :, :]
        mask = (mask.view(batch_size, BOARD_SIZE, BOARD_SIZE) == 0).float()
        mask = mask.view(batch_size, BOARD_SIZE*BOARD_SIZE)

        # 通过卷积层提取特征，后接BN与ReLU
        x = F.relu(self.bn1(self.conv1(state)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))

        # 通过残差块进一步提取特征
        x = self.res_block1(x)
        x = self.res_block2(x)

        # 策略头：1x1卷积层 + BN + ReLU
        x = F.relu(self.policy_bn(self.policy_conv(x)))
        x = self.policy_out(x)  # 输出 shape: [batch, 1, BOARD_SIZE, BOARD_SIZE]

        # 展平成 [batch, 81] 作为 logits
        x = x.view(batch_size, BOARD_SIZE*BOARD_SIZE)

        # 对不可落子的位置赋极小值 (-inf)
        x = x.masked_fill(mask == 0, float('-inf'))

        # 计算 softmax 仅在合法位置
        x = F.softmax(x, dim=1)

        return x.view(batch_size, BOARD_SIZE, BOARD_SIZE)

# 选择动作
def select_action(policy_net, state):
    """
    选择动作，避免已落子点，并处理棋盘已满的情况。

    :param policy_net: 策略网络
    :param state: 当前棋盘状态 (9x9)
    :return: 选择的动作坐标 (x, y) 或 None（表示平局）
    """
    state = torch.stack(state).to(device)
    state_tensor = torch.tensor(state, dtype=torch.float32, device=device)
    state_tensor = reorganize_state(state_tensor, True)
    policy_net.eval()
    with torch.no_grad():
        action_probs = policy_net(state_tensor).squeeze().detach().cpu().numpy()  # 转到CPU计算
    policy_net.train()


    # 不能选择已落子点
    action_probs[state[-1,-1].cpu() != 0] = 0 # 取最新的一次state里的

    total_prob = action_probs.sum()
    if total_prob == 0:
        policy_net(state_tensor)
        return None  # 棋盘已满，平局

    action_probs /= total_prob  # 归一化
    choice = np.random.choice(BOARD_SIZE * BOARD_SIZE, p=action_probs.flatten())
    return divmod(choice, BOARD_SIZE)  # 返回 (x, y) 坐标

def entropy(coords):
    if not coords:
        return 0  # 空列表的熵定义为0

    counter = Counter(coords)  # 统计每个坐标的出现次数
    total = len(coords)  # 总数
    probs = np.array([count / total for count in counter.values()])  # 计算概率分布
    #print(f"{len(coords)}, {len(probs)}")
    return -np.sum(probs * np.log2(probs))  # 计算熵（以2为底）

# 运行完整一局游戏，收集经验
def play_one_episode(policy_net,  win_coords):
    env = Gomoku()
    state = env.reset()
    states, actions, rewards = [], [], []
    blackwin = False
    draw = False


    while True:
        history = states.copy()
        history.append(torch.tensor(state).unsqueeze(0).to(device))
        action = select_action(policy_net, history)

        if action is None:  # 平局
            print(f"Draw detected. Ending episode. rewards len:{len(rewards)}")
            draw = True
            break
        new_state, reward, done, draw, lastcoord = env.black_step(*action)


        #time.sleep(2)

        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)
        states.append(state_tensor.squeeze(0))
        actions.append(torch.tensor(action, dtype=torch.long, device=device))
        rewards.append(reward)


        if done:
            if  reward > 0:
                blackwin = True
            win_coords.append(lastcoord)
            break
        state = new_state


    return states, actions, rewards, blackwin, draw


# 计算折扣回报（G_t）
def compute_returns(rewards, gamma=1):
    returns = []
    G = 0
    for r in reversed(rewards):
        G = r + gamma * G
        returns.insert(0, G)  # 按时间步正序排列
    return torch.tensor(returns, dtype=torch.float32, device=device)


def reorganize_state(states: torch.Tensor, only_return_last_one=False):
    # states本身的shape：batchsz, 1, BOARD_SIZE, BOARD_SIZE
    batchsz = len(states)
    state_tensor = torch.zeros(batchsz, HISTORY_LEN * 2, BOARD_SIZE, BOARD_SIZE)

    for i in range(batchsz): #每个样本
        for j in range(HISTORY_LEN): #历史状态
            if i - j < 0:
                break
            s = states[i - j][0]
            state_tensor[i, j] = (s == 1).int()  # 通道 0 1 2 3 4 存放黑棋(自己)的最近5步
            state_tensor[i, 5+j] = (s == -1).int()  # 通道 5 6 7 8 9 存放白棋（对手）的最近5步
    state_tensor = state_tensor.to(device)
    if only_return_last_one:
        return state_tensor[-1:, :,:,:]
    else:
        return state_tensor

def train_policy_network(policy, optimizer, states, actions, returns):
    """
    policy: 策略网络
    optimizer: 优化器
    states:  [T, BOARD_SIZE, BOARD_SIZE]，每一步的棋盘状态
    actions: [T, 2]，每一步 (x, y) 位置的动作
    returns: [T]，每一步的折扣回报
    """

    states = torch.stack(states).to(device)  # [T, 1, BOARD_SIZE, BOARD_SIZE]
    actions = torch.stack(actions).to(device)  # [T, 2]，包含 x 和 y 坐标

    # **Step 1: 计算 state_mask**
    state_mask = (states != 0).float().squeeze(1)  # [T, BOARD_SIZE, BOARD_SIZE]，非 0（已落子）为 1，空位为 0

    states = reorganize_state(states)

    # **Step 2: 计算策略网络输出**
    probs = policy(states)  # [T, BOARD_SIZE, BOARD_SIZE]

    # **Step 3: 过滤已落子点**
    masked_probs = probs * (1 - state_mask)  # 已落子的位置概率设为 0
    masked_probs = masked_probs / (masked_probs.sum(dim=(1, 2), keepdim=True) + 1e-9)  # 重新归一化

    # **Step 4: 计算 log 概率**
    # 从 masked_probs 这个 [T, BOARD_SIZE, BOARD_SIZE] 的张量中，提取每一步行动 actions[x, y] 处的概率, 并求log()
    log_probs = torch.log( masked_probs[torch.arange(actions.shape[0]), actions[:, 0], actions[:, 1]] + 1e-9)

    # **Step 5: 归一化 returns**
    returns = (returns - returns.mean()) / (returns.std() + 1e-9)

    # **Step 6: 计算损失**
    loss = -(log_probs * returns).mean()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()

# 主训练循环
def train_by_human_rules(policy_net, episodes=500000, checkpoint_path='policy.pth', gamma=0.99):

    optimizer = optim.Adam(policy_net.parameters(), lr=0.0001)

    # 加载已有的模型参数
    if os.path.exists(checkpoint_path):
        policy_net.load_state_dict(torch.load(checkpoint_path, map_location=device))



    policy_net = policy_net.to(device)

    win_cnt = 0

    win_coords = []
    for episode in range(episodes):

        env = Gomoku()
        env.reset()
        # 植入了人肉智能的两个自动程序下棋，把回合记录下来用于训练
        states, actions, rewards, draw, lastcoord = env.ai_play_episode()
        if rewards[-1] > 0:
            blackwin = True
        else:
            blackwin = False

        if blackwin:
            win_cnt += 1
        if draw:  #平局导致return都为0，没有必要训练
            continue

        win_coords.append(lastcoord)

        # 计算折扣回报
        returns = compute_returns(rewards,gamma)

        # 更新策略网络
        loss = train_policy_network(policy_net, optimizer, states, actions, returns)

        update_interval = 128
        if episode % update_interval == 0:
            torch.save(policy_net.state_dict(), checkpoint_path)
            entro = entropy(win_coords)
            print(f'Episode {episode}: Model updated, checkpoint saved, Loss: {loss:.4f}, win_ratio:{win_cnt / update_interval:.2f},entropy:{entro:.2f}')
            win_cnt = 0
            win_coords = []

def train_by_self_play(policy_net, episodes=100000, checkpoint_path='policy.pth', gamma=0.9):

    optimizer = optim.Adam(policy_net.parameters(), lr=0.0001)

    # 加载已有的模型参数
    if os.path.exists(checkpoint_path):
        policy_net.load_state_dict(torch.load(checkpoint_path, map_location=device))


    policy_net = policy_net.to(device)

    win_cnt = 0

    win_coords = []
    for episode in range(episodes):
        # 网络基于自己的策略，跟人肉智能对弈一个回合，收集 (states, actions, rewards),
        # win_coords收集一段时间的赢的坐标，用于监控是不是棋局总是在重复
        states, actions, rewards, blackwin, draw = play_one_episode(policy_net, win_coords)


        if blackwin:
            win_cnt += 1
        if draw:  #平局导致return都为0，没有必要训练
            continue

        # 计算折扣回报
        returns = compute_returns(rewards,gamma)

        # 更新策略网络
        loss = train_policy_network(policy_net, optimizer, states, actions, returns)

        update_interval = 128
        if episode % update_interval == 0:
            path = checkpoint_path
            if win_cnt / update_interval > 0.4:
                path=f"./gomoku_{int(win_cnt*100 / update_interval)}.pth"
            torch.save(policy_net.state_dict(), path)
            entro = entropy(win_coords)
            print(f'Episode {episode}: Model updated, checkpoint saved, Loss: {loss:.4f}, win_ratio:{win_cnt / update_interval:.2f},entropy:{entro:.2f}')
            win_cnt = 0
            win_coords = []



def play_with_human(checkpoint_path='policy.pth'):
    env = Gomoku()

    policy_net = PolicyNet().to(device)
    policy_net.load_state_dict(torch.load(checkpoint_path))
    print(f"load {checkpoint_path} successfully")
    state = env.reset()
    turn = 1  # AI 执黑，人类执白
    done = False
    states = []

    while not done:
        if turn == 1:
            history = states.copy()
            history.append(torch.tensor(state).unsqueeze(0).to(device))
            action = select_action(policy_net, history)
            state, _, done = env.step(1, *action)  # AI 落子
            if done:
                print("AI win")
            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)
            states.append(state_tensor.squeeze(0))
        else:
            state, _, done = env.human_play(-1)  # 人类落子
            if done:
                print("human win")

        turn *= -1  # 轮流落子


if __name__ == "__main__":
    import argparse
    policy = PolicyNet().to(device)
    arg = "train"
    if arg == "train":
        train_by_human_rules(policy)
    else:
        play_with_human(checkpoint_path='./policy.pth')

```

学习完22完次“人肉智能”的黑方棋谱后，前后对比能看到明显有提升：

[训练前的对弈视频，相当愚蠢](img/RL/stupid_chess.mp4)

[训练后的对弈视频，有一定的智能](img/RL/clever_chess.mp4)

从0开始，RL通过直接与人肉智能对弈，也能学习到一些技能，胜率高的时候能到70%，但不能稳定在一个较高的胜率上。

### 4. 方式三：Actor-Critic方法

#### 4.1 原理

![image-20250326143312064](img/RL/image-20250326143312064.png)



按照上图进行的训练，我没有能够收敛，所以我又找了[李宏毅老师的课程](https://www.bilibili.com/video/BV15hw9euExZ?spm_id_from=333.788.videopod.episodes&vd_source=2173cb93b451f2278a1c87becf3ef529&p=3)学习，总结如下，并在4.3的训练里收敛了：

![image-20250327090246933](img/RL/image-20250327090246933.png)

进一步理解一下：

![image-20250329173212697](img/RL/image-20250329173212697.png)

#### 4.2 实操1 按照王树森老师的课程进行训练

按照上面的算法，代码如下，但是不收敛：

```python
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import argparse
import os
import matplotlib.pyplot as plt

# 设置随机种子
torch.manual_seed(42)
np.random.seed(42)

# 检测设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


# ----------------------------
# 1. 策略网络定义
# ----------------------------
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, action_dim)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        if len(x.shape) == 1:
            x = x.unsqueeze(0)  # [state_dim] -> [1, state_dim]
        x = torch.relu(self.fc1(x))  # [batch_size, hidden_dim]
        logits = self.fc2(x)  # [batch_size, action_dim]
        probs = self.softmax(logits)  # [batch_size, action_dim]
        return probs


class Critic(nn.Module):
    def __init__(self, state_dim=4, action_dim=1, hidden_dim=128):
        super(Critic, self).__init__()
        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)  # 输出单个Q值
        self.relu = nn.ReLU()

    def forward(self, state, action):
        x = torch.cat([state, action], dim=-1)  # 拼接状态和动作
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        q_value = self.fc3(x)
        return q_value


# ----------------------------
# 2. Checkpoint 保存与加载
# ----------------------------
def save_checkpoint(policy, optimizer1, critic, optimizer2, episode, reward, path):
    torch.save({
        'policy_state_dict': policy.state_dict(),
        'optimizer1_state_dict': optimizer1.state_dict(),
        'critic_state_dict': critic.state_dict(),
        'optimizer2_state_dict': optimizer2.state_dict(),
        'episode': episode,
        'reward': reward
    }, path)
    print(f"Checkpoint saved to {path} (Reward: {reward:.2f})")


def load_checkpoint(policy, optimizer1, critic, optimizer2, path):
    if os.path.exists(path):
        checkpoint = torch.load(path)
        policy.load_state_dict(checkpoint['policy_state_dict'])
        optimizer1.load_state_dict(checkpoint['optimizer1_state_dict'])
        critic.load_state_dict(checkpoint['critic_state_dict'])
        optimizer2.load_state_dict(checkpoint['optimizer2_state_dict'])
        print(f"Loaded checkpoint from {path} (Episode: {checkpoint['episode']}, Reward: {checkpoint['reward']:.2f})")
        return checkpoint['episode'], checkpoint['reward']
    else:
        print(f"No checkpoint found at {path}")
        return 0, 0


# ----------------------------
# 3. 训练函数（带Checkpoint和可视化）
# ----------------------------
def train(env_name="CartPole-v1", hidden_dim=128, lr=1e-3,
          gamma=0.99, max_episodes=10000, print_interval=2):
    env = gym.make(env_name, render_mode="human")  # 开启可视化
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)
    critic = Critic(state_dim, action_dim, hidden_dim).to(device)
    optimizer1 = optim.Adam(policy.parameters(), lr=lr/10.0)
    optimizer2 = optim.Adam(critic.parameters(), lr=lr)

    # 尝试加载Checkpoint
    checkpoint_path = "./actor_critic_checkpoint_150_2955xx.0.pth"
    start_episode, _ = load_checkpoint(policy, optimizer1, critic, optimizer2, checkpoint_path)

    rewards_list = []
    for episode in range(start_episode, max_episodes):
        state = env.reset()
        state = state[0]
        episode_reward = 0
        episode_loss = 0.0
        step_cnt = 0

        while True:
            # randomly sample action from PolicyNet
            state_tensor = torch.FloatTensor(state).to(device)
            state_tensor = state_tensor.unsqueeze(0)
            probs = policy(state_tensor)
            m = Categorical(probs) #根据各种action的概率值probs创建一个离散的概率分布
            action = m.sample() #使用该概率分布进行抽样，得到一个具体的action
            log_probs = m.log_prob(action)

            # perform the action
            next_state, reward, done, _, _ = env.step(action.item())
            episode_reward += reward

            # randomly sample next_action from policynet,but do not perform it
            policy.eval()
            with torch.no_grad():
                state_tensor = torch.FloatTensor(next_state).to(device)
                state_tensor = state_tensor.unsqueeze(0)
                probs = policy(state_tensor)
                m = Categorical(probs)  # 根据各种action的概率值probs创建一个离散的概率分布
                next_action = m.sample()  # 使用该概率分布进行抽样，得到一个具体的action
            policy.train()

            # evaluate value network twice
            state_tensor = torch.FloatTensor(state).to(device)
            state_tensor = state_tensor.unsqueeze(0)
            action_tensor = torch.tensor([0,0], dtype=torch.float, device=device)
            action_tensor[action.item()] = 1.0 # one-hot编码action
            action_tensor = action_tensor.unsqueeze(0)
            qt = critic.forward(state_tensor, action_tensor)

            critic.eval()
            with torch.no_grad():
                state_tensor = torch.FloatTensor(next_state).to(device)
                state_tensor = state_tensor.unsqueeze(0)
                action_tensor = torch.tensor([0, 0], dtype=torch.float, device=device)
                action_tensor[next_action.item()] = 1.0  # one-hot编码action
                action_tensor = action_tensor.unsqueeze(0)
                qt_next = critic.forward(state_tensor, action_tensor)
            critic.train()

            # calculate TD error and update critc network
            loss1 = (qt - (reward + gamma * qt_next)) * (qt - (reward + gamma * qt_next))  / 2.0
            optimizer2.zero_grad()
            loss1.backward()
            optimizer2.step()
            episode_loss += loss1.item()

            # update policy network
            loss2 = -(log_probs * qt.detach()).mean()
            optimizer1.zero_grad()
            loss2.backward()
            optimizer1.step()
            episode_loss += loss2.item()

            state = next_state
            step_cnt += 1
            if done:
                break
            '''到后面模型能力强了，游戏一把玩好久也不死。就导致两次训练的时间间隔很长
            粗暴的截断会有两个问题：
            1、可能导致游戏后面才会出现的states，从来没有出现在训练集里，使得模型失去泛化能力
            2、compute_returns不准确了，这个可能对于密集奖励型游戏还好，毕竟到了后面状态，伽马的n次方接近0，后面的项影响有限
            '''
            '''if  len(rewards) > 1000:
                print('brutally cut')
                break'''

        rewards_list.append(episode_reward)
        if len(rewards_list) > 10000:
            rewards_list = rewards_list[-10000:]
        # 保存Checkpoint（如果回报>1000）
        if episode_reward > 1000:
            checkpoint_path = f"./actor_critic_checkpoint_{episode}_{episode_reward}.pth"
            save_checkpoint(policy, optimizer1, critic, optimizer2,episode, episode_reward, checkpoint_path)

        print(f"Episode {episode + 1}, Reward: {episode_reward:.2f}, Loss: {episode_loss / step_cnt:.4f}")


    env.close()
    return rewards_list


# ----------------------------
# 4. 推理函数（演示训练好的模型）
# ----------------------------
def test(env_name="CartPole-v1", hidden_dim=128, checkpoint_path="checkpoint.pth"):
    env = gym.make(env_name, render_mode="human")  # 开启可视化
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)
    critic = Critic(state_dim, action_dim, hidden_dim).to(device) #占位符，不会被用于推理
    optimizer = optim.Adam(policy.parameters(), lr=0.001)  # 仅占位，实际不用于测试
    optimizer2 = optim.Adam(critic.parameters(), lr=0.001)  # 仅占位，实际不用于测试

    # 加载Checkpoint
    load_checkpoint(policy, optimizer, critic, optimizer2, checkpoint_path)

    print("Starting inference...")
    while True:  # 无限运行直到手动停止
        state = env.reset()[0]
        total_reward = 0

        while True:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).to(device)
                probs = policy(state_tensor)
                action = torch.argmax(probs).item()  # 直接选最优动作

            next_state, reward, done, _, _ = env.step(action)
            total_reward += reward
            state = next_state

            if done:
                print(f"Inference Reward: {total_reward}")
                break



# ----------------------------
# 6. 主函数（命令行参数解析）
# ----------------------------
def main(mode):
    if mode == "train":
        rewards = train()
        plt.plot(rewards)
        plt.xlabel("Episode")
        plt.ylabel("Total Reward")
        plt.title("Policy Gradient Training")
        plt.show()
    elif mode == "test":
        test(checkpoint_path="./actor_critic_checkpoint_1001.0.pth")


if __name__ == "__main__":
    main("train")
```



#### 4.3 实操2 按照李宏毅老师的课程进行训练

下面的代码，模型能够收敛，能上2万多分：

```python
Episode 298, Reward: 372.00, Loss: 1.0105
Checkpoint saved to ./actor_critic_checkpoint_298_26247.0.pth (Reward: 26247.00)
```

代码如下：

```python
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import argparse
import os
import matplotlib.pyplot as plt

# 设置随机种子
torch.manual_seed(42)
np.random.seed(42)

# 检测设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


# ----------------------------
# 1. 策略网络定义
# ----------------------------
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, action_dim)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        if len(x.shape) == 1:
            x = x.unsqueeze(0)  # [state_dim] -> [1, state_dim]
        x = torch.relu(self.fc1(x))  # [batch_size, hidden_dim]
        logits = self.fc2(x)  # [batch_size, action_dim]
        probs = self.softmax(logits)  # [batch_size, action_dim]
        return probs

# 价值网络的定义
class Critic(nn.Module):
    def __init__(self, state_dim=4, hidden_dim=128):
        super(Critic, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)  # 输出状态价值V
        self.relu = nn.ReLU()

    def forward(self, state):
        x = self.relu(self.fc1(state))
        x = self.relu(self.fc2(x))
        value = self.fc3(x)
        return value


# ----------------------------
# 2. Checkpoint 保存与加载
# ----------------------------
def save_checkpoint(policy, optimizer1, critic, optimizer2, episode, reward, path):
    torch.save({
        'policy_state_dict': policy.state_dict(),
        'optimizer1_state_dict': optimizer1.state_dict(),
        'critic_state_dict': critic.state_dict(),
        'optimizer2_state_dict': optimizer2.state_dict(),
        'episode': episode,
        'reward': reward
    }, path)
    print(f"Checkpoint saved to {path} (Reward: {reward:.2f})")


def load_checkpoint(policy, optimizer1, critic, optimizer2, path):
    if os.path.exists(path):
        checkpoint = torch.load(path)
        policy.load_state_dict(checkpoint['policy_state_dict'])
        optimizer1.load_state_dict(checkpoint['optimizer1_state_dict'])
        critic.load_state_dict(checkpoint['critic_state_dict'])
        optimizer2.load_state_dict(checkpoint['optimizer2_state_dict'])
        print(f"Loaded checkpoint from {path} (Episode: {checkpoint['episode']}, Reward: {checkpoint['reward']:.2f})")
        return checkpoint['episode'], checkpoint['reward']
    else:
        print(f"No checkpoint found at {path}")
        return 0, 0


# ----------------------------
# 3. 训练函数（带Checkpoint和可视化）
# ----------------------------
def train(env_name="CartPole-v1", hidden_dim=128, lr=1e-3,
          gamma=0.99, max_episodes=10000, print_interval=2):
    env = gym.make(env_name, render_mode="human")  # 开启可视化
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)
    critic = Critic().to(device)
    optimizer1 = optim.Adam(policy.parameters(), lr=lr)
    optimizer2 = optim.Adam(critic.parameters(), lr=lr)

    # 尝试加载Checkpoint
    checkpoint_path = "./actor_critic_checkpoint_150_2955xx.0.pth"
    start_episode, _ = load_checkpoint(policy, optimizer1, critic, optimizer2, checkpoint_path)

    rewards_list = []
    for episode in range(start_episode, max_episodes):
        state = env.reset()
        state = state[0]
        episode_reward = 0
        episode_loss = 0.0
        step_cnt = 0

        while True:
            # 选择动作
            state_tensor = torch.FloatTensor(state).to(device).unsqueeze(0)
            probs = policy(state_tensor)
            m = Categorical(probs)
            action = m.sample()
            log_prob = m.log_prob(action)

            # 执行动作
            next_state, reward, done, _, _ = env.step(action.item())
            episode_reward += reward

            # 计算V值
            state_value = critic(state_tensor)
            next_state_tensor = torch.FloatTensor(next_state).to(device).unsqueeze(0)
            next_state_value = critic(next_state_tensor).detach()

            # 计算TD误差(优势函数)
            td_error = reward + gamma * next_state_value * (1 - done) - state_value

            # 更新Critic
            critic_loss = td_error.pow(2).mean()
            optimizer2.zero_grad()
            critic_loss.backward()
            optimizer2.step()
            episode_loss += critic_loss.item()

            # 更新Actor
            policy_loss = -log_prob * td_error.detach()
            optimizer1.zero_grad()
            policy_loss.backward()
            optimizer1.step()
            episode_loss += policy_loss.item()

            state = next_state
            step_cnt += 1
            if done:
                break
            '''到后面模型能力强了，游戏一把玩好久也不死。就导致两次训练的时间间隔很长
            粗暴的截断会有两个问题：
            1、可能导致游戏后面才会出现的states，从来没有出现在训练集里，使得模型失去泛化能力
            2、compute_returns不准确了，这个可能对于密集奖励型游戏还好，毕竟到了后面状态，伽马的n次方接近0，后面的项影响有限
            '''
            '''if  len(rewards) > 1000:
                print('brutally cut')
                break'''

        rewards_list.append(episode_reward)
        if len(rewards_list) > 10000:
            rewards_list = rewards_list[-10000:]
        # 保存Checkpoint（如果回报>1000）
        if episode_reward > 1000:
            checkpoint_path = f"./actor_critic_checkpoint_{episode}_{episode_reward}.pth"
            save_checkpoint(policy, optimizer1, critic, optimizer2,episode, episode_reward, checkpoint_path)

        print(f"Episode {episode + 1}, Reward: {episode_reward:.2f}, Loss: {episode_loss / step_cnt:.4f}")


    env.close()
    return rewards_list


# ----------------------------
# 4. 推理函数（演示训练好的模型）
# ----------------------------
def test(env_name="CartPole-v1", hidden_dim=128, checkpoint_path="checkpoint.pth"):
    env = gym.make(env_name, render_mode="human")  # 开启可视化
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)
    critic = Critic().to(device) #占位符，不会被用于推理
    optimizer = optim.Adam(policy.parameters(), lr=0.001)  # 仅占位，实际不用于测试
    optimizer2 = optim.Adam(critic.parameters(), lr=0.001)  # 仅占位，实际不用于测试

    # 加载Checkpoint
    load_checkpoint(policy, optimizer, critic, optimizer2, checkpoint_path)

    print("Starting inference...")
    while True:  # 无限运行直到手动停止
        state = env.reset()[0]
        total_reward = 0

        while True:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).to(device)
                probs = policy(state_tensor)
                action = torch.argmax(probs).item()  # 直接选最优动作

            next_state, reward, done, _, _ = env.step(action)
            total_reward += reward
            state = next_state

            if done:
                print(f"Inference Reward: {total_reward}")
                break



# ----------------------------
# 6. 主函数（命令行参数解析）
# ----------------------------
def main(mode):
    if mode == "train":
        rewards = train()
        plt.plot(rewards)
        plt.xlabel("Episode")
        plt.ylabel("Total Reward")
        plt.title("Policy Gradient Training")
        plt.show()
    elif mode == "test":
        test(checkpoint_path="./actor_critic_checkpoint_1001.0.pth")


if __name__ == "__main__":
    main("train")
```

### 5. AlphaGo的原理

![image-20250327200522090](img/RL/image-20250327200522090.png)

### 6. 蒙特卡洛算法

蒙特卡洛算法是一种通过反复随机抽样以逼近某个数值的数值方法。

![image-20250401190436992](img/RL/image-20250401190436992.png)



![image-20250328160918242](img/RL/image-20250328160918242.png)

### 7. Off-Policy方法与近端策略优化

#### 7.1 原理

1. 把on-policy方法转为off-policy方法，可以更高效，避免必须采样一回合才能训练一回合。
2. 但被训练的策略网络和负责与环境交互的策略网络不是同一个，采样是遵从后者的分布，这时候就需要用重要性采样对梯度计算进行调整。
3. 两者的分布不能相差太远，否则会触发重要性采样的issue，所以PPO会引入KL作为惩罚项



![image-20250329110628801](img/RL/image-20250329110628801.png)

#### 7.2 实操

还是平衡车小游戏：

```python
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import argparse
import os
import matplotlib.pyplot as plt

# 设置随机种子
torch.manual_seed(42)
np.random.seed(42)

# 检测设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


# ----------------------------
# 1. 策略网络定义
# ----------------------------
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, action_dim)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        if len(x.shape) == 1:
            x = x.unsqueeze(0)  # [state_dim] -> [1, state_dim]
        x = torch.relu(self.fc1(x))  # [batch_size, hidden_dim]
        logits = self.fc2(x)  # [batch_size, action_dim]
        probs = self.softmax(logits)  # [batch_size, action_dim]
        return probs


# ----------------------------
# 2. Checkpoint 保存与加载
# ----------------------------
def save_checkpoint(policy, optimizer, episode, reward, path="checkpoint.pth"):
    torch.save({
        'policy_state_dict': policy.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'episode': episode,
        'reward': reward
    }, path)
    print(f"Checkpoint saved to {path} (Reward: {reward:.2f})")


def load_checkpoint(policy, optimizer, path="checkpoint.pth"):
    if os.path.exists(path):
        checkpoint = torch.load(path)
        policy.load_state_dict(checkpoint['policy_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print(f"Loaded checkpoint from {path} (Episode: {checkpoint['episode']}, Reward: {checkpoint['reward']:.2f})")
        return checkpoint['episode'], checkpoint['reward']
    else:
        print(f"No checkpoint found at {path}")
        return 0, 0


# ----------------------------
# 3. 训练函数（带Checkpoint和可视化）
# ----------------------------
def train(env_name="CartPole-v1", hidden_dim=128, lr=1e-3,    # backup  lr=1e-2
          gamma=0.99, max_episodes=1000, print_interval=2):
    env = gym.make(env_name, render_mode="human")  # 开启可视化
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    learner = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device) #负责学习，更新梯度
    worker = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)  #负责与环境交互
    optimizer = optim.Adam(learner.parameters(), lr=lr)

    # 尝试加载Checkpoint
    checkpoint_path = "./ppo_checkpoint_150_2955xx.0.pth"
    start_episode, _ = load_checkpoint(learner, optimizer, checkpoint_path)
    worker.load_state_dict(learner.state_dict())

    episode_rewards = []

    for episode in range(start_episode, max_episodes):

        #用worker收集若干个回合的轨迹
        states, actions, rewards = [], [], []
        while len(actions) < 64:
            state = env.reset()
            state = state[0]
            while True:
                state_tensor = torch.FloatTensor(state).to(device)
                state_tensor = state_tensor.unsqueeze(0)
                probs = worker(state_tensor)
                m = Categorical(probs) #根据各种action的概率值probs创建一个离散的概率分布
                action = m.sample() #使用该概率分布进行抽样，得到一个具体的action

                next_state, reward, done, _, _ = env.step(action.item())

                states.append(state_tensor.squeeze(0))
                actions.append(action.squeeze(0))
                rewards.append(reward)

                state = next_state
                if done:
                    break


        # 计算回报
        total_reward = sum(rewards)
        episode_rewards.append(total_reward)

        returns = compute_returns(rewards, gamma)
        returns = (returns - returns.mean()) / (returns.std() + 1e-9)

        # 反复利用收集到的数据，更新learner策略
        i = 0
        for i in range(5):
            loss, kl = train_policy_network(learner, worker, optimizer, states, actions, returns)
            if abs(kl) > 0.01: # kl散度比较大，说明两个模型之间分布差异偏大，不适合继续训练，需要重新跟环境交互
                break

        worker.load_state_dict(learner.state_dict())

        # 保存Checkpoint（如果回报>1000）
        if total_reward > 1000:
            checkpoint_path = f"./ppo_checkpoint_{episode}_{total_reward}.pth"
            save_checkpoint(learner, optimizer, episode, total_reward, checkpoint_path)



        # 打印进度
        if (episode + 1) % print_interval == 0:
            avg_reward = np.mean(episode_rewards[-print_interval:])
            print(f"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}, Loss: {loss:.4f}")

        # 提前终止
        if len(episode_rewards) >= 100 and np.mean(episode_rewards[-100:]) >= 2000:
            print(f"Solved at Episode {episode + 1}!")
            break

    env.close()
    return episode_rewards


# ----------------------------
# 4. 推理函数（演示训练好的模型）
# ----------------------------
def test(env_name="CartPole-v1", hidden_dim=128, checkpoint_path="checkpoint.pth"):
    env = gym.make(env_name, render_mode="human")  # 开启可视化
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)
    optimizer = optim.Adam(policy.parameters(), lr=0.001)  # 仅占位，实际不用于测试

    # 加载Checkpoint
    load_checkpoint(policy, optimizer, checkpoint_path)

    print("Starting inference...")
    while True:  # 无限运行直到手动停止
        state = env.reset()[0]
        total_reward = 0

        while True:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).to(device)
                probs = policy(state_tensor)
                action = torch.argmax(probs).item()  # 直接选最优动作

            next_state, reward, done, _, _ = env.step(action)
            total_reward += reward
            state = next_state

            if done:
                print(f"Inference Reward: {total_reward}")
                break


# ----------------------------
# 5. 辅助函数， 用于策略梯度更新（替代Q(s,a) 的蒙特卡洛估计）
# ----------------------------
def compute_returns(rewards, gamma=0.99):
    returns = []
    R = 0
    for r in reversed(rewards):
        R = r + gamma * R
        returns.insert(0, R)
    return torch.tensor(returns, device=device)

def importance_sample_and_kl(learner, worker, states, actions, epsilon=0.2):
    # 计算当前策略（learner）下的动作概率
    with torch.no_grad():
        probs = learner(states)  # [T, action_dim]
    pa1 = probs[torch.arange(probs.size(0)), actions]  # 当前策略下，动作a的概率

    # 计算旧策略（worker）下的动作概率
    with torch.no_grad():
        old_probs = worker(states)  # [T, action_dim]
    pa2 = old_probs[torch.arange(probs.size(0)), actions]  # 旧策略下，动作a的概率

    # 计算重要性采样比率: r_t = pi_new(a|s) / pi_old(a|s)
    importance_ratio = pa1 / (pa2 + 1e-6)

    # 计算KL散度
    log_pa1 = torch.log(pa1 + 1e-6)  # 当前策略下的log概率
    log_pa2 = torch.log(pa2 + 1e-6)  # 旧策略下的log概率
    kl_divergence = torch.mean( pa2 * (log_pa2 - log_pa1)  )  # KL散度

    return importance_ratio, kl_divergence



def train_policy_network(learner, worker, optimizer, states, actions, returns):
    # 将列表中的状态/动作/回报堆叠成张量， 假设一把游戏玩下来的状态个数是T
    states = torch.stack(states)  # [T, 4]
    actions = torch.stack(actions) # [T]
    returns = returns  # [T]
    # 1. 通过策略网络计算动作概率
    probs = learner(states)  # [T, action_dim]
    # 2. 创建分类分布（用于采样和计算对数概率）
    m = Categorical(probs)
    # 3. 计算所选动作的对数概率
    log_probs = m.log_prob(actions)  # [T,]
    importance_ratio, kl = importance_sample_and_kl(learner, worker, states, actions)
    importance_ratio = torch.clamp(importance_ratio, 1 - 0.2, 1 + 0.2)
    # 4. 因为基于策略的强化学习要使用梯度上升使得state-value函数的期望最大化，所以损失函数是期望值的负数
    # returns已经在函数外面进行了带折扣的汇总运算，也就是已经是U了，不是每一步的r
    loss = -(log_probs * returns*importance_ratio).mean() + 0.01 * kl
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss.item(), kl


# ----------------------------
# 6. 主函数（命令行参数解析）
# ----------------------------
def main(mode):
    if mode == "train":
        rewards = train()
        plt.plot(rewards)
        plt.xlabel("Episode")
        plt.ylabel("Total Reward")
        plt.title("Policy Gradient Training")
        plt.show()
    elif mode == "test":
        test(checkpoint_path="./pbrl_checkpoint_1001.0.pth")


if __name__ == "__main__":
    main("train")
```

感觉收敛会比之前的On Policy的方法要慢一些：

```shell
Episode 218, Avg Reward: 384.00, Loss: -0.0044
Checkpoint saved to ./ppo_checkpoint_219_1834.0.pth (Reward: 1834.00)
Episode 220, Avg Reward: 1230.50, Loss: -0.0003
```



### 8. imitation learning

![image-20250327142459868](img/RL/image-20250327142459868.png)

### 9. RLHF

![image-20250329203734345](img/RL/image-20250329203734345.png)

### 10. 马尔可夫决策过程(MDP)与动态规划(DP)

#### 10.1 原理

##### 10.1.1 基本概念

![image-20250331210825112](img/RL/image-20250331210825112.png)

#### 10.2 如何用动态规划进行预测与控制

![image-20250401103901779](img/RL/image-20250401103901779.png)

思考：

![image-20250403200338963](img/RL/image-20250403200338963.png)

##### 10.2.1 实操1  用动态规划实现GridWorld的预测与控制

![image-20250401090243435](img/RL/image-20250401090243435.png)

代码如下，包括预测与控制：

```python
import random
import torch
from tqdm import tqdm

# 这里环境的代码之所以我自己直接写，说明
# 1. 环境对我们是已知的，也就是状态变迁概率函数和奖励函数是完全掌握额
# 2. 说明我们不需要真的跟环境交互，用自己的代码来描述环境的状态变迁和奖励
class GridWorldEnv:
    def __init__(self):
        self.GRID_NUM = 5
        self.ACTION_SPACE = {1:'UP', 2:'DOWN', 3:'LEFT',4:'RIGHT'}
        self.current_x = 0
        self.current_y = 0
        self.A = (1, 0)
        self.B = (3, 0)
        self.AA = (1, 4)
        self.BB = (3, 2)
        self.gamma = 0.9

    def reset(self):
        self.current_x = random.randint(0, self.GRID_NUM-1)
        self.current_y = random.randint(0, self.GRID_NUM - 1)
        return (self.current_x, self.current_y)

    def step(self, action):
        reward = 0
        if not action in self.ACTION_SPACE:
            print(f"invalid action {action}")
            return (self.current_x, self.current_y), reward, False # new_state, reward, done
        if (self.current_x, self.current_y) == self.A:
            self.current_x, self.current_y = self.AA
            reward = 10
            return (self.current_x, self.current_y), reward, False
        if (self.current_x, self.current_y) == self.B:
            self.current_x, self.current_y = self.BB
            reward = 5
            return (self.current_x, self.current_y), reward, False
        if action == 1 and self.current_y == 0: # up but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 2 and self.current_y == (self.GRID_NUM-1): # down but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 3 and self.current_x == 0: # left but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 4 and self.current_x == (self.GRID_NUM-1): # left but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 1: # up
            self.current_y -= 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 2: # down
            self.current_y += 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 3: # left
            self.current_x -= 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 4: # right
            self.current_x += 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
	#额外赠送：蒙特卡洛方式进行预测
    def monte_carlo_update(self):
        v = torch.zeros(self.GRID_NUM, self.GRID_NUM, dtype=torch.float32)
        return_sum = torch.zeros(self.GRID_NUM, self.GRID_NUM, dtype=torch.float32)
        return_cnt = torch.zeros(self.GRID_NUM, self.GRID_NUM, dtype=torch.float32)

        for episode in range(1000): # 1000个回合
            self.reset()
            # 抽样很多了就换个起点，这是一个优化，可以去掉
            if return_cnt[self.current_y, self.current_x] > 1000:
                continue

            # walk 1000 steps，得到一个回合的步骤
            trajectory = []
            for i in range(500): #每个回合最多500步
                from_x, from_y = self.current_x, self.current_y
                action = random.randint(1,4)
                _, r, _ = self.step(action)
                trajectory.append((from_x, from_y, r))

                # 如果已经有了100步，且当前状态的抽样已经很多了。这是一个优化，可以去掉
                if len(trajectory) > 100 and return_cnt[self.current_y, self.current_x] > 200: #抽样很多了就可以不继续往下走
                    G = return_sum[self.current_y, self.current_x] / return_cnt[self.current_y, self.current_x]
                    trajectory.append((self.current_x, self.current_y, G ))
                    break

            # calculate returns
            G = 0
            for (x, y, r) in reversed(trajectory):
                G = r + G * self.gamma
                return_sum[y, x] += G
                return_cnt[y, x] += 1

        for y in range(self.GRID_NUM):
            for x in range(self.GRID_NUM):
                if return_cnt[y, x] > 0:
                    v[y, x] = return_sum[y, x] / return_cnt[y, x]

        for y in range(self.GRID_NUM):
            for x in range(self.GRID_NUM):
                print(f"{v[y, x]:.2f} ", end="")
            print("")
        return v
 ########################### 进行预测 ##################################
	#动态规划对 等概率随机动作的策略 进行预测
    def calculate_V(self):
        v = torch.zeros(self.GRID_NUM, self.GRID_NUM, dtype=torch.float32)

        for episode in range(1000):#最多迭代这么多次
            deltaBig = False
            self.reset()
            old_v = v.clone()
            #每一个状态
            for y in range(self.GRID_NUM):
                for x in range(self.GRID_NUM):
                    #每一个动作
                    newvalue = 0
                    for action in [1,2,3,4]:
                        #强制设置当前位置在(y,x)处，并做出一个行动
                        self.current_y = y
                        self.current_x = x
                        _, r, _ = self.step(action)
                        newvalue += 0.25 * (self.gamma * old_v[self.current_y, self.current_x] + r)
                    if (newvalue-old_v[y, x])*(newvalue-old_v[y, x]) > 0.000001:
                        deltaBig = True
                    v[y, x] = newvalue
            if not deltaBig: #早停
                break

        for y in range(self.GRID_NUM):
            for x in range(self.GRID_NUM):
                print(f"{v[y, x]:.2f} ", end="")
            print("")
        return v
################## 进行控制 #################################
    # Q*
    def calculate_Q(self):
        self.Q = torch.zeros(self.GRID_NUM, self.GRID_NUM, len(self.ACTION_SPACE))
        for episode in range(1000): #最多1000轮
            delta = 0#统计本轮最大的迭代变化绝对值
            old_Q = self.Q.clone()
            #每一个状态
            for y in range(self.GRID_NUM):
                for x in range(self.GRID_NUM):
                    #每一个动作
                    for action in range( len(self.ACTION_SPACE)):
                        # 强制设置当前位置在(y,x)处，并做出一个行动
                        self.current_y = y
                        self.current_x = x
                        (next_x, next_y), r, _ = self.step(action+1)
                        max_next_Q = old_Q[next_y, next_x].max().item()
                        self.Q[y, x, action] = self.gamma * max_next_Q + r

                        if (old_Q[y, x, action] - (self.gamma * max_next_Q + r)).abs() > delta:
                            delta = (old_Q[y, x, action] - (self.gamma * max_next_Q + r)).abs().item()

            if (delta * delta) < 0.000001:
                break

        for y in range(self.GRID_NUM):
            for x in range(self.GRID_NUM):
                print("(", end="")
                for action in range(len(self.ACTION_SPACE)):
                    print(f"{self.Q[y, x, action]:.1f},", end="")
                print(") ", end="")
            print("")
        return self.Q
    
#根据Q* 找到每个状态下回报最大的动作
    def extract_best_policy(self):
        self.Pi = torch.zeros(self.GRID_NUM, self.GRID_NUM, dtype=torch.int32)
        arrows = {0: '↑', 1: '↓', 2: '←', 3: '→'}

        for y in range(self.GRID_NUM):
            for x in range(self.GRID_NUM):
                max_Q_action = self.Q[y, x].argmax().item()
                self.Pi[y, x] = max_Q_action+1
                print(f"{arrows[max_Q_action]} ", end="")
            print("")


env = GridWorldEnv()
print("dynamic plan:")
env.calculate_V()
print("monte carlo:")
env.monte_carlo_update()
print("\nQ* :")
env.calculate_Q()
print("\nbest policy:")
env.extract_best_policy()
```

【思考】：

![image-20250403154309774](img/RL/image-20250403154309774.png)

##### 10.2.2 实操2 策略迭代法进行控制

还是实操1的toy problem，策略迭代法进行最优策略求解：

![image-20250401161829675](img/RL/image-20250401161829675.png)



代码如下：

```python
import random
import torch
from tqdm import tqdm

class GridWorldEnv:
    def __init__(self):
        self.GRID_NUM = 5
        self.ACTION_SPACE = {1:'UP', 2:'DOWN', 3:'LEFT',4:'RIGHT'}
        self.current_x = 0
        self.current_y = 0
        self.A = (1, 0)
        self.B = (3, 0)
        self.AA = (1, 4)
        self.BB = (3, 2)
        self.gamma = 0.9

    def reset(self):
        self.current_x = random.randint(0, self.GRID_NUM-1)
        self.current_y = random.randint(0, self.GRID_NUM - 1)
        return (self.current_x, self.current_y)

    def step(self, action):
        reward = 0
        if not action in self.ACTION_SPACE:
            print(f"invalid action {action}")
            return (self.current_x, self.current_y), reward, False # new_state, reward, done
        if (self.current_x, self.current_y) == self.A:
            self.current_x, self.current_y = self.AA
            reward = 10
            return (self.current_x, self.current_y), reward, False
        if (self.current_x, self.current_y) == self.B:
            self.current_x, self.current_y = self.BB
            reward = 5
            return (self.current_x, self.current_y), reward, False
        if action == 1 and self.current_y == 0: # up but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 2 and self.current_y == (self.GRID_NUM-1): # down but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 3 and self.current_x == 0: # left but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 4 and self.current_x == (self.GRID_NUM-1): # left but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 1: # up
            self.current_y -= 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 2: # down
            self.current_y += 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 3: # left
            self.current_x -= 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 4: # right
            self.current_x += 1
            reward = 0
            return (self.current_x, self.current_y), reward, False


    def policy_iterate(self, theta=1e-4):
        """ 使用策略迭代求解最优策略 """
        V = torch.zeros((self.GRID_NUM, self.GRID_NUM), dtype=torch.float32)  # 状态值函数 V(s)
        policy = torch.zeros((self.GRID_NUM, self.GRID_NUM), dtype=torch.int32)  # 初始随机策略

        while True:
            # 1. 策略评估 (Policy Evaluation)
            while True:
                delta = 0
                V_old = V.clone()

                for x in range(self.GRID_NUM):
                    for y in range(self.GRID_NUM):
                        self.current_x = x
                        self.current_y = y
                        action = policy[y, x].item()
                        (next_x, next_y), reward, _ = self.step( action+1)
                        V[y, x] = reward + self.gamma * V[next_y, next_x]
                        delta = max(delta, abs(V[y, x] - V_old[y, x]))

                if delta < theta:  # 价值函数收敛
                    break

            # 2. 策略改进 (Policy Improvement)
            policy_stable = True

            for y in range(self.GRID_NUM):
                for x in range(self.GRID_NUM):
                    old_action = policy[y, x].item()

                    # 计算 Q(s, a) 并选择最优动作
                    Q_values = []
                    for action in range(4):
                        self.current_x = x
                        self.current_y = y
                        (next_x, next_y), reward, _ = self.step( action+1)
                        Q_values.append(reward + self.gamma * V[next_y, next_x])

                    best_action = torch.tensor(Q_values).argmax().item()
                    policy[y, x] = best_action  # 选择最优动作

                    if old_action != best_action:
                        policy_stable = False  # 策略仍在变化

            if policy_stable:  # 当策略不再变化时，停止迭代
                break

        return policy, V

env = GridWorldEnv()
policy, v = env.policy_iterate()
arrows = {0: '↑', 1: '↓', 2: '←', 3: '→'}
for y in range(env.GRID_NUM):
    for x in range(env.GRID_NUM):
        action = policy[y, x].item()
        print(f"{arrows[action]} ", end="")
    print("")
```

##### 10.2.3 实操3 价值迭代法进行控制

还是实操1的toy problem，价值迭代法进行最优策略求解：

![image-20250401172732086](img/RL/image-20250401172732086.png)

代码如下：

```python
import random
import torch
from tqdm import tqdm

class GridWorldEnv:
    def __init__(self):
        self.GRID_NUM = 5
        self.ACTION_SPACE = {1:'UP', 2:'DOWN', 3:'LEFT',4:'RIGHT'}
        self.current_x = 0
        self.current_y = 0
        self.A = (1, 0)
        self.B = (3, 0)
        self.AA = (1, 4)
        self.BB = (3, 2)
        self.gamma = 0.9

    def reset(self):
        self.current_x = random.randint(0, self.GRID_NUM-1)
        self.current_y = random.randint(0, self.GRID_NUM - 1)
        return (self.current_x, self.current_y)

    def step(self, action):
        reward = 0
        if not action in self.ACTION_SPACE:
            print(f"invalid action {action}")
            return (self.current_x, self.current_y), reward, False # new_state, reward, done
        if (self.current_x, self.current_y) == self.A:
            self.current_x, self.current_y = self.AA
            reward = 10
            return (self.current_x, self.current_y), reward, False
        if (self.current_x, self.current_y) == self.B:
            self.current_x, self.current_y = self.BB
            reward = 5
            return (self.current_x, self.current_y), reward, False
        if action == 1 and self.current_y == 0: # up but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 2 and self.current_y == (self.GRID_NUM-1): # down but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 3 and self.current_x == 0: # left but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 4 and self.current_x == (self.GRID_NUM-1): # left but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 1: # up
            self.current_y -= 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 2: # down
            self.current_y += 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 3: # left
            self.current_x -= 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 4: # right
            self.current_x += 1
            reward = 0
            return (self.current_x, self.current_y), reward, False


    def value_iterate(self):
        V = torch.zeros((self.GRID_NUM, self.GRID_NUM), dtype=torch.float32)
        for k in range(100):
            Q = torch.zeros((self.GRID_NUM, self.GRID_NUM, 4), dtype=torch.float32)
            old_V = V.clone()
            for x in range(self.GRID_NUM):
                for y in range(self.GRID_NUM):
                    for action  in range(4):
                        self.current_x = x
                        self.current_y = y
                        (next_x, next_y), reward, _ = self.step(action + 1)
                        Q[y, x, action] = reward + self.gamma * V[next_y, next_x].item()
                    V[y,x] = Q[y,x].max().item()
            if torch.dist(V, old_V, p=2).item() < 0.000001:#这个对于性能提升很重要
                break
        policy = torch.zeros((self.GRID_NUM, self.GRID_NUM), dtype=torch.int32)
        for x in range(self.GRID_NUM):
            for y in range(self.GRID_NUM):
                vlist = []
                for action in range(4):
                    self.current_x = x
                    self.current_y = y
                    (next_x, next_y), reward, _ = self.step(action + 1)
                    vlist.append( reward + self.gamma * V[next_y, next_x].item() )
                a = torch.tensor(vlist).argmax().item()
                policy[y,x] = a
        return policy, V



env = GridWorldEnv()
print("value iterate result:")
policy, v = env.value_iterate()
arrows = {0: '↑', 1: '↓', 2: '←', 3: '→'}
for y in range(env.GRID_NUM):
    for x in range(env.GRID_NUM):
        action = policy[y, x].item()
        print(f"{arrows[action]} ", end="")
    print("")

```

#### 10.3 刻意练习

动态规划和马尔可夫决策过程，我一直没有入脑入心。所以安排刻意练习的环节

##### 10.3.1 练习1

学习到的一个经验就是：

1. Value的初始化会影响到是否收敛。不能照搬书上的算法把它初始化为0.
2. reward的设计会影响到是否收敛，例如下面的问题里，对于不能移动的情况的耗时，必须要比较大才会收敛。而前面cliffwalking的例子，跌下悬崖太大又会导致不能收敛...
3. reward不是说都要归一化，不是要都在同一个量级。有时候在同一量级，会导致不收敛。下面的练习题里，不能移动的情况下cost要比较大才能收敛



【练习的问题是：】

在一个n*m的矩阵里,只有一个格子是字符X,表示pizza店所在地。 $表示写字楼位置，需要把pizza送到写字楼。 

在矩阵里: 

1. 字符‘$'表示该格子是一座写字楼 ** (注意,在本题中,pizza店本身也是写字楼) ** ,里面的客户订 了一个pizza。
2. 字符'0’~'9'表示该格子是空地,同时表示该空地的高度。每个空地格子的高度范围为[0,9]。 送餐员可以从当前格子往 上、下、左、右四个方向移动一格。当然根据高度的不同,移动一次的时间也不同。 

你能从A格子进入B格子的条件是: 

1. A、B两个格子相邻 ** (有一条公共边) **; 
2. 关于移动方法： 
   1. 如果A、B均为空地: 
      1. 如果A、B高度相等,那么花费的时间是1; 
      2. 如果A、B高度差为1,那么花费的时间是3;
      3. 如果高度差大于1,则不能从A走到B。
   2. 如果A与B当中,至少有一个格子是写字楼(包括两个格子都是写字楼的情况， X也是写字楼) 花费的时间是2。(如果A或B当中有空地的话,不需要考虑A或B的高度) 注意:送餐员可以进入写字楼,即使他不是给该写字楼的客户送pizza(也就是可以借道)。

 矩阵如下，大小为3行7列： 

```shell
3442211 
34$221X 
3442211 
```

请实现了动态规划的策略迭代方法，为送餐员找到最合适的行路策略，使得送餐时间最短。代码如下：

```c++
#include <stdlib.h>
#include <stdio.h>
#include <string.h>
#include <vector>
#include <utility>
#include <string>
#include <stdint.h>
#include <cmath>
#include <climits>
using namespace std;

#define MAX_SZ (50)

int row, col; // 实际的地图大小
int start_x, start_y; // pizza店位置
int dest_x, dest_y; // 送餐的位置
char arr[MAX_SZ][MAX_SZ]; // 地图
char policy[MAX_SZ][MAX_SZ]; // 策略，action: 0-up, 1-down, 2-left, 3-right
float gamma_w = 1.0; // 折扣率设为1.0，因为我们想要准确的总成本
const int CANNOT_MOVE_COST = INT_MAX/2; // 不能移动时的成本（设为一个大数） 这个值也会影响到是否收敛！！

// action: 0-up, 1-down, 2-left, 3-right
int env_step(int from_x, int from_y, int action, int & next_x, int & next_y, int &cost)
{
    // 边界检查
    if (from_x < 0 || from_y < 0 || from_x >= col || from_y >= row || action < 0 || action > 3)
    {
        printf("invalid argument at %d ! %d, %d, %d\n", __LINE__, from_x, from_y, action);
        return -1;
    }

    // 检查是否不能移动
    if ((from_x == 0 && action == 2) ||  // 左边界且向左移动
        (from_y == 0 && action == 0) ||  // 上边界且向上移动
        (from_x == col-1 && action == 3) ||  // 右边界且向右移动
        (from_y == row-1 && action == 1))  // 下边界且向下移动
    {
        next_x = from_x;
        next_y = from_y;
        cost = CANNOT_MOVE_COST;
        return 1;
    }

    // 计算目标位置
    if (action == 0) next_y = from_y-1, next_x = from_x; // up
    else if (action == 1) next_y = from_y+1, next_x = from_x; // down
    else if (action == 2) next_x = from_x-1, next_y = from_y; // left
    else if (action == 3) next_x = from_x+1, next_y = from_y; // right

    char B = arr[next_y][next_x];
    char A = arr[from_y][from_x];

    // 处理写字楼情况（包括X和$）
    if (A == '$' || B == '$' || A == 'X' || B == 'X')
    {
        cost = 2; // 穿过写字楼耗时2
        return 0;
    }
    
    // 处理空地情况
    if ('0' <= A && A <= '9' && '0' <= B && B <= '9')
    {
        int aa = A - '0';
        int bb = B - '0';
        int diff = abs(aa - bb);

        if (diff == 0) {
            cost = 1; // 高度相同耗时1
            return 0;
        } else if (diff == 1) {
            cost = 3; // 高度差1耗时3
            return 0;
        } else {
            next_x = from_x;
            next_y = from_y;
            cost = CANNOT_MOVE_COST;
            return 1; // 不能移动
        }
    }

    printf("invalid state at [%d,%d] and [%d,%d]\n", from_y, from_x, next_y, next_x);
    return -1;
}

void print_arr()
{
    for (int i = 0; i < row; ++i)
    {
        for (int j = 0; j < col; ++j)
        {
            printf("%c", arr[i][j]);
        }
        printf("\n");
    }
    printf("\n");
}

void print_value(float v[MAX_SZ][MAX_SZ])
{
    for (int i = 0; i < row; ++i)
    {
        for (int j = 0; j < col; ++j)
        {
            if (v[i][j] > CANNOT_MOVE_COST/2) printf("  INF ");
            else printf("%5.1f ", v[i][j]);
        }
        printf("\n");
    }
    printf("\n");
}

void print_policy()
{
    for (int i = 0; i < row; ++i)
    {
        for (int j = 0; j < col; ++j)
        {
            if (i == dest_y && j == dest_x) {
                printf("  G "); // 目标位置
                continue;
            }
            
            switch(policy[i][j]) {
                case 0: printf("  ↑ "); break;
                case 1: printf("  ↓ "); break;
                case 2: printf("  ← "); break;
                case 3: printf("  → "); break;
                default: printf("  ? "); break;
            }
        }
        printf("\n");
    }
    printf("\n");
}

int policy_iteration()
{
    float value[MAX_SZ][MAX_SZ];
    // 初始化值函数为一个大数（类似无穷大），因为我们最后是选择成本最低的动作，而不是选择回报最大的动作。
    for (int i = 0; i < row; ++i) {
        for (int j = 0; j < col; ++j) {
            value[i][j] = CANNOT_MOVE_COST; //初始化为0也可以收敛，但这里初始化比较大会可靠一些
        }
    }
    // 终止状态值为0
    value[dest_y][dest_x] = 0;
    
    memset(policy, 0, sizeof(policy));
    
    int iteration = 0;
    while (1)
    {
        printf("Iteration %d:\n", ++iteration);
        
        // 策略评估
        float delta;
        do {
            delta = 0.0;
            for (int i = 0; i < row; ++i)
            {
                for (int j = 0; j < col; ++j)
                {
                    if (i == dest_y && j == dest_x) continue; // 跳过终止状态
                    
                    int from_x = j, from_y = i;
                    int next_x, next_y, cost;
                    float old_v = value[from_y][from_x];
                    
                    // 执行当前策略的动作
                    int action = policy[from_y][from_x];
                    int ret = env_step(from_x, from_y, action, next_x, next_y, cost);
                    
                    if (ret == 0) {
                        // 最小化总成本：当前成本 + 下一步的期望成本
                        value[from_y][from_x] = cost + gamma_w * value[next_y][next_x];
                        delta = max(delta, abs(value[from_y][from_x] - old_v));
                    } else {
                        value[from_y][from_x] = CANNOT_MOVE_COST;
                    }
                }
            }
        } while (delta > 0.01); // 收敛阈值

        printf("Value function after evaluation:\n");
        print_value(value);

        // 策略改进
        int stable = 1;
        for (int i = 0; i < row; ++i)
        {
            for (int j = 0; j < col; ++j)
            {
                if (i == dest_y && j == dest_x) continue; // 跳过终止状态
                
                int old_action = policy[i][j];
                int from_x = j, from_y = i;
                float min_q = CANNOT_MOVE_COST;
                int best_action = old_action;
                
                // 测试所有可能的动作，寻找最小成本的动作
                for (int a = 0; a < 4; ++a)
                {
                    int next_x, next_y, cost;
                    if (env_step(from_x, from_y, a, next_x, next_y, cost) == 0)
                    {
                        float q = cost + gamma_w * value[next_y][next_x];
                        if (q < min_q) { //不一样的地方，这里要取最小的值作为最佳动作
                            min_q = q;
                            best_action = a;
                        }
                    }
                }
                
                policy[i][j] = best_action;
                if (best_action != old_action) {
                    stable = 0;
                }
            }
        }

        printf("Policy after improvement:\n");
        print_policy();

        if (stable) break;
    }

    return 0;
}

int main()
{
    // 测试数据
    const char * map_data[] = {
        "3442211",
        "34$221X",
        "3442211"
    };
    
    row = 3;
    col = 7;
    
    // 初始化地图
    for (int i = 0; i < row; ++i)
    {
        for (int j = 0; j < col; ++j)
        {
            arr[i][j] = map_data[i][j];
            if (arr[i][j] == 'X') {
                start_x = j;
                start_y = i;
            } else if (arr[i][j] == '$') {
                dest_x = j;
                dest_y = i;
            }
        }
    }
    
    printf("Initial map:\n");
    print_arr();
    
    printf("Pizza shop at (%d, %d)\n", start_x, start_y);
    printf("Destination at (%d, %d)\n", dest_x, dest_y);
    
    policy_iteration();
    
    printf("Final optimal policy:\n");
    print_policy();
    
    // 打印从起点到终点的路径
    printf("Path from pizza shop to destination:\n");
    int x = start_x, y = start_y;
    while (!(x == dest_x && y == dest_y)) {
        printf("(%d, %d) -> ", x, y);
        int action = policy[y][x];
        if (action == 0) y--;
        else if (action == 1) y++;
        else if (action == 2) x--;
        else if (action == 3) x++;
    }
    printf("(%d, %d) [Destination]\n", dest_x, dest_y);
    
    return 0;
}
```

可能AI写的这份代码可读性更好：

```c++
#include <iostream>
#include <vector>
#include <climits>
#include <cmath>
#include <algorithm>
#include <iomanip>
#include <unordered_map>

using namespace std;

// 定义方向：上、下、左、右
const vector<pair<int, int>> DIRECTIONS = {{-1, 0}, {1, 0}, {0, -1}, {0, 1}};
const vector<string> DIRECTION_SYMBOLS = {"↑", "↓", "←", "→"};

struct Cell {
    char type;  // 'X': pizza店, '$': 写字楼, '0'-'9': 空地
    int height; // 如果是空地，存储高度
};

class PizzaDeliverySolver {
private:
    vector<vector<Cell>> grid;
    int rows;
    int cols;
    pair<int, int> pizza_shop;
    vector<pair<int, int>> office_buildings;
    vector<vector<double>> value_function;
    vector<vector<int>> policy;

public:
    PizzaDeliverySolver(const vector<vector<Cell>>& grid) : grid(grid) {
        rows = grid.size();
        cols = rows > 0 ? grid[0].size() : 0;
        findSpecialLocations();
        initializeValueAndPolicy();
    }

    void solve() {
        bool policy_stable = false;
        int iterations = 0;

        while (!policy_stable) {
            iterations++;
            policy_stable = true;

            // 策略评估
            evaluatePolicy();

            // 策略改进
            policy_stable = improvePolicy();
        }

        cout << "Policy Iteration completed in " << iterations << " iterations.\n";
    }

    void printResults() const {
        printPolicyTable();
        printOptimalPath();
    }

private:
    void findSpecialLocations() {
        for (int i = 0; i < rows; ++i) {
            for (int j = 0; j < cols; ++j) {
                if (grid[i][j].type == 'X') {
                    pizza_shop = {i, j};
                } else if (grid[i][j].type == '$') {
                    office_buildings.emplace_back(i, j);
                }
            }
        }
    }

    void initializeValueAndPolicy() {
        value_function.assign(rows, vector<double>(cols, INT_MAX)); //必须初始化比较大一点的值，0的话不收敛。
        policy.assign(rows, vector<int>(cols, -1));

        // 设置目标状态的值函数为0
        for (const auto& office : office_buildings) {
            value_function[office.first][office.second] = 0;
        }
    }

    void evaluatePolicy() {
        bool value_changed;
        do {
            value_changed = false;
            for (int i = 0; i < rows; ++i) {
                for (int j = 0; j < cols; ++j) {
                    if (isTargetLocation(i, j)) continue;

                    double old_value = value_function[i][j];
                    double new_value = INT_MAX;

                    for (int d = 0; d < DIRECTIONS.size(); ++d) {
                        auto di = DIRECTIONS[d].first;
                        auto dj = DIRECTIONS[d].second;
                        int ni = i + di;
                        int nj = j + dj;

                        if (!isValidMove(ni, nj)) continue;

                        double cost = calculateMoveCost(i, j, ni, nj);
                        double candidate_value = cost + value_function[ni][nj];
                        new_value = min(new_value, candidate_value);
                    }

                    if (new_value < old_value) {
                        value_function[i][j] = new_value;
                        value_changed = true;
                    }
                }
            }
        } while (value_changed);
    }

    bool improvePolicy() {
        bool policy_stable = true;

        for (int i = 0; i < rows; ++i) {
            for (int j = 0; j < cols; ++j) {
                if (isTargetLocation(i, j)) continue;

                int old_action = policy[i][j];
                int best_action = -1;
                double min_value = INT_MAX;

                for (int d = 0; d < DIRECTIONS.size(); ++d) {
                    auto di = DIRECTIONS[d].first;
                    auto dj = DIRECTIONS[d].second;
                    int ni = i + di;
                    int nj = j + dj;

                    if (!isValidMove(ni, nj)) continue;

                    double cost = calculateMoveCost(i, j, ni, nj);
                    double candidate_value = cost + value_function[ni][nj];

                    if (candidate_value < min_value) {
                        min_value = candidate_value;
                        best_action = d;
                    }
                }

                if (best_action != -1 && best_action != old_action) {
                    policy[i][j] = best_action;
                    policy_stable = false;
                }
            }
        }

        return policy_stable;
    }

    bool isTargetLocation(int i, int j) const {
        for (const auto& office : office_buildings) {
            if (i == office.first && j == office.second) {
                return true;
            }
        }
        return false;
    }

    bool isValidMove(int i, int j) const {
        return i >= 0 && i < rows && j >= 0 && j < cols;
    }

    double calculateMoveCost(int i, int j, int ni, int nj) const {
        // 如果任一方是写字楼或pizza店
        if (grid[i][j].type == '$' || grid[i][j].type == 'X' || 
            grid[ni][nj].type == '$' || grid[ni][nj].type == 'X') {
            return 2;
        }
        
        // 否则计算高度差
        int height_diff = abs(grid[i][j].height - grid[ni][nj].height);
        if (height_diff == 0) return 1;
        if (height_diff == 1) return 3;
        return INT_MAX; // 不能移动
    }

    void printPolicyTable() const {
        cout << "\nOptimal Policy Table:\n";
        cout << "Each cell shows the best action to take (↑, ↓, ←, →)\n";
        cout << "X: Pizza shop, $: Office building\n\n";

        for (int i = 0; i < rows; ++i) {
            for (int j = 0; j < cols; ++j) {
                if (grid[i][j].type == 'X') {
                    cout << " X ";
                } else if (grid[i][j].type == '$') {
                    cout << " $ ";
                } else {
                    if (policy[i][j] == -1) {
                        cout << " - ";
                    } else {
                        cout << " " << DIRECTION_SYMBOLS[policy[i][j]] << " ";
                    }
                }
            }
            cout << endl;
        }
    }

    void printOptimalPath() const {
        vector<pair<int, int>> path;
        vector<string> directions;
        
        int x = pizza_shop.first;
        int y = pizza_shop.second;
        path.emplace_back(x, y);

        while (!isTargetLocation(x, y)) {
            int action = policy[x][y];
            if (action == -1) {
                cout << "\nNo valid path found from pizza shop to office building!\n";
                return;
            }

    
            auto dx = DIRECTIONS[action].first;
            auto dy = DIRECTIONS[action].second;
            x += dx;
            y += dy;
            path.emplace_back(x, y);
            directions.push_back(DIRECTION_SYMBOLS[action]);
        }

        cout << "\nOptimal Path from Pizza Shop to Office Building:\n";
        for (size_t i = 0; i < path.size(); ++i) {
            cout << "(" << path[i].first << "," << path[i].second << ")";
            if (i < directions.size()) {
                cout << " -> " << directions[i] << " -> ";
            }
        }
        cout << "\nTotal delivery time: " << value_function[pizza_shop.first][pizza_shop.second] << endl;
    }
};

int main() {
    // 定义网格
    vector<string> grid_str = {
        "3442211",
        "34$221X",
        "3442211"
    };
    
    // 转换为Cell结构
    vector<vector<Cell>> grid(grid_str.size(), vector<Cell>(grid_str[0].size()));
    for (int i = 0; i < grid_str.size(); ++i) {
        for (int j = 0; j < grid_str[i].size(); ++j) {
            grid[i][j].type = grid_str[i][j];
            if (grid_str[i][j] >= '0' && grid_str[i][j] <= '9') {
                grid[i][j].height = grid_str[i][j] - '0';
            } else {
                grid[i][j].height = 0;
            }
        }
    }
    
    // 创建并运行求解器
    PizzaDeliverySolver solver(grid);
    solver.solve();
    solver.printResults();
    
    return 0;
}
```

##### 10.3.2 练习2

![image-20250404114946730](img/RL/image-20250404114946730.png)

代码如下，可以收敛：

```python
import torch


class GridWorld:
    def __init__(self):
        self.size = 4
        self.cannot_move_reward = -1
        self.terminator1 = (0, 0)
        self.terminator2 = (self.size-1, self.size-1)
        return

    def step(self, state, action):
        x,y = state
        if x < 0 or x >= self.size or y < 0 or y >= self.size or action <0 or action > 3:
            raise ValueError(f" invalid arguments")
        next_x = x
        next_y = y
        reward = -1
        if action == 0: # up
            next_y = y-1
        if action == 1: # down
            next_y = y+1
        if action == 2: # left
            next_x = x-1
        if action == 3: #right
            next_x = x+1
        if next_x < 0 or next_x >= self.size:
            next_x = x
            reward = self.cannot_move_reward
        if next_y < 0 or next_y >= self.size:
            next_y = y
            reward = self.cannot_move_reward

        done = False
        if (next_y, next_x) == self.terminator1 or (next_y, next_x) == self.terminator2:
            done = True
        return (next_x, next_y), reward, done
    def policy_iteration_initialize(self):
        self.policy = torch.zeros((self.size, self.size), dtype=torch.int32)
        self.value = torch.ones( (self.size, self.size), dtype=torch.float32) * 0
        self.value[self.terminator1[0], self.terminator1[1]] = 0
        self.value[self.terminator2[0], self.terminator2[1]] = 0
    def policy_iteration_evaluating(self):
        while True:
            delta = 0
            for h in range(self.size):
                for w in range(self.size):
                    old_v = self.value[h, w].item()
                    if (h,w) == self.terminator1 or (h, w) == self.terminator2:
                        continue
                    action = self.policy[h, w]
                    (next_x, next_y), r, done = self.step( (w,h), action)
                    if next_x != w or next_y != h : # moved
                        self.value[h, w] = r + self.value[next_y, next_x].item()
                        delta = max(delta, abs(self.value[h, w].item() - old_v) )
            if delta < 0.0001:
                break
        return self.value
    def policy_iteration_improving(self):
        stable = True
        for h in range(self.size):
            for w in range(self.size):
                if (h, w) == self.terminator1 or (h, w) == self.terminator2:
                    continue
                old_policy = self.policy[h,w].item()
                q_values = []
                for action in range(4):
                    (next_x, next_y), r, done = self.step((w, h), action)
                    if next_x != w or next_y != h:  # moved
                        q_values.append(r + self.value[next_y, next_x].item())
                    else: #如果不能移动，这个动作也要占位，否则后面求argmax的时候就出bug了
                        q_values.append(float('-inf'))
                self.policy[h, w] = torch.tensor(q_values).argmax().item()
                if self.policy[h, w].item() != old_policy:
                    stable = False
        return stable
    #价值迭代法
    def value_iteration(self):
        self.value = torch.ones((self.size, self.size), dtype=torch.float32) * 0
        self.value[self.terminator1[0], self.terminator1[1]] = 0
        self.value[self.terminator2[0], self.terminator2[1]] = 0
        while True:
            delta = 0
            for h in range(self.size):
                for w in range(self.size):
                    old_v = self.value[h, w].item()
                    if (h,w) == self.terminator1 or (h, w) == self.terminator2:
                        continue
                    q_values = []
                    for action in range(4):
                        (next_x, next_y), r, done = self.step( (w,h), action)
                        if next_x != w or next_y != h : # moved
                            q_values.append(r + self.value[next_y, next_x].item())
                    self.value[h, w] = torch.tensor(q_values).max().item()
                    delta = max(delta, abs(self.value[h, w].item() - old_v) )
            if delta < 0.0001:
                break
        # extract policy from v*
        self.policy = torch.zeros((self.size, self.size), dtype=torch.int32)
        for h in range(self.size):
            for w in range(self.size):
                old_v = self.value[h, w].item()
                if (h, w) == self.terminator1 or (h, w) == self.terminator2:
                    continue
                q_values = []
                for action in range(4):
                    (next_x, next_y), r, done = self.step((w, h), action)
                    if next_x != w or next_y != h:  # moved
                        q_values.append(r + self.value[next_y, next_x].item())
                    else:
                        q_values.append(float('-inf')) #占位，确保argmax正确
                self.policy[h,w] = torch.tensor(q_values).argmax().item()
        return
    #策略迭代法
    def policy_iteration(self):
        self.policy_iteration_initialize()
        while True:
            self.policy_iteration_evaluating()
            #print("iteration finished:")
            #env.print_value()
            stable = self.policy_iteration_improving()
            if stable:
                break

    def print_policy(self):
        arrows = {0: '↑', 1: '↓', 2: '←', 3: '→'}

        for y in range(self.size):
            for x in range(self.size):
                if (x,y) == self.terminator1 or (x,y) == self.terminator2:
                    print("X ", end="")
                else:
                    print(f"{arrows[self.policy[y,x].item()]} ", end="")
            print("")
    def print_value(self):
        for y in range(self.size):
            for x in range(self.size):
                print(f"{self.value[y,x].item():.2f} ", end="")
            print("")


env = GridWorld()
env.policy_iteration()
print("\noptimal prolicy and value:")
env.print_policy()
env.print_value()
env.value_iteration()
print("\noptimal prolicy and value:")
env.print_policy()
env.print_value()

```



### 11. 表格型方法

#### 11.1 Q-Learning法实现CliffWalk

cliffwaling的游戏介绍在[这里](https://gymnasium.farama.org/environments/toy_text/cliff_walking/)

代码如下，可以收敛

```python
import torch
import random
import gym
import numpy as np

# 创建环境
env = gym.make('CliffWalking-v0')
state_num = env.observation_space.n
action_num = env.action_space.n
gamma = 0.95  # 折扣因子
lr = 0.001


# 产生动作，一定概率随机（epsilon-greedy）
def generate_action(epsilon, q:torch.Tensor, state):
    if random.random() < epsilon:
        return random.randint(0, action_num - 1)
    else:
        return q[state].argmax().item()  # 根据当前Q函数选择最优动作



def Q_learning(epsilon, q:torch.Tensor):
    episode = 0
    for episode in range(100):  # 迭代次数
        # 收集一条轨迹，存放到buffer里
        buffer = []
        state = env.reset()[0]  # 重置环境并获取初始状态
        while True:
            action = generate_action(epsilon, q, state)
            next_state, reward, done, _, _ = env.step(action)
            buffer.append((state, action, reward, done, next_state))  #
            if done or len(buffer) > 200:
                break
            state = next_state

		#重放buffer并更新Q
        for s, a, r,d, ss in buffer:
            predict = q[s, a].item()
            if d:
                target = r
            else:
                target = r + gamma * q[ss].max().item()
            q[s, a] += lr * (target - predict)

    return


# 根据Q函数表格得到最佳策略函数（也是一个表格）
def get_policy_from_Q(Q):
    policy = torch.zeros((state_num,), dtype=torch.int32)
    for s in range(state_num):
        policy[s] = torch.argmax(Q[s]).item()  # 每个状态选择Q值最大的动作
    return policy


# 打印策略的函数
def print_policy(policy):
    arrows = {0: '↑', 2: '↓', 3: '←', 1: '→'}
    for i in range(state_num):
        print(f"{arrows[policy[i].item()]} ", end="")
        if (i + 1) % 12 == 0:
            print("")


# 反复迭代计算Q
def main():
    epsilon = 1.0  # 初始epsilon
    Q = torch.zeros((state_num, action_num), dtype=torch.float32)  # 初始化Q表
    for episode in range(100):  # 迭代次数
        Q_learning(epsilon, Q)
        # epsilon衰减
        epsilon = max(0.1, epsilon * 0.90)  # 保证epsilon不会小于0.1，避免过早陷入确定性策略

        if (episode+1) % 10 == 0:  # 每10个回合打印一次策略
            print(f"Updated Policy at Episode {episode}:")
            policy = get_policy_from_Q(Q)  # 从Q值更新策略
            print_policy(policy)
            print()

    print("Final Q-table:")
    print(Q)
    print("Final Policy:")
    policy = get_policy_from_Q(Q)  # 从Q值更新策略
    print_policy(policy)


if __name__ == "__main__":
    main()

```

结果能够收敛，在第89次的时候得到了正确的策略：

```shell
Updated Policy at Episode 89:
→ → → → ↓ → → → → ← ↓ ↑ 
→ → ↑ ↑ → → → → → → ↓ ↓ 
→ → → → → → → → → → → ↓ 
↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ 

Updated Policy at Episode 99:
→ → → → → → → → → ↓ → ↓ 
→ → → ↓ → → → → → → → ↓ 
→ → → → → → → → → → → ↓ 
↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ 
```

#### 11.2 蒙特卡洛+策略迭代法实现CliffWalk



![image-20250401192434621](img/RL/image-20250401192434621.png)

我发现 env并不能随意指定state进行step，它内部有连续的状态管理，reset后我step(0)，它会返回next_state=36，把agent强行拉回七点。所以我不能完全按照上面的算法先求V再求Q，而是直接用蒙特卡洛方法求Q。

并且，在师兄的指导下，我把跌入悬崖的reward特殊处理了一下，改为-3。这样在第39次迭代的时候收敛到了最优策略。后面就稳定了

```python
Episode #39: avg_reword:-11.81, traj_num:10000
Updated Policy at Episode 39:
↑ ← ↓ ↓ → → ↑ ↓ → → → → 
→ → ↓ ↓ → ↓ ↓ → → → → ↓ 
→ → → → → → → → → → → ↓ 
↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ 
```

如果掉悬崖情况下保持reward原值（-100），几个小时候跑了300个迭代，也没有收敛。

代码如下：

```python
import torch
import random
import gym
import numpy as np

# 创建环境
env = gym.make('CliffWalking-v0')
state_num = env.observation_space.n
action_num = env.action_space.n
gamma = 0.95  # 折扣因子


# 产生动作，一定概率随机（epsilon-greedy）
def generate_action(epsilon, policy, state):
    if random.random() < epsilon:
        return random.randint(0, action_num - 1)
    else:
        return policy[state].item()  # 根据当前策略选择最优动作


# 基于当前的策略（外加一些随机 epsilon-greedy），使用蒙特卡洛法计算Q价值函数（一张表格）
def calculate_Q(epsilon, policy):
    Q = torch.zeros((state_num, action_num), dtype=torch.float32)  # 初始化Q表
    N = torch.zeros((state_num, action_num), dtype=torch.int32)  # 记录每个state-action对的访问次数

    total_reward = 0.0
    trajectory_count = 0

    traj_num = 0
    for traj_num in range(10000): #这个一万次不能改小，改小了可能不收敛
        # 收集一条轨迹
        trajectory = []
        state = env.reset()[0]  # 重置环境并获取初始状态
        while True:
            action = generate_action(epsilon, policy, state)
            next_state, reward, done, _, _ = env.step(action)
            if reward == -100:
                reward = -3
            trajectory.append((state, action, reward))  # 存储轨迹 (state, action, reward)
            if done or len(trajectory) > 500:
                break
            state = next_state

        # 计算每一步的回报，更新Q表
        G = 0  # 初始化回报
        for s, a, r in reversed(trajectory):
            G = r + gamma * G  # 计算折扣回报
            N[s, a] += 1
            Q[s, a] += (G - Q[s, a].item()) / N[s, a].item()  # 使用蒙特卡洛方法更新Q表

        total_reward += G
        trajectory_count += 1

        # 如果每个状态的每个动作都有足够的访问次数，可以提前结束蒙特卡洛计算
        tmp = N >= 10
        tmp = tmp.count_nonzero().item()
        if tmp >= (38*action_num):  # 只要每个状态-动作对都访问了至少若干次就结束
            print("early stop")
            break

    average_reward = total_reward / trajectory_count if trajectory_count > 0 else 0
    return Q, average_reward, traj_num+1


# 根据Q函数表格得到最优策略函数（也是一个表格）
def get_policy_from_Q(Q):
    policy = torch.zeros((state_num,), dtype=torch.int32)
    for s in range(state_num):
        policy[s] = torch.argmax(Q[s]).item()  # 每个状态选择Q值最大的动作
    return policy


# 打印策略的函数
def print_policy(policy):
    arrows = {0: '↑', 2: '↓', 3: '←', 1: '→'}
    for i in range(state_num):
        print(f"{arrows[policy[i].item()]} ", end="")
        if (i + 1) % 12 == 0:
            print("")


# 策略迭代法：根据当前policy计算Q -> 根据Q更新policy -> 根据新的policy计算Q-->
def main():
    epsilon = 1.0  # 初始epsilon
    policy = torch.zeros((state_num,), dtype=torch.int32)  # 初始化随机策略
    for episode in range(100):  # 迭代次数
        Q, avg_r, traj_num = calculate_Q(epsilon, policy)
        print(f"Episode #{episode}: avg_reword:{avg_r:.2f}, traj_num:{traj_num}")
        policy = get_policy_from_Q(Q)  # 从Q值更新策略

        # epsilon衰减
        epsilon = max(0.1, epsilon * 0.90)  # 保证epsilon不会小于0.1，避免过早陷入确定性策略

        if (episode+1) % 10 == 0:  # 每10个回合打印一次策略
            print(f"Updated Policy at Episode {episode}:")
            print_policy(policy)
            print()

    print("Final Q-table:")
    print(Q)
    print("Final Policy:")
    print_policy(policy)


if __name__ == "__main__":
    main()

```

