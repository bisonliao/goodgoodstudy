

## 强化学习入门

### 1. 基本原理

推荐王树森老师的这个[教学视频](https://www.bilibili.com/video/BV12o4y197US?spm_id_from=333.788.videopod.episodes&vd_source=2173cb93b451f2278a1c87becf3ef529)

![image-20250324221405752](img/RL/image-20250324221405752.png)

![image-20250330085209214](img/RL/image-20250330085209214.png)

#### 1.1 两个容易混淆的价值函数：

在深度强化学习中，**action-value函数**和**state-value函数**是评估策略性能的两个关键函数，它们的区别和联系如下：

**1. 定义**

- **State-Value Function (V函数)**:
  
  - 表示在状态 \( s \) 下，遵循策略 \( \pi \) 的预期回报。
  
  - 公式：
  $$
    V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t R_{t+1} \mid S_t = s \right] 
  $$
  
  
  - 其中，
    $$
    \gamma  是折扣因子，( R_{t+1} ) 是时刻 ( t+1 ) 的奖励。
    $$
  
- **Action-Value Function (Q函数)**:
  
  - 表示在状态 \( s \) 下采取动作 \( a \)，然后遵循策略 Pi 的预期回报。
  
  - 公式：
    $$
    Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t R_{t+1} \mid S_t = s, A_t = a \right] 
    $$
    

**2. 区别**

- **输入**:
  
  - V 函数只依赖于状态 \( s \)。
  - Q 函数依赖于状态 \( s \) 和动作 \( a \)。
  
- **用途**:
  - V 函数 用于评估策略在状态 \( s \) 下的表现。
  - Q 函数用于评估在状态 \( s \) 下采取动作 \( a \) 的效果。
  





### 2.方式一：基于价值的学习（Value based RL）

#### 2.1 原理

又叫Q-Learning。

Q(s, a)函数，即action-value函数，能返回状态s下 分别采取各种行动a的未来总的奖励期望值。agent可以根据Q(s, a)函数，在每个状态下采取奖励期望值最大的行动，持续进行下去。

DQN就是要训练出一个深度神经网络，拟合Q(s, a)函数。准确的说，是输入s，输出各种a下的奖励期望。



DQN（Deep Q-Network）是强化学习的一种方法，结合了 Q-Learning 和深度学习。其核心原理如下：

1. **Q-Learning**：使用 Q 函数 Q(s,a)Q(s, a) 估计状态 ss 下采取动作 aa 的长期回报。更新规则：
   $$
   Q(s,a)←Q(s,a)+α[r+γmax⁡Q(s′,a′)−Q(s,a)]
   $$

   $$
   其中 r 是奖励，γ 是折扣因子，s′ 是下一个状态。
   $$

2. **深度神经网络**：用 CNN/MLP 逼近 Q 函数，输入状态 ss，输出所有动作的 Q 值。

3. **经验回放（Experience Replay）**：收集游戏经历，存入缓冲区，**随机采样**训练，减少数据相关性，提高稳定性。

4. **目标网络（Target Network）**：用一个 **滞后的 Q 网络**计算目标 Q 值，减少训练不稳定性。

5. **ε-greedy 策略**：开始时以一定概率随机探索（选择随机动作），逐渐减少探索，更多利用学习到的策略（选择 Q 值最大的动作）。



下图是李宏毅老师的课件笔记：

![image-20250329153218963](img/RL/image-20250329153218963.png)



我的理解：对于状态和动作空间离散且有限的场景（甚至连续的也可以？），还可以用策略迭代更新 + 蒙特卡洛方法，从而得到最优策略：

![image-20250401192434621](img/RL/image-20250401192434621.png)

#### 2.2 实操1 举高高

OpenAI Gym 提供了许多强化学习环境，可以用来训练 DQN（Deep Q-Network）。下面使用 **CartPole-v1**，它的状态空间小，动作空间离散，训练速度快，适合作为 DQN 的入门案例。

**CartPole 任务简介**

- **目标**：控制一个小车，使其上的杆子保持平衡
- **状态空间**：4 维（小车位置、速度、杆子角度、杆子角速度）
- **动作空间**：2 维（向左推、向右推）
- **奖励**：每个时间步 +1，直到杆子倒下或小车超出边界



action只有向左或向右移动小车两个动作，策略看起来是简单的依据当前杆子的角度，杆子往哪边倒就往哪边推车子，这是一个贪心算法，但并不work。

因为车子的状态还包括车子的速度、杆子的角速度，所以杆子的角度并不表示一定像某一侧倾倒，杆子同时也在转动，且有惯性，车子也在移动和有加速度。

```python
import random
import gym
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from collections import deque
import argparse
import os

# 设备选择
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 超参数
gamma = 0.99  # 折扣因子
epsilon = 1.0  # 初始探索率
epsilon_min = 0.01  # 最低探索率
epsilon_decay = 0.995  # 探索率衰减
learning_rate = 1e-3  # 学习率
batch_size = 64  # 经验回放的批量大小
memory_size = 10000  # 经验池大小
target_update_freq = 10  # 目标网络更新频率

env = gym.make("CartPole-v1", render_mode="human")
n_state = env.observation_space.shape[0]  # 状态维度
n_action = env.action_space.n  # 动作数量


# DQN 网络定义
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)


# 初始化网络
model = DQN(n_state, n_action).to(device)
target_model = DQN(n_state, n_action).to(device)
target_model.load_state_dict(model.state_dict())
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
memory = deque(maxlen=memory_size)


def select_action(state, epsilon):
    """基于 ε-greedy 选择动作"""
    if random.random() < epsilon:
        return random.randint(0, n_action - 1)  # 随机选择
    else:
        state = torch.FloatTensor(state).unsqueeze(0).to(device)  # 变换前：[4] -> 变换后：[1, 4]
        return model(state).argmax(1).item()  # 选取 Q 值最大的动作


def train():
    if len(memory) < batch_size:
        return 9999.0  # 经验池数据不足时不训练

    batch = random.sample(memory, batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)

    states = torch.FloatTensor(states).to(device)  # (batch_size, 4)
    actions = torch.LongTensor(actions).unsqueeze(1).to(device)  # (batch_size,) -> (batch_size, 1)
    rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)  # (batch_size,) -> (batch_size, 1)
    next_states = torch.FloatTensor(next_states).to(device)  # (batch_size, 4)
    dones = torch.FloatTensor(dones).unsqueeze(1).to(device)  # (batch_size,) -> (batch_size, 1)

    # 计算当前 Q 值
    q_values = model(states).gather(1, actions)  # 从 Q(s, a) 选取执行的动作 Q 值

    # 计算目标 Q 值
    next_q_values = target_model(next_states).max(1, keepdim=True)[0]  # 选取 Q(s', a') 的最大值
    target_q_values = rewards + gamma * next_q_values * (1 - dones)  # TD 目标

    # 计算损失
    loss = F.mse_loss(q_values, target_q_values.detach())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()


def save_checkpoint(id):
    path=f"dqn_checkpoint_{id}.pth"
    torch.save(model.state_dict(), path)
    print(f"Checkpoint saved to {path}")


def load_checkpoint(path):
    if os.path.exists(path):
        model.load_state_dict(torch.load(path, map_location=device))
        print(f"Checkpoint loaded from {path}")
    else:
        print("No checkpoint found, starting from scratch.")


def main(mode):
    global epsilon

    if mode == "train":
        episodes = 500
        for episode in range(episodes):
            state = env.reset()
            state = state[0]  # 适配 Gym v26
            total_reward = 0

            while True:
                action = select_action(state, epsilon)
                next_state, reward, done, _, _ = env.step(action)

                # 经验回放缓存
                memory.append((state, action, reward, next_state, done))
                state = next_state
                total_reward += reward

                # 训练 DQN
                loss = train()

                if done:
                    break

            # 逐步降低 epsilon，减少随机探索，提高利用率
            epsilon = max(epsilon_min, epsilon * epsilon_decay)

            # 定期更新目标网络，提高稳定性
            if episode % target_update_freq == 0:
                target_model.load_state_dict(model.state_dict())

            # 定期保存模型
            #if episode % 50 == 0:
            if total_reward > 1000:
                save_checkpoint(total_reward)

            print(f"Episode {episode}, Reward: {total_reward}, Epsilon: {epsilon:.3f}, loss:{loss}")

    elif mode == "infer":
        load_checkpoint("./dqn_checkpoint.pth")
        state = env.reset()
        state = state[0]
        total_reward = 0

        while True:
            env.render()
            action = select_action(state, 0)  # 纯利用，epsilon=0
            state, reward, done, _, _ = env.step(action)
            total_reward += reward

            if done:
                break

        print(f"Inference finished. Total reward: {total_reward}")


if __name__ == "__main__":
    main("train")
```

训练效果：

```shell
#一开始杆子竖立不了多久 （看total reward值）
Episode 3, Reward: 24.0, Epsilon: 0.980, loss:0.08007372915744781
Episode 4, Reward: 20.0, Epsilon: 0.975, loss:0.028971994295716286

# 400个episode的时候，杆子可以竖立很久，图片上可以看到不怎么晃
Episode 383, Reward: 8844.0, Epsilon: 0.146, loss:0.049317866563797
Episode 384, Reward: 266.0, Epsilon: 0.145, loss:0.011158960871398449
Episode 385, Reward: 2882.0, Epsilon: 0.144, loss:0.006994911935180426
Episode 386, Reward: 19383.0, Epsilon: 0.144, loss:0.02331690490245819
Episode 387, Reward: 13857.0, Epsilon: 0.143, loss:0.007248120382428169
Episode 388, Reward: 12.0, Epsilon: 0.142, loss:0.27341461181640625
Episode 389, Reward: 60.0, Epsilon: 0.142, loss:0.019126372411847115
Episode 390, Reward: 2144.0, Epsilon: 0.141, loss:0.01851404272019863
Episode 391, Reward: 5288.0, Epsilon: 0.140, loss:0.009122185409069061
Episode 392, Reward: 100.0, Epsilon: 0.139, loss:0.0026022980455309153
Episode 393, Reward: 1567.0, Epsilon: 0.139, loss:0.006415518932044506
Episode 394, Reward: 3256.0, Epsilon: 0.138, loss:1.4469681978225708
#中间效果变差过
Episode 489, Reward: 287.0, Epsilon: 0.086, loss:0.053130991756916046
Episode 490, Reward: 222.0, Epsilon: 0.085, loss:0.09821200370788574
Episode 491, Reward: 1530.0, Epsilon: 0.085, loss:0.4331984519958496
Episode 492, Reward: 12340.0, Epsilon: 0.084, loss:0.03685907647013664
#到这里的时候，已经过去两个小时没有输出了，杆子屹立不倒...
```

![image-20250324122259027](img/RL/image-20250324122259027.png)

### 3. 方式二：基于策略的学习（Policy based RL）

#### 3.1 原理

相比基于价值的强化学习，基于策略的强化学习（如Policy Gradient方法）的一个重要优势是：**它不需要每一步（step-wise）都有明确的即时奖励信号，只要在整局游戏（episode）结束时能获得一个累积奖励（或最终胜负结果），就可以直接训练策略网络**。这是与基于值函数的方法（如DQN）的关键区别之一。

这样我就可以开发一个五子棋小游戏，不需要每一步落子都显式编程给出奖励值，只需要在棋局终局给胜者1负者-1的奖励即可。降低了设计的难度。

但它明显的劣势是梯度倍乘的Return值变化太大，不稳定。

![image-20250324202948777](img/RL/image-20250324202948777.png)

![image-20250324203037983](img/RL/image-20250324203037983.png)

我没有想明白的是：如果能够近似计算Q(s,a)，那似乎也就回到了DQN，不需要做策略学习了？

AI这样说上面训练过程不需要拟合Q函数，只需要计算一次从s开始采取a的游戏局的return就可以。两个方法的区别：

![image-20250324203820771](img/RL/image-20250324203820771.png)



下面两个图是李宏毅老师课程的笔记：

![image-20250327120345688](img/RL/image-20250327120345688.png)



![image-20250329103037264](img/RL/image-20250329103037264.png)

#### 3.2 实操1 举高高

还是平衡车小游戏：

```python
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import argparse
import os
import matplotlib.pyplot as plt

# 设置随机种子
torch.manual_seed(42)
np.random.seed(42)

# 检测设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


# ----------------------------
# 1. 策略网络定义
# ----------------------------
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, action_dim)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        if len(x.shape) == 1:
            x = x.unsqueeze(0)  # [state_dim] -> [1, state_dim]
        x = torch.relu(self.fc1(x))  # [batch_size, hidden_dim]
        logits = self.fc2(x)  # [batch_size, action_dim]
        probs = self.softmax(logits)  # [batch_size, action_dim]
        return probs


# ----------------------------
# 2. Checkpoint 保存与加载
# ----------------------------
def save_checkpoint(policy, optimizer, episode, reward, path="checkpoint.pth"):
    torch.save({
        'policy_state_dict': policy.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'episode': episode,
        'reward': reward
    }, path)
    print(f"Checkpoint saved to {path} (Reward: {reward:.2f})")


def load_checkpoint(policy, optimizer, path="checkpoint.pth"):
    if os.path.exists(path):
        checkpoint = torch.load(path)
        policy.load_state_dict(checkpoint['policy_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print(f"Loaded checkpoint from {path} (Episode: {checkpoint['episode']}, Reward: {checkpoint['reward']:.2f})")
        return checkpoint['episode'], checkpoint['reward']
    else:
        print(f"No checkpoint found at {path}")
        return 0, 0


# ----------------------------
# 3. 训练函数（带Checkpoint和可视化）
# ----------------------------
def train(env_name="CartPole-v1", hidden_dim=128, lr=1e-2,
          gamma=0.99, max_episodes=1000, print_interval=2):
    env = gym.make(env_name, render_mode="human")  # 开启可视化
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)
    optimizer = optim.Adam(policy.parameters(), lr=lr)

    # 尝试加载Checkpoint
    checkpoint_path = "./pbrl_checkpoint_150_2955xx.0.pth"
    start_episode, _ = load_checkpoint(policy, optimizer, checkpoint_path)
    episode_rewards = []

    for episode in range(start_episode, max_episodes):
        state = env.reset()
        state = state[0]
        states, actions, rewards = [], [], []

        while True:
            state_tensor = torch.FloatTensor(state).to(device)
            state_tensor = state_tensor.unsqueeze(0)
            probs = policy(state_tensor)
            m = Categorical(probs) #根据各种action的概率值probs创建一个离散的概率分布
            action = m.sample() #使用该概率分布进行抽样，得到一个具体的action

            next_state, reward, done, _, _ = env.step(action.item())

            states.append(state_tensor.squeeze(0))
            actions.append(action.squeeze(0))
            rewards.append(reward)

            state = next_state
            if done:
                break
            '''到后面模型能力强了，游戏一把玩好久也不死。就导致两次训练的时间间隔很长
            粗暴的截断会有两个问题：
            1、可能导致游戏后面才会出现的states，从来没有出现在训练集里，使得模型失去泛化能力
            2、compute_returns不准确了，这个可能对于密集奖励型游戏还好，毕竟到了后面状态，伽马的n次方接近0，后面的项影响有限
            '''
            '''if  len(rewards) > 1000:
                print('brutally cut')
                break'''

        # 计算回报
        total_reward = sum(rewards)
        episode_rewards.append(total_reward)

        returns = compute_returns(rewards, gamma)
        returns = (returns - returns.mean()) / (returns.std() + 1e-9)

        # 更新策略
        loss = train_policy_network(policy, optimizer, states, actions, returns)

        # 保存Checkpoint（如果回报>1000）
        if total_reward > 1000:
            checkpoint_path = f"./pbrl_checkpoint_{episode}_{total_reward}.pth"
            save_checkpoint(policy, optimizer, episode, total_reward, checkpoint_path)

        # 打印进度
        if (episode + 1) % print_interval == 0:
            avg_reward = np.mean(episode_rewards[-print_interval:])
            print(f"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}, Loss: {loss:.4f}")

        # 提前终止
        if len(episode_rewards) >= 100 and np.mean(episode_rewards[-100:]) >= 2000:
            print(f"Solved at Episode {episode + 1}!")
            break

    env.close()
    return episode_rewards


# ----------------------------
# 4. 推理函数（演示训练好的模型）
# ----------------------------
def test(env_name="CartPole-v1", hidden_dim=128, checkpoint_path="checkpoint.pth"):
    env = gym.make(env_name, render_mode="human")  # 开启可视化
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)
    optimizer = optim.Adam(policy.parameters(), lr=0.001)  # 仅占位，实际不用于测试

    # 加载Checkpoint
    load_checkpoint(policy, optimizer, checkpoint_path)

    print("Starting inference...")
    while True:  # 无限运行直到手动停止
        state = env.reset()[0]
        total_reward = 0

        while True:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).to(device)
                probs = policy(state_tensor)
                action = torch.argmax(probs).item()  # 直接选最优动作

            next_state, reward, done, _, _ = env.step(action)
            total_reward += reward
            state = next_state

            if done:
                print(f"Inference Reward: {total_reward}")
                break


# ----------------------------
# 5. 辅助函数， 用于策略梯度更新（替代Q(s,a) 的蒙特卡洛估计）
# ----------------------------
def compute_returns(rewards, gamma=0.99):
    returns = []
    R = 0
    for r in reversed(rewards):
        R = r + gamma * R
        returns.insert(0, R)
    return torch.tensor(returns, device=device)


def train_policy_network(policy, optimizer, states, actions, returns):
    # 将列表中的状态/动作/回报堆叠成张量， 假设一把游戏玩下来的状态个数是T
    states = torch.stack(states)  # [T, 4]
    actions = torch.stack(actions) # [T]
    returns = returns  # [T]
    # 1. 通过策略网络计算动作概率
    probs = policy(states)  # [T, action_dim]
    # 2. 创建分类分布（用于采样和计算对数概率）
    m = Categorical(probs)
    # 3. 计算所选动作的对数概率
    log_probs = m.log_prob(actions)  # [T,] 
    # 4. 因为基于策略的强化学习要使用梯度上升使得state-value函数的期望最大化，所以损失函数是期望值的负数
    # returns已经在函数外面进行了带折扣的汇总运算，也就是已经是U了，不是每一步的r
    loss = -(log_probs * returns).mean()
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss.item()


# ----------------------------
# 6. 主函数（命令行参数解析）
# ----------------------------
def main(mode):
    if mode == "train":
        rewards = train()
        plt.plot(rewards)
        plt.xlabel("Episode")
        plt.ylabel("Total Reward")
        plt.title("Policy Gradient Training")
        plt.show()
    elif mode == "test":
        test(checkpoint_path="./pbrl_checkpoint_1001.0.pth")


if __name__ == "__main__":
    main("train")
```

效果类似基于价值的强化学习，150个episode后，一根棍子可以举很久不倒。

上面的代码，是严格按照王树森老师的使V(s)最大化的思路来实现的，如果按照李宏毅老师的交叉熵的思路，train函数改为如下：

```python
def train_policy_network(policy, optimizer, states, actions, returns):
    # 将列表中的状态/动作/回报堆叠成张量， 假设一把游戏玩下来的状态个数是T
    states = torch.stack(states)  # [T, 4]
    actions = torch.stack(actions) # [T]
    returns = returns  # [T]
    # 1. 通过策略网络计算动作概率
    probs = policy(states)  # [T, action_dim]
    # 2. 计算交叉熵，因为probs是softmax处理过后的，所以用nll_loss函数
    loss = (torch.nn.functional.nll_loss(torch.log(probs), actions, reduction='none')  * returns).mean()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss.item()
```

这样修改后，也能够快速收敛，把棍子举得久久的。

#### 3.3 实操2 左右博弈五子棋 v1

具体的： 

1. 先让AI帮忙写一个简单的五子棋小游戏，提供reset()  step()等接口，供RL程序调用
2. 定义一个策略CNN网络，输入是9x9的棋局，也就是状态，返回9x9个位置上落黑子的概率。在获取概率每个位置的概率的时候，把当期棋局已经落子的位置剔除掉不考虑
3.  把上面的CNN策略网络创建两个实例，一个叫A，一个叫B。他们是同一个类型网络的左右手对弈。在调用B获得下一步的action的时候，需要把棋局的黑白对调一下后作为输入状态传给B，返回下一步在每个坐标下黑子的概率。因为他们是同一个网络的两个实例，我们训练的网络是执黑的。 
4. 每隔10个epsode就把A的参数赋值给B 
5. 五子棋程序只有在分出胜负的时候才返回非0的reward 



一开始如上面所示的去做，怎么训练都没有起色。后来简化了问题：

1. 棋盘大小改为5x5
2. 连成3个就算赢
3. 让RL程序执黑，和人肉智能对弈（ black_step()函数背后会有白子执行人肉智能 ）， 每128局统计一下黑方的赢率

简化后训练会有效果，慢慢的黑棋的赢率会达到53%，最高到90%，但不能稳住在一个高水平，经常在30%+附近。

人类与之对弈，会明显发现训练后的模型会比训练前聪明。

下面是简化后的代码

```python
import random
import time
import numpy as np
import pygame
import sys
import torch
from exceptiongroup import catch

# 定义颜色
WHITE = (255, 255, 255)
BLACK = (0, 0, 0)
GRAY = (200, 200, 200)
GREEN = (0, 255, 0)

# 棋盘大小
BOARD_SIZE = 5
CELL_SIZE = 50
WINDOW_SIZE = BOARD_SIZE * CELL_SIZE


class Gomoku:
    def __init__(self):
        """ 初始化五子棋环境 """
        pygame.init()
        self.screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))
        pygame.display.set_caption("五子棋")
        self.font = pygame.font.Font(None, 36)
        self.reset()

    def reset(self):
        """ 重新初始化棋局，在中心落一个黑子 """
        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)
        self.last_move = None
        # 随机初始位置
        '''x = random.randint(0, BOARD_SIZE - 1)
        y = random.randint(0, BOARD_SIZE - 1)
        self.board[x, y] = 1  # 黑棋先手
        self.last_move = (x, y)'''
        self.game_over = False
        self.winner = None
        return self.board.copy()

    def step(self, role, x, y):
        """ 执行落子，并计算局部奖励 """
        if self.game_over or self.board[x, y] != 0:
            return self.board.copy(), 0, self.game_over  # 非法落子

        self.board[x, y] = role
        self.last_move = (x, y)

        # 检查胜负
        if self.check_win(x, y, role):
            self.game_over = True
            self.winner = role
            reward = 1 if role == 1 else -1
        else:
            reward = 0
        self.render()
        return self.board.copy(), reward, self.game_over

    def black_step(self, x, y):
        """ 黑方落子，然后白方智能应对 """
        draw = False
        # 黑方落子
        state, reward, done = self.step(1, x, y)
        if done:
            return state, reward, done, draw, (x,y)

        # 白方智能落子
        if not done:
            white_x, white_y = self.find_best_move(-1)
            if white_x is None:
                draw = True
                return None, None, None, draw, (x, y)
            state, reward, done = self.step(-1, white_x, white_y)

        return state, reward, done, draw, (white_x, white_y)

    def ai_play_episode(self, slow=False):
        """ 自动对弈一局，返回黑方的状态、动作和奖励序列 """
        states = []
        actions = []
        rewards = []


        self.reset()
        states.append(torch.tensor(self.board.copy()).unsqueeze(0))

        draw = False
        lastcoord = None
        while not self.game_over:
            # 黑方智能落子
            black_x, black_y = self.find_best_move(1)
            if black_x is None:
                draw = True
                break
            actions.append(torch.tensor([black_x, black_y]))
            state, reward, done, draw, lastcoord = self.black_step(black_x, black_y)
            if draw:
                break
            if slow:
                time.sleep(2)
            states.append( torch.tensor(state.copy()).unsqueeze(0))
            rewards.append(reward)
            if done :
                break


            # 处理游戏结束事件
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    sys.exit()
        if slow:
            time.sleep(2)
        return states[:-1], actions, rewards, draw, lastcoord  # 最后一个状态不需要

    def find_best_move(self, role):
        """ 智能寻找最佳落子位置 """
        opponent = -role

        # 1. 检查自己是否能直接获胜
        for x in range(BOARD_SIZE):
            for y in range(BOARD_SIZE):
                if self.board[x, y] == 0:
                    self.board[x, y] = role
                    if self.check_win(x, y, role):
                        self.board[x, y] = 0  # 恢复
                        return x, y
                    self.board[x, y] = 0  # 恢复

        # 2. 检查对手是否能直接获胜，需要防守
        for x in range(BOARD_SIZE):
            for y in range(BOARD_SIZE):
                if self.board[x, y] == 0:
                    self.board[x, y] = opponent
                    if self.check_win(x, y, opponent):
                        self.board[x, y] = 0  # 恢复
                        return x, y
                    self.board[x, y] = 0  # 恢复

        # 3. 评估每个空位的得分
        best_score = -1
        best_moves = []
        for x in range(BOARD_SIZE):
            for y in range(BOARD_SIZE):
                if self.board[x, y] == 0:
                    score = self.evaluate_position(x, y, role)
                    if score > best_score:
                        best_score = score
                        best_moves = [(x, y)]
                    elif score == best_score:
                        best_moves.append((x, y))

        # 从最佳候选位置中随机选择一个
        if len(best_moves) == 0:
            return None,None
        else:
            return random.choice(best_moves)

    def evaluate_position(self, x, y, role):
        """ 评估在(x,y)落子的价值 """
        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]
        total_score = 0

        for dx, dy in directions:
            # 计算这个方向上的棋型
            line = []
            for d in [-4, -3, -2, -1, 0, 1, 2, 3, 4]:  # 检查9个位置
                nx, ny = x + d * dx, y + d * dy
                if 0 <= nx < BOARD_SIZE and 0 <= ny < BOARD_SIZE:
                    line.append(self.board[nx, ny])
                else:
                    line.append(2)  # 边界外

            # 只关注以(x,y)为中心的5个位置
            center = BOARD_SIZE // 2  # 因为前面检查了9个位置，(x,y)在中间
            segment = line[center - BOARD_SIZE // 2:center + BOARD_SIZE // 2+1]

            # 计算这个方向的得分
            total_score += self.evaluate_segment(segment, role)

        return total_score

    def evaluate_segment(self, segment, role):
        """ 评估一个5连位置的得分 """
        opponent = -role
        count_role = segment.count(role)
        count_opponent = segment.count(opponent)

        # 如果有对手的棋子，这个位置价值降低
        if count_opponent > 0:
            return 0

        # 根据连子数给分
        if count_role == 4: return 10000  # 活四
        if count_role == 3: return 1000  # 活三
        if count_role == 2: return 100  # 活二
        if count_role == 1: return 10  # 活一
        return 1  # 空位

    def check_win(self, x, y, role):
        """ 判断当前落子是否形成五连胜 """
        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]
        for dx, dy in directions:
            count = 1
            for d in [-1, 1]:  # 计算两个方向
                nx, ny = x, y
                while True:
                    nx += d * dx
                    ny += d * dy
                    if 0 <= nx < BOARD_SIZE and 0 <= ny < BOARD_SIZE and self.board[nx, ny] == role:
                        count += 1
                    else:
                        break
            if count >= 3:
                return True
        return False

    def render(self):
        """ 渲染棋盘 """
        self.screen.fill(WHITE)

        # 画网格
        for i in range(BOARD_SIZE):
            pygame.draw.line(self.screen, GRAY, (i * CELL_SIZE + CELL_SIZE // 2, CELL_SIZE // 2),
                             (i * CELL_SIZE + CELL_SIZE // 2, WINDOW_SIZE - CELL_SIZE // 2), 2)
            pygame.draw.line(self.screen, GRAY, (CELL_SIZE // 2, i * CELL_SIZE + CELL_SIZE // 2),
                             (WINDOW_SIZE - CELL_SIZE // 2, i * CELL_SIZE + CELL_SIZE // 2), 2)

        # 画棋子
        for x in range(BOARD_SIZE):
            for y in range(BOARD_SIZE):
                if self.board[x, y] == 1:  # 黑子
                    pygame.draw.circle(self.screen, BLACK, (y * CELL_SIZE + CELL_SIZE // 2,
                                                            x * CELL_SIZE + CELL_SIZE // 2), CELL_SIZE // 3)
                elif self.board[x, y] == -1:  # 白子
                    pygame.draw.circle(self.screen, WHITE, (y * CELL_SIZE + CELL_SIZE // 2,
                                                            x * CELL_SIZE + CELL_SIZE // 2), CELL_SIZE // 3)
                    pygame.draw.circle(self.screen, BLACK, (y * CELL_SIZE + CELL_SIZE // 2,
                                                            x * CELL_SIZE + CELL_SIZE // 2), CELL_SIZE // 3, 2)

        # 标记最后一手棋
        if self.last_move:
            lx, ly = self.last_move
            pygame.draw.circle(self.screen, GREEN, (ly * CELL_SIZE + CELL_SIZE // 2,
                                                    lx * CELL_SIZE + CELL_SIZE // 2), CELL_SIZE // 3 + 3, 2)

        # 游戏结束显示赢家
        if self.game_over:
            msg = "black wins." if self.winner == 1 else "white wins."
            text = self.font.render(msg, True, BLACK)
            self.screen.blit(text, (WINDOW_SIZE // 3, WINDOW_SIZE // 2))

        pygame.display.flip()

    def play_human(self):
        """ 让人类玩家交互式落子 """
        running = True
        role = 1
        while running:
            self.render()
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    running = False
                    pygame.quit()
                    sys.exit()
                elif event.type == pygame.MOUSEBUTTONDOWN and not self.game_over:
                    x, y = event.pos[1] // CELL_SIZE, event.pos[0] // CELL_SIZE
                    _, r, done = self.step(role, x, y)
                    print(f"r={r}")
                    if not done:
                        role *= -1  # 轮到另一方落子
        pygame.quit()

    def human_play(self, role):
        """ 让人类玩家落子，阻塞直到有效输入，并返回新状态 """
        self.render()

        while True:  # 阻塞等待有效输入
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    exit()  # 退出程序

                elif event.type == pygame.MOUSEBUTTONDOWN and not self.game_over:
                    x, y = event.pos[1] // CELL_SIZE, event.pos[0] // CELL_SIZE

                    # 确保落子位置未被占据
                    if self.board[x, y] == 0:
                        state, reward, done,  = self.step(role,  x, y)
                        return state, reward, done  # 返回新状态、奖励、是否终局


def main():
    go = Gomoku()
    print(5//2)
    go.ai_play_episode(True)








if __name__ == "__main__":
    main()
```

然后是训练的代码：

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import time
import pygame
import numpy as np
from collections import Counter
import os
from gomoku import Gomoku  # 假设 gomoku.py 里定义了五子棋环境
from gomoku import BOARD_SIZE
from torch.distributions import Categorical

# 设备初始化
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



# 定义策略网络
class PolicyNet(nn.Module):
    def __init__(self):
        super(PolicyNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.fc = nn.Linear(128 * BOARD_SIZE * BOARD_SIZE, BOARD_SIZE * BOARD_SIZE)

    def forward(self, state):
        batch_size = state.size(0)  # 获取 batch 大小

        # 计算合法落子 mask（state=0 的地方可以落子）
        mask = (state.view(batch_size, BOARD_SIZE, BOARD_SIZE) == 0).float()  # [batch, BOARD_SIZE, BOARD_SIZE]，可落子=1，不可落子=0
        mask = mask.view(batch_size, BOARD_SIZE*BOARD_SIZE)  # [batch, BOARD_SIZE*BOARD_SIZE]

        # 通过 CNN 提取特征
        x = F.relu(self.conv1(state))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))

        # 展平成 [batch, 128 * BOARD_SIZE * BOARD_SIZE]
        x = x.view(batch_size, -1)
        x = self.fc(x)

        # 对不可落子的位置赋极小值 (-inf)
        x = x.masked_fill(mask == 0, float('-inf'))  # 屏蔽不可落子的位置

        # 计算 softmax 仅在合法位置
        x = F.softmax(x, dim=1)

        return x.view(batch_size, BOARD_SIZE, BOARD_SIZE)  # 变回 [batch, BOARD_SIZE, BOARD_SIZE] 的棋盘格式

# 选择动作
def select_action(policy_net, state):
    """
    选择动作，避免已落子点，并处理棋盘已满的情况。

    :param policy_net: 策略网络
    :param state: 当前棋盘状态 (9x9)
    :return: 选择的动作坐标 (x, y) 或 None（表示平局）
    """
    state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)
    policy_net.eval()
    with torch.no_grad():
        action_probs = policy_net(state_tensor).squeeze().detach().cpu().numpy()  # 转到CPU计算
    policy_net.train()


    # 不能选择已落子点
    action_probs[state != 0] = 0

    total_prob = action_probs.sum()
    if total_prob == 0:
        return None  # 棋盘已满，平局

    action_probs /= total_prob  # 归一化
    choice = np.random.choice(BOARD_SIZE * BOARD_SIZE, p=action_probs.flatten())
    return divmod(choice, BOARD_SIZE)  # 返回 (x, y) 坐标

def entropy(coords):
    if not coords:
        return 0  # 空列表的熵定义为0

    counter = Counter(coords)  # 统计每个坐标的出现次数
    total = len(coords)  # 总数
    probs = np.array([count / total for count in counter.values()])  # 计算概率分布
    #print(f"{len(coords)}, {len(probs)}")
    return -np.sum(probs * np.log2(probs))  # 计算熵（以2为底）

# 运行完整一局游戏，收集经验
def play_one_episode(policy_net, win_coords):
    env = Gomoku()
    state = env.reset()
    states, actions, rewards = [], [], []

    blackwin = False


    while True:
        action = select_action(policy_net , state )
        if action is None:  # 平局
            print(f"Draw detected. Ending episode. rewards len:{len(rewards)}")
            break
        #print(f"on {action[0], action[1]}")
        new_state, reward, done, draw, (x,y) = env.black_step(*action)
        if draw:
            continue;


        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)
        states.append(state_tensor.squeeze(0))
        actions.append(torch.tensor(action, dtype=torch.long, device=device))
        rewards.append(reward)


        if done:
            if reward > 0:
                blackwin = True
            win_coords.append((x,y))
            break

        state = new_state


    return states, actions, rewards, blackwin


# 计算折扣回报（G_t）
def compute_returns(rewards, gamma=0.99):
    returns = []
    G = 0
    for r in reversed(rewards):
        G = r + gamma * G
        returns.insert(0, G)  # 按时间步正序排列
    return torch.tensor(returns, dtype=torch.float32, device=device)


def train_policy_network(policy, optimizer, states, actions, returns):
    """
    policy: 策略网络
    optimizer: 优化器
    states:  [T, BOARD_SIZE, BOARD_SIZE]，每一步的棋盘状态
    actions: [T, 2]，每一步 (x, y) 位置的动作
    returns: [T]，每一步的折扣回报
    """
    states = torch.stack(states)  # [T, 1, BOARD_SIZE, BOARD_SIZE]
    actions = torch.stack(actions)  # [T, 2]，包含 x 和 y 坐标

    # **Step 1: 计算 state_mask**
    state_mask = (states != 0).float().squeeze(1)  # [T, BOARD_SIZE, BOARD_SIZE]，非 0（已落子）为 1，空位为 0

    # **Step 2: 计算策略网络输出**
    probs = policy(states)  # [T, BOARD_SIZE, BOARD_SIZE]

    # **Step 3: 过滤已落子点**
    masked_probs = probs * (1 - state_mask)  # 已落子的位置概率设为 0
    masked_probs = masked_probs / (masked_probs.sum(dim=(1, 2), keepdim=True) + 1e-9)  # 重新归一化

    # **Step 4: 计算 log 概率**
    # 从 masked_probs 这个 [T, BOARD_SIZE, BOARD_SIZE] 的张量中，提取每一步行动 actions[x, y] 处的概率, 并求log()
    log_probs = torch.log( masked_probs[torch.arange(actions.shape[0]), actions[:, 0], actions[:, 1]] + 1e-9)

    # **Step 5: 归一化 returns**
    returns = (returns - returns.mean()) / (returns.std() + 1e-9)

    # **Step 6: 计算损失**
    loss = -(log_probs * returns).mean()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()

# 主训练循环
def train(policy_net, episodes=50000, checkpoint_path='policy.pth', gamma=0.99):
    optimizer = optim.Adam(policy_net.parameters(), lr=0.00001)

    # 加载已有的模型参数
    if os.path.exists(checkpoint_path):
        policy_net.load_state_dict(torch.load(checkpoint_path, map_location=device))


    policy_net = policy_net.to(device)

    win_cnt = 0

    win_coords = []
    for episode in range(episodes):
        # 运行一局游戏，收集 (states, actions, rewards), win_coords收集一段时间的赢的坐标，用于监控是不是棋局总是在重复
        states, actions, rewards, blackwin = play_one_episode(policy_net,  win_coords)
        if blackwin:
            win_cnt += 1

        # 计算折扣回报
        returns = compute_returns(rewards, gamma)

        # 更新策略网络
        loss = train_policy_network(policy_net, optimizer, states, actions, returns)


        # 每 128 轮更新对手策略，
        update_interval = 128
        if episode % update_interval == 0:
            path = checkpoint_path
            if win_cnt / update_interval > 0.4:
                path = f"simple_gomoku_{int(win_cnt*100 / update_interval)}.pth"
            torch.save(policy_net.state_dict(), path)
            entro = entropy(win_coords)
            print(f'Episode {episode}: Model updated, checkpoint saved, Loss: {loss:.4f}, win_ratio:{win_cnt / update_interval:.2f},entropy:{entro:.2f}')
            win_cnt = 0
            win_coords = []

def play_with_human(checkpoint_path='policy.pth'):
    env = Gomoku()

    policy_net = PolicyNet().to(device)
    policy_net.load_state_dict(torch.load(checkpoint_path))
    state = env.reset()
    turn = 1  # AI 执黑，人类执白
    done = False

    while not done:
        if turn == 1:
            action = select_action(policy_net, state)
            state, _, done = env.step(1, *action)  # AI 落子
        else:
            state, _, done = env.human_play(-1)  # 人类落子

        turn *= -1  # 轮流落子


if __name__ == "__main__":
    import argparse
    policy = PolicyNet().to(device)
    arg = "test"
    if arg == "train":
        train(policy)
    else:
        play_with_human('./simple_gomoku_53.pth')

```



#### 3.4 实操3 左右博弈五子棋 v2

参考AlphaGo的训练方式，修改一下：

1. 状态的表示，用10个通道，而不是只有一个通道。分别表示黑子和白子过去5步的棋子布局
3. 网络结构也调整一下，加了残差网络、归一化、1x1卷积等
4. 棋盘缩小到5x5，连成3个即为胜出
5. 五子棋类Gomoku增加了基于规则搜索的人肉智能：
   1. ai_play_episode()两个人肉智能对弈一局，记录下棋谱可以供RL训练；
   2. black_step()用于RL作为黑方下了后，人肉智能作为对手会再落子并返回新的状态

五子棋代码：

```python
import random
import time
import numpy as np
import pygame
import sys
import torch
from exceptiongroup import catch

# 定义颜色
WHITE = (255, 255, 255)
BLACK = (0, 0, 0)
GRAY = (200, 200, 200)
GREEN = (0, 255, 0)

# 棋盘大小
BOARD_SIZE = 5
CELL_SIZE = 50
WINDOW_SIZE = BOARD_SIZE * CELL_SIZE


class Gomoku:
    def __init__(self):
        """ 初始化五子棋环境 """
        pygame.init()
        self.screen = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))
        pygame.display.set_caption("五子棋")
        self.font = pygame.font.Font(None, 36)
        self.reset()

    def reset(self):
        """ 重新初始化棋局，在中心落一个黑子 """
        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)
        self.last_move = None
        # 随机初始位置
        '''x = random.randint(0, BOARD_SIZE - 1)
        y = random.randint(0, BOARD_SIZE - 1)
        self.board[x, y] = 1  # 黑棋先手
        self.last_move = (x, y)'''
        self.game_over = False
        self.winner = None
        return self.board.copy()

    def step(self, role, x, y):
        """ 执行落子，并计算局部奖励 """
        if self.game_over or self.board[x, y] != 0:
            return self.board.copy(), 0, self.game_over  # 非法落子

        self.board[x, y] = role
        self.last_move = (x, y)

        # 检查胜负
        if self.check_win(x, y, role):
            self.game_over = True
            self.winner = role
            reward = 1 if role == 1 else -1
        else:
            reward = 0
        self.render()
        return self.board.copy(), reward, self.game_over

    def black_step(self, x, y):
        """ 黑方落子，然后白方智能应对 """
        draw = False
        # 黑方落子
        state, reward, done = self.step(1, x, y)
        if done:
            return state, reward, done, draw, (x,y)

        # 白方智能落子
        if not done:
            white_x, white_y = self.find_best_move(-1)
            if white_x is None:
                draw = True
                return None, None, None, draw, (x, y)
            state, reward, done = self.step(-1, white_x, white_y)

        return state, reward, done, draw, (white_x, white_y)

    def ai_play_episode(self, slow=False):
        """ 自动对弈一局，返回黑方的状态、动作和奖励序列 """
        states = []
        actions = []
        rewards = []


        self.reset()
        states.append(torch.tensor(self.board.copy()).unsqueeze(0))

        draw = False
        lastcoord = None
        while not self.game_over:
            # 黑方智能落子
            black_x, black_y = self.find_best_move(1)
            if black_x is None:
                draw = True
                break
            actions.append(torch.tensor([black_x, black_y]))
            state, reward, done, draw, lastcoord = self.black_step(black_x, black_y)
            if draw:
                break
            if slow:
                time.sleep(2)
            states.append( torch.tensor(state.copy()).unsqueeze(0))
            rewards.append(reward)
            if done :
                break


            # 处理游戏结束事件
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    sys.exit()
        if slow:
            time.sleep(2)
        return states[:-1], actions, rewards, draw, lastcoord  # 最后一个状态不需要

    def find_best_move(self, role):
        """ 智能寻找最佳落子位置 """
        opponent = -role

        # 1. 检查自己是否能直接获胜
        for x in range(BOARD_SIZE):
            for y in range(BOARD_SIZE):
                if self.board[x, y] == 0:
                    self.board[x, y] = role
                    if self.check_win(x, y, role):
                        self.board[x, y] = 0  # 恢复
                        return x, y
                    self.board[x, y] = 0  # 恢复

        # 2. 检查对手是否能直接获胜，需要防守
        for x in range(BOARD_SIZE):
            for y in range(BOARD_SIZE):
                if self.board[x, y] == 0:
                    self.board[x, y] = opponent
                    if self.check_win(x, y, opponent):
                        self.board[x, y] = 0  # 恢复
                        return x, y
                    self.board[x, y] = 0  # 恢复

        # 3. 评估每个空位的得分
        best_score = -1
        best_moves = []
        for x in range(BOARD_SIZE):
            for y in range(BOARD_SIZE):
                if self.board[x, y] == 0:
                    score = self.evaluate_position(x, y, role)
                    if score > best_score:
                        best_score = score
                        best_moves = [(x, y)]
                    elif score == best_score:
                        best_moves.append((x, y))

        # 从最佳候选位置中随机选择一个
        if len(best_moves) == 0:
            return None,None
        else:
            return random.choice(best_moves)

    def evaluate_position(self, x, y, role):
        """ 评估在(x,y)落子的价值 """
        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]
        total_score = 0

        for dx, dy in directions:
            # 计算这个方向上的棋型
            line = []
            for d in [-4, -3, -2, -1, 0, 1, 2, 3, 4]:  # 检查9个位置
                nx, ny = x + d * dx, y + d * dy
                if 0 <= nx < BOARD_SIZE and 0 <= ny < BOARD_SIZE:
                    line.append(self.board[nx, ny])
                else:
                    line.append(2)  # 边界外

            # 只关注以(x,y)为中心的5个位置
            center = BOARD_SIZE // 2  # 因为前面检查了9个位置，(x,y)在中间
            segment = line[center - BOARD_SIZE // 2:center + BOARD_SIZE // 2+1]

            # 计算这个方向的得分
            total_score += self.evaluate_segment(segment, role)

        return total_score

    def evaluate_segment(self, segment, role):
        """ 评估一个5连位置的得分 """
        opponent = -role
        count_role = segment.count(role)
        count_opponent = segment.count(opponent)

        # 如果有对手的棋子，这个位置价值降低
        if count_opponent > 0:
            return 0

        # 根据连子数给分
        if count_role == 4: return 10000  # 活四
        if count_role == 3: return 1000  # 活三
        if count_role == 2: return 100  # 活二
        if count_role == 1: return 10  # 活一
        return 1  # 空位

    def check_win(self, x, y, role):
        """ 判断当前落子是否形成五连胜 """
        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]
        for dx, dy in directions:
            count = 1
            for d in [-1, 1]:  # 计算两个方向
                nx, ny = x, y
                while True:
                    nx += d * dx
                    ny += d * dy
                    if 0 <= nx < BOARD_SIZE and 0 <= ny < BOARD_SIZE and self.board[nx, ny] == role:
                        count += 1
                    else:
                        break
            if count >= 3:
                return True
        return False

    def render(self):
        """ 渲染棋盘 """
        self.screen.fill(WHITE)

        # 画网格
        for i in range(BOARD_SIZE):
            pygame.draw.line(self.screen, GRAY, (i * CELL_SIZE + CELL_SIZE // 2, CELL_SIZE // 2),
                             (i * CELL_SIZE + CELL_SIZE // 2, WINDOW_SIZE - CELL_SIZE // 2), 2)
            pygame.draw.line(self.screen, GRAY, (CELL_SIZE // 2, i * CELL_SIZE + CELL_SIZE // 2),
                             (WINDOW_SIZE - CELL_SIZE // 2, i * CELL_SIZE + CELL_SIZE // 2), 2)

        # 画棋子
        for x in range(BOARD_SIZE):
            for y in range(BOARD_SIZE):
                if self.board[x, y] == 1:  # 黑子
                    pygame.draw.circle(self.screen, BLACK, (y * CELL_SIZE + CELL_SIZE // 2,
                                                            x * CELL_SIZE + CELL_SIZE // 2), CELL_SIZE // 3)
                elif self.board[x, y] == -1:  # 白子
                    pygame.draw.circle(self.screen, WHITE, (y * CELL_SIZE + CELL_SIZE // 2,
                                                            x * CELL_SIZE + CELL_SIZE // 2), CELL_SIZE // 3)
                    pygame.draw.circle(self.screen, BLACK, (y * CELL_SIZE + CELL_SIZE // 2,
                                                            x * CELL_SIZE + CELL_SIZE // 2), CELL_SIZE // 3, 2)

        # 标记最后一手棋
        if self.last_move:
            lx, ly = self.last_move
            pygame.draw.circle(self.screen, GREEN, (ly * CELL_SIZE + CELL_SIZE // 2,
                                                    lx * CELL_SIZE + CELL_SIZE // 2), CELL_SIZE // 3 + 3, 2)

        # 游戏结束显示赢家
        if self.game_over:
            msg = "black wins." if self.winner == 1 else "white wins."
            text = self.font.render(msg, True, BLACK)
            self.screen.blit(text, (WINDOW_SIZE // 3, WINDOW_SIZE // 2))

        pygame.display.flip()

    def play_human(self):
        """ 让人类玩家交互式落子 """
        running = True
        role = 1
        while running:
            self.render()
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    running = False
                    pygame.quit()
                    sys.exit()
                elif event.type == pygame.MOUSEBUTTONDOWN and not self.game_over:
                    x, y = event.pos[1] // CELL_SIZE, event.pos[0] // CELL_SIZE
                    _, r, done = self.step(role, x, y)
                    print(f"r={r}")
                    if not done:
                        role *= -1  # 轮到另一方落子
        pygame.quit()

    def human_play(self, role):
        """ 让人类玩家落子，阻塞直到有效输入，并返回新状态 """
        self.render()

        while True:  # 阻塞等待有效输入
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    exit()  # 退出程序

                elif event.type == pygame.MOUSEBUTTONDOWN and not self.game_over:
                    x, y = event.pos[1] // CELL_SIZE, event.pos[0] // CELL_SIZE

                    # 确保落子位置未被占据
                    if self.board[x, y] == 0:
                        state, reward, done,  = self.step(role,  x, y)
                        return state, reward, done  # 返回新状态、奖励、是否终局


def main():
    go = Gomoku()
    print(5//2)
    go.ai_play_episode(True)








if __name__ == "__main__":
    main()
```

训练的代码：

```python
import time

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import numpy as np
from collections import Counter
import os
from gomoku import BOARD_SIZE

from gomoku import Gomoku  # 假设 gomoku.py 里定义了五子棋环境

# 设备初始化
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
'''
'''
HISTORY_LEN=5


# 定义一个残差块
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)

    def forward(self, x):
        residual = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += residual  # 残差连接
        return F.relu(out)


# 定义策略网络
class PolicyNet(nn.Module):
    def __init__(self):
        super(PolicyNet, self).__init__()
        # 初始卷积层 + BN
        self.conv1 = nn.Conv2d(HISTORY_LEN * 2, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)

        # 添加两个残差块
        self.res_block1 = ResidualBlock(128)
        self.res_block2 = ResidualBlock(128)

        # 策略头：使用 1x1 卷积替代全连接层
        self.policy_conv = nn.Conv2d(128, 4, kernel_size=1)
        self.policy_bn = nn.BatchNorm2d(4)
        self.policy_out = nn.Conv2d(4, 1, kernel_size=1)

    def forward(self, state):
        batch_size = state.size(0)

        # 计算合法落子 mask（state=0 的地方可以落子）
        # 这里假设通道0和通道5代表双方棋子
        mask = state[:, 0, :, :] + state[:, 5, :, :]
        mask = (mask.view(batch_size, BOARD_SIZE, BOARD_SIZE) == 0).float()
        mask = mask.view(batch_size, BOARD_SIZE*BOARD_SIZE)

        # 通过卷积层提取特征，后接BN与ReLU
        x = F.relu(self.bn1(self.conv1(state)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))

        # 通过残差块进一步提取特征
        x = self.res_block1(x)
        x = self.res_block2(x)

        # 策略头：1x1卷积层 + BN + ReLU
        x = F.relu(self.policy_bn(self.policy_conv(x)))
        x = self.policy_out(x)  # 输出 shape: [batch, 1, BOARD_SIZE, BOARD_SIZE]

        # 展平成 [batch, 81] 作为 logits
        x = x.view(batch_size, BOARD_SIZE*BOARD_SIZE)

        # 对不可落子的位置赋极小值 (-inf)
        x = x.masked_fill(mask == 0, float('-inf'))

        # 计算 softmax 仅在合法位置
        x = F.softmax(x, dim=1)

        return x.view(batch_size, BOARD_SIZE, BOARD_SIZE)

# 选择动作
def select_action(policy_net, state):
    """
    选择动作，避免已落子点，并处理棋盘已满的情况。

    :param policy_net: 策略网络
    :param state: 当前棋盘状态 (9x9)
    :return: 选择的动作坐标 (x, y) 或 None（表示平局）
    """
    state = torch.stack(state).to(device)
    state_tensor = torch.tensor(state, dtype=torch.float32, device=device)
    state_tensor = reorganize_state(state_tensor, True)
    policy_net.eval()
    with torch.no_grad():
        action_probs = policy_net(state_tensor).squeeze().detach().cpu().numpy()  # 转到CPU计算
    policy_net.train()


    # 不能选择已落子点
    action_probs[state[-1,-1].cpu() != 0] = 0 # 取最新的一次state里的

    total_prob = action_probs.sum()
    if total_prob == 0:
        policy_net(state_tensor)
        return None  # 棋盘已满，平局

    action_probs /= total_prob  # 归一化
    choice = np.random.choice(BOARD_SIZE * BOARD_SIZE, p=action_probs.flatten())
    return divmod(choice, BOARD_SIZE)  # 返回 (x, y) 坐标

def entropy(coords):
    if not coords:
        return 0  # 空列表的熵定义为0

    counter = Counter(coords)  # 统计每个坐标的出现次数
    total = len(coords)  # 总数
    probs = np.array([count / total for count in counter.values()])  # 计算概率分布
    #print(f"{len(coords)}, {len(probs)}")
    return -np.sum(probs * np.log2(probs))  # 计算熵（以2为底）

# 运行完整一局游戏，收集经验
def play_one_episode(policy_net,  win_coords):
    env = Gomoku()
    state = env.reset()
    states, actions, rewards = [], [], []
    blackwin = False
    draw = False


    while True:
        history = states.copy()
        history.append(torch.tensor(state).unsqueeze(0).to(device))
        action = select_action(policy_net, history)

        if action is None:  # 平局
            print(f"Draw detected. Ending episode. rewards len:{len(rewards)}")
            draw = True
            break
        new_state, reward, done, draw, lastcoord = env.black_step(*action)


        #time.sleep(2)

        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)
        states.append(state_tensor.squeeze(0))
        actions.append(torch.tensor(action, dtype=torch.long, device=device))
        rewards.append(reward)


        if done:
            if  reward > 0:
                blackwin = True
            win_coords.append(lastcoord)
            break
        state = new_state


    return states, actions, rewards, blackwin, draw


# 计算折扣回报（G_t）
def compute_returns(rewards, gamma=1):
    returns = []
    G = 0
    for r in reversed(rewards):
        G = r + gamma * G
        returns.insert(0, G)  # 按时间步正序排列
    return torch.tensor(returns, dtype=torch.float32, device=device)


def reorganize_state(states: torch.Tensor, only_return_last_one=False):
    # states本身的shape：batchsz, 1, BOARD_SIZE, BOARD_SIZE
    batchsz = len(states)
    state_tensor = torch.zeros(batchsz, HISTORY_LEN * 2, BOARD_SIZE, BOARD_SIZE)

    for i in range(batchsz): #每个样本
        for j in range(HISTORY_LEN): #历史状态
            if i - j < 0:
                break
            s = states[i - j][0]
            state_tensor[i, j] = (s == 1).int()  # 通道 0 1 2 3 4 存放黑棋(自己)的最近5步
            state_tensor[i, 5+j] = (s == -1).int()  # 通道 5 6 7 8 9 存放白棋（对手）的最近5步
    state_tensor = state_tensor.to(device)
    if only_return_last_one:
        return state_tensor[-1:, :,:,:]
    else:
        return state_tensor

def train_policy_network(policy, optimizer, states, actions, returns):
    """
    policy: 策略网络
    optimizer: 优化器
    states:  [T, BOARD_SIZE, BOARD_SIZE]，每一步的棋盘状态
    actions: [T, 2]，每一步 (x, y) 位置的动作
    returns: [T]，每一步的折扣回报
    """

    states = torch.stack(states).to(device)  # [T, 1, BOARD_SIZE, BOARD_SIZE]
    actions = torch.stack(actions).to(device)  # [T, 2]，包含 x 和 y 坐标

    # **Step 1: 计算 state_mask**
    state_mask = (states != 0).float().squeeze(1)  # [T, BOARD_SIZE, BOARD_SIZE]，非 0（已落子）为 1，空位为 0

    states = reorganize_state(states)

    # **Step 2: 计算策略网络输出**
    probs = policy(states)  # [T, BOARD_SIZE, BOARD_SIZE]

    # **Step 3: 过滤已落子点**
    masked_probs = probs * (1 - state_mask)  # 已落子的位置概率设为 0
    masked_probs = masked_probs / (masked_probs.sum(dim=(1, 2), keepdim=True) + 1e-9)  # 重新归一化

    # **Step 4: 计算 log 概率**
    # 从 masked_probs 这个 [T, BOARD_SIZE, BOARD_SIZE] 的张量中，提取每一步行动 actions[x, y] 处的概率, 并求log()
    log_probs = torch.log( masked_probs[torch.arange(actions.shape[0]), actions[:, 0], actions[:, 1]] + 1e-9)

    # **Step 5: 归一化 returns**
    returns = (returns - returns.mean()) / (returns.std() + 1e-9)

    # **Step 6: 计算损失**
    loss = -(log_probs * returns).mean()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()

# 主训练循环
def train_by_human_rules(policy_net, episodes=500000, checkpoint_path='policy.pth', gamma=0.99):

    optimizer = optim.Adam(policy_net.parameters(), lr=0.0001)

    # 加载已有的模型参数
    if os.path.exists(checkpoint_path):
        policy_net.load_state_dict(torch.load(checkpoint_path, map_location=device))



    policy_net = policy_net.to(device)

    win_cnt = 0

    win_coords = []
    for episode in range(episodes):

        env = Gomoku()
        env.reset()
        # 植入了人肉智能的两个自动程序下棋，把回合记录下来用于训练
        states, actions, rewards, draw, lastcoord = env.ai_play_episode()
        if rewards[-1] > 0:
            blackwin = True
        else:
            blackwin = False

        if blackwin:
            win_cnt += 1
        if draw:  #平局导致return都为0，没有必要训练
            continue

        win_coords.append(lastcoord)

        # 计算折扣回报
        returns = compute_returns(rewards,gamma)

        # 更新策略网络
        loss = train_policy_network(policy_net, optimizer, states, actions, returns)

        update_interval = 128
        if episode % update_interval == 0:
            torch.save(policy_net.state_dict(), checkpoint_path)
            entro = entropy(win_coords)
            print(f'Episode {episode}: Model updated, checkpoint saved, Loss: {loss:.4f}, win_ratio:{win_cnt / update_interval:.2f},entropy:{entro:.2f}')
            win_cnt = 0
            win_coords = []

def train_by_self_play(policy_net, episodes=100000, checkpoint_path='policy.pth', gamma=0.9):

    optimizer = optim.Adam(policy_net.parameters(), lr=0.0001)

    # 加载已有的模型参数
    if os.path.exists(checkpoint_path):
        policy_net.load_state_dict(torch.load(checkpoint_path, map_location=device))


    policy_net = policy_net.to(device)

    win_cnt = 0

    win_coords = []
    for episode in range(episodes):
        # 网络基于自己的策略，跟人肉智能对弈一个回合，收集 (states, actions, rewards),
        # win_coords收集一段时间的赢的坐标，用于监控是不是棋局总是在重复
        states, actions, rewards, blackwin, draw = play_one_episode(policy_net, win_coords)


        if blackwin:
            win_cnt += 1
        if draw:  #平局导致return都为0，没有必要训练
            continue

        # 计算折扣回报
        returns = compute_returns(rewards,gamma)

        # 更新策略网络
        loss = train_policy_network(policy_net, optimizer, states, actions, returns)

        update_interval = 128
        if episode % update_interval == 0:
            path = checkpoint_path
            if win_cnt / update_interval > 0.4:
                path=f"./gomoku_{int(win_cnt*100 / update_interval)}.pth"
            torch.save(policy_net.state_dict(), path)
            entro = entropy(win_coords)
            print(f'Episode {episode}: Model updated, checkpoint saved, Loss: {loss:.4f}, win_ratio:{win_cnt / update_interval:.2f},entropy:{entro:.2f}')
            win_cnt = 0
            win_coords = []



def play_with_human(checkpoint_path='policy.pth'):
    env = Gomoku()

    policy_net = PolicyNet().to(device)
    policy_net.load_state_dict(torch.load(checkpoint_path))
    print(f"load {checkpoint_path} successfully")
    state = env.reset()
    turn = 1  # AI 执黑，人类执白
    done = False
    states = []

    while not done:
        if turn == 1:
            history = states.copy()
            history.append(torch.tensor(state).unsqueeze(0).to(device))
            action = select_action(policy_net, history)
            state, _, done = env.step(1, *action)  # AI 落子
            if done:
                print("AI win")
            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)
            states.append(state_tensor.squeeze(0))
        else:
            state, _, done = env.human_play(-1)  # 人类落子
            if done:
                print("human win")

        turn *= -1  # 轮流落子


if __name__ == "__main__":
    import argparse
    policy = PolicyNet().to(device)
    arg = "train"
    if arg == "train":
        train_by_human_rules(policy)
    else:
        play_with_human(checkpoint_path='./policy.pth')

```

学习完22万次“人肉智能”的黑方棋谱后，前后对比能看到明显有提升：

[训练前的对弈视频，相当愚蠢](img/RL/stupid_chess.mp4)

[训练后的对弈视频，有一定的智能](img/RL/clever_chess.mp4)

从0开始，RL通过直接与人肉智能对弈，也能学习到一些技能，胜率高的时候能到70%，但不能稳定在一个较高的胜率上。

### 4. 方式三：Actor-Critic方法

#### 4.1 原理

![image-20250326143312064](img/RL/image-20250326143312064.png)



按照上图进行的训练，我没有能够收敛，所以我又找了[李宏毅老师的课程](https://www.bilibili.com/video/BV15hw9euExZ?spm_id_from=333.788.videopod.episodes&vd_source=2173cb93b451f2278a1c87becf3ef529&p=3)学习，总结如下，并在4.3的训练里收敛了：

![image-20250327090246933](img/RL/image-20250327090246933.png)

进一步理解一下：

![image-20250329173212697](img/RL/image-20250329173212697.png)

#### 4.2 实操1 按照王树森老师的课程进行训练

按照上面的算法，代码如下，但是不收敛：

```python
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import argparse
import os
import matplotlib.pyplot as plt

# 设置随机种子
torch.manual_seed(42)
np.random.seed(42)

# 检测设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


# ----------------------------
# 1. 策略网络定义
# ----------------------------
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, action_dim)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        if len(x.shape) == 1:
            x = x.unsqueeze(0)  # [state_dim] -> [1, state_dim]
        x = torch.relu(self.fc1(x))  # [batch_size, hidden_dim]
        logits = self.fc2(x)  # [batch_size, action_dim]
        probs = self.softmax(logits)  # [batch_size, action_dim]
        return probs


class Critic(nn.Module):
    def __init__(self, state_dim=4, action_dim=1, hidden_dim=128):
        super(Critic, self).__init__()
        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)  # 输出单个Q值
        self.relu = nn.ReLU()

    def forward(self, state, action):
        x = torch.cat([state, action], dim=-1)  # 拼接状态和动作
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        q_value = self.fc3(x)
        return q_value


# ----------------------------
# 2. Checkpoint 保存与加载
# ----------------------------
def save_checkpoint(policy, optimizer1, critic, optimizer2, episode, reward, path):
    torch.save({
        'policy_state_dict': policy.state_dict(),
        'optimizer1_state_dict': optimizer1.state_dict(),
        'critic_state_dict': critic.state_dict(),
        'optimizer2_state_dict': optimizer2.state_dict(),
        'episode': episode,
        'reward': reward
    }, path)
    print(f"Checkpoint saved to {path} (Reward: {reward:.2f})")


def load_checkpoint(policy, optimizer1, critic, optimizer2, path):
    if os.path.exists(path):
        checkpoint = torch.load(path)
        policy.load_state_dict(checkpoint['policy_state_dict'])
        optimizer1.load_state_dict(checkpoint['optimizer1_state_dict'])
        critic.load_state_dict(checkpoint['critic_state_dict'])
        optimizer2.load_state_dict(checkpoint['optimizer2_state_dict'])
        print(f"Loaded checkpoint from {path} (Episode: {checkpoint['episode']}, Reward: {checkpoint['reward']:.2f})")
        return checkpoint['episode'], checkpoint['reward']
    else:
        print(f"No checkpoint found at {path}")
        return 0, 0


# ----------------------------
# 3. 训练函数（带Checkpoint和可视化）
# ----------------------------
def train(env_name="CartPole-v1", hidden_dim=128, lr=1e-3,
          gamma=0.99, max_episodes=10000, print_interval=2):
    env = gym.make(env_name, render_mode="human")  # 开启可视化
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)
    critic = Critic(state_dim, action_dim, hidden_dim).to(device)
    optimizer1 = optim.Adam(policy.parameters(), lr=lr/10.0)
    optimizer2 = optim.Adam(critic.parameters(), lr=lr)

    # 尝试加载Checkpoint
    checkpoint_path = "./actor_critic_checkpoint_150_2955xx.0.pth"
    start_episode, _ = load_checkpoint(policy, optimizer1, critic, optimizer2, checkpoint_path)

    rewards_list = []
    for episode in range(start_episode, max_episodes):
        state = env.reset()
        state = state[0]
        episode_reward = 0
        episode_loss = 0.0
        step_cnt = 0

        while True:
            # randomly sample action from PolicyNet
            state_tensor = torch.FloatTensor(state).to(device)
            state_tensor = state_tensor.unsqueeze(0)
            probs = policy(state_tensor)
            m = Categorical(probs) #根据各种action的概率值probs创建一个离散的概率分布
            action = m.sample() #使用该概率分布进行抽样，得到一个具体的action
            log_probs = m.log_prob(action)

            # perform the action
            next_state, reward, done, _, _ = env.step(action.item())
            episode_reward += reward

            # randomly sample next_action from policynet,but do not perform it
            policy.eval()
            with torch.no_grad():
                state_tensor = torch.FloatTensor(next_state).to(device)
                state_tensor = state_tensor.unsqueeze(0)
                probs = policy(state_tensor)
                m = Categorical(probs)  # 根据各种action的概率值probs创建一个离散的概率分布
                next_action = m.sample()  # 使用该概率分布进行抽样，得到一个具体的action
            policy.train()

            # evaluate value network twice
            state_tensor = torch.FloatTensor(state).to(device)
            state_tensor = state_tensor.unsqueeze(0)
            action_tensor = torch.tensor([0,0], dtype=torch.float, device=device)
            action_tensor[action.item()] = 1.0 # one-hot编码action
            action_tensor = action_tensor.unsqueeze(0)
            qt = critic.forward(state_tensor, action_tensor)

            critic.eval()
            with torch.no_grad():
                state_tensor = torch.FloatTensor(next_state).to(device)
                state_tensor = state_tensor.unsqueeze(0)
                action_tensor = torch.tensor([0, 0], dtype=torch.float, device=device)
                action_tensor[next_action.item()] = 1.0  # one-hot编码action
                action_tensor = action_tensor.unsqueeze(0)
                qt_next = critic.forward(state_tensor, action_tensor)
            critic.train()

            # calculate TD error and update critc network
            loss1 = (qt - (reward + gamma * qt_next)) * (qt - (reward + gamma * qt_next))  / 2.0
            optimizer2.zero_grad()
            loss1.backward()
            optimizer2.step()
            episode_loss += loss1.item()

            # update policy network
            loss2 = -(log_probs * qt.detach()).mean()
            optimizer1.zero_grad()
            loss2.backward()
            optimizer1.step()
            episode_loss += loss2.item()

            state = next_state
            step_cnt += 1
            if done:
                break
            '''到后面模型能力强了，游戏一把玩好久也不死。就导致两次训练的时间间隔很长
            粗暴的截断会有两个问题：
            1、可能导致游戏后面才会出现的states，从来没有出现在训练集里，使得模型失去泛化能力
            2、compute_returns不准确了，这个可能对于密集奖励型游戏还好，毕竟到了后面状态，伽马的n次方接近0，后面的项影响有限
            '''
            '''if  len(rewards) > 1000:
                print('brutally cut')
                break'''

        rewards_list.append(episode_reward)
        if len(rewards_list) > 10000:
            rewards_list = rewards_list[-10000:]
        # 保存Checkpoint（如果回报>1000）
        if episode_reward > 1000:
            checkpoint_path = f"./actor_critic_checkpoint_{episode}_{episode_reward}.pth"
            save_checkpoint(policy, optimizer1, critic, optimizer2,episode, episode_reward, checkpoint_path)

        print(f"Episode {episode + 1}, Reward: {episode_reward:.2f}, Loss: {episode_loss / step_cnt:.4f}")


    env.close()
    return rewards_list


# ----------------------------
# 4. 推理函数（演示训练好的模型）
# ----------------------------
def test(env_name="CartPole-v1", hidden_dim=128, checkpoint_path="checkpoint.pth"):
    env = gym.make(env_name, render_mode="human")  # 开启可视化
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)
    critic = Critic(state_dim, action_dim, hidden_dim).to(device) #占位符，不会被用于推理
    optimizer = optim.Adam(policy.parameters(), lr=0.001)  # 仅占位，实际不用于测试
    optimizer2 = optim.Adam(critic.parameters(), lr=0.001)  # 仅占位，实际不用于测试

    # 加载Checkpoint
    load_checkpoint(policy, optimizer, critic, optimizer2, checkpoint_path)

    print("Starting inference...")
    while True:  # 无限运行直到手动停止
        state = env.reset()[0]
        total_reward = 0

        while True:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).to(device)
                probs = policy(state_tensor)
                action = torch.argmax(probs).item()  # 直接选最优动作

            next_state, reward, done, _, _ = env.step(action)
            total_reward += reward
            state = next_state

            if done:
                print(f"Inference Reward: {total_reward}")
                break



# ----------------------------
# 6. 主函数（命令行参数解析）
# ----------------------------
def main(mode):
    if mode == "train":
        rewards = train()
        plt.plot(rewards)
        plt.xlabel("Episode")
        plt.ylabel("Total Reward")
        plt.title("Policy Gradient Training")
        plt.show()
    elif mode == "test":
        test(checkpoint_path="./actor_critic_checkpoint_1001.0.pth")


if __name__ == "__main__":
    main("train")
```



#### 4.3 实操2 按照李宏毅老师的课程进行训练

下面的代码，模型能够收敛，能上2万多分：

```python
Episode 298, Reward: 372.00, Loss: 1.0105
Checkpoint saved to ./actor_critic_checkpoint_298_26247.0.pth (Reward: 26247.00)
```

代码如下：

```python
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import argparse
import os
import matplotlib.pyplot as plt

# 设置随机种子
torch.manual_seed(42)
np.random.seed(42)

# 检测设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


# ----------------------------
# 1. 策略网络定义
# ----------------------------
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, action_dim)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        if len(x.shape) == 1:
            x = x.unsqueeze(0)  # [state_dim] -> [1, state_dim]
        x = torch.relu(self.fc1(x))  # [batch_size, hidden_dim]
        logits = self.fc2(x)  # [batch_size, action_dim]
        probs = self.softmax(logits)  # [batch_size, action_dim]
        return probs

# 价值网络的定义
class Critic(nn.Module):
    def __init__(self, state_dim=4, hidden_dim=128):
        super(Critic, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)  # 输出状态价值V
        self.relu = nn.ReLU()

    def forward(self, state):
        x = self.relu(self.fc1(state))
        x = self.relu(self.fc2(x))
        value = self.fc3(x)
        return value


# ----------------------------
# 2. Checkpoint 保存与加载
# ----------------------------
def save_checkpoint(policy, optimizer1, critic, optimizer2, episode, reward, path):
    torch.save({
        'policy_state_dict': policy.state_dict(),
        'optimizer1_state_dict': optimizer1.state_dict(),
        'critic_state_dict': critic.state_dict(),
        'optimizer2_state_dict': optimizer2.state_dict(),
        'episode': episode,
        'reward': reward
    }, path)
    print(f"Checkpoint saved to {path} (Reward: {reward:.2f})")


def load_checkpoint(policy, optimizer1, critic, optimizer2, path):
    if os.path.exists(path):
        checkpoint = torch.load(path)
        policy.load_state_dict(checkpoint['policy_state_dict'])
        optimizer1.load_state_dict(checkpoint['optimizer1_state_dict'])
        critic.load_state_dict(checkpoint['critic_state_dict'])
        optimizer2.load_state_dict(checkpoint['optimizer2_state_dict'])
        print(f"Loaded checkpoint from {path} (Episode: {checkpoint['episode']}, Reward: {checkpoint['reward']:.2f})")
        return checkpoint['episode'], checkpoint['reward']
    else:
        print(f"No checkpoint found at {path}")
        return 0, 0


# ----------------------------
# 3. 训练函数（带Checkpoint和可视化）
# ----------------------------
def train(env_name="CartPole-v1", hidden_dim=128, lr=1e-3,
          gamma=0.99, max_episodes=10000, print_interval=2):
    env = gym.make(env_name, render_mode="human")  # 开启可视化
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)
    critic = Critic().to(device)
    optimizer1 = optim.Adam(policy.parameters(), lr=lr)
    optimizer2 = optim.Adam(critic.parameters(), lr=lr)

    # 尝试加载Checkpoint
    checkpoint_path = "./actor_critic_checkpoint_150_2955xx.0.pth"
    start_episode, _ = load_checkpoint(policy, optimizer1, critic, optimizer2, checkpoint_path)

    rewards_list = []
    for episode in range(start_episode, max_episodes):
        state = env.reset()
        state = state[0]
        episode_reward = 0
        episode_loss = 0.0
        step_cnt = 0

        while True:
            # 选择动作
            state_tensor = torch.FloatTensor(state).to(device).unsqueeze(0)
            probs = policy(state_tensor)
            m = Categorical(probs)
            action = m.sample()
            log_prob = m.log_prob(action)

            # 执行动作
            next_state, reward, done, _, _ = env.step(action.item())
            episode_reward += reward

            # 计算V值
            state_value = critic(state_tensor)
            next_state_tensor = torch.FloatTensor(next_state).to(device).unsqueeze(0)
            next_state_value = critic(next_state_tensor).detach()

            # 计算TD误差(优势函数)
            td_error = reward + gamma * next_state_value * (1 - done) - state_value

            # 更新Critic
            critic_loss = td_error.pow(2).mean()
            optimizer2.zero_grad()
            critic_loss.backward()
            optimizer2.step()
            episode_loss += critic_loss.item()

            # 更新Actor
            policy_loss = -log_prob * td_error.detach()
            optimizer1.zero_grad()
            policy_loss.backward()
            optimizer1.step()
            episode_loss += policy_loss.item()

            state = next_state
            step_cnt += 1
            if done:
                break
            '''到后面模型能力强了，游戏一把玩好久也不死。就导致两次训练的时间间隔很长
            粗暴的截断会有两个问题：
            1、可能导致游戏后面才会出现的states，从来没有出现在训练集里，使得模型失去泛化能力
            2、compute_returns不准确了，这个可能对于密集奖励型游戏还好，毕竟到了后面状态，伽马的n次方接近0，后面的项影响有限
            '''
            '''if  len(rewards) > 1000:
                print('brutally cut')
                break'''

        rewards_list.append(episode_reward)
        if len(rewards_list) > 10000:
            rewards_list = rewards_list[-10000:]
        # 保存Checkpoint（如果回报>1000）
        if episode_reward > 1000:
            checkpoint_path = f"./actor_critic_checkpoint_{episode}_{episode_reward}.pth"
            save_checkpoint(policy, optimizer1, critic, optimizer2,episode, episode_reward, checkpoint_path)

        print(f"Episode {episode + 1}, Reward: {episode_reward:.2f}, Loss: {episode_loss / step_cnt:.4f}")


    env.close()
    return rewards_list


# ----------------------------
# 4. 推理函数（演示训练好的模型）
# ----------------------------
def test(env_name="CartPole-v1", hidden_dim=128, checkpoint_path="checkpoint.pth"):
    env = gym.make(env_name, render_mode="human")  # 开启可视化
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)
    critic = Critic().to(device) #占位符，不会被用于推理
    optimizer = optim.Adam(policy.parameters(), lr=0.001)  # 仅占位，实际不用于测试
    optimizer2 = optim.Adam(critic.parameters(), lr=0.001)  # 仅占位，实际不用于测试

    # 加载Checkpoint
    load_checkpoint(policy, optimizer, critic, optimizer2, checkpoint_path)

    print("Starting inference...")
    while True:  # 无限运行直到手动停止
        state = env.reset()[0]
        total_reward = 0

        while True:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).to(device)
                probs = policy(state_tensor)
                action = torch.argmax(probs).item()  # 直接选最优动作

            next_state, reward, done, _, _ = env.step(action)
            total_reward += reward
            state = next_state

            if done:
                print(f"Inference Reward: {total_reward}")
                break



# ----------------------------
# 6. 主函数（命令行参数解析）
# ----------------------------
def main(mode):
    if mode == "train":
        rewards = train()
        plt.plot(rewards)
        plt.xlabel("Episode")
        plt.ylabel("Total Reward")
        plt.title("Policy Gradient Training")
        plt.show()
    elif mode == "test":
        test(checkpoint_path="./actor_critic_checkpoint_1001.0.pth")


if __name__ == "__main__":
    main("train")
```

### 5. AlphaGo的原理

![image-20250327200522090](img/RL/image-20250327200522090.png)

### 6. 蒙特卡洛算法

蒙特卡洛算法是一种通过反复随机抽样以逼近某个数值的数值方法。

![image-20250401190436992](img/RL/image-20250401190436992.png)



![image-20250328160918242](img/RL/image-20250328160918242.png)

### 7. Off-Policy方法与近端策略优化(PPO)

#### 7.1 原理

1. 把on-policy方法转为off-policy方法，可以更高效，避免必须采样一回合才能训练一回合。
2. 但被训练的策略网络和负责与环境交互的策略网络不是同一个，采样是遵从后者的分布，这时候就需要用重要性采样对梯度计算进行调整。
3. 两者的分布不能相差太远，否则会触发重要性采样的issue，所以PPO会引入KL作为惩罚项



![image-20250329110628801](img/RL/image-20250329110628801.png)

#### 7.2 实操

还是平衡车小游戏：

```python
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import argparse
import os
import matplotlib.pyplot as plt

# 设置随机种子
torch.manual_seed(42)
np.random.seed(42)

# 检测设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


# ----------------------------
# 1. 策略网络定义
# ----------------------------
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, action_dim)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        if len(x.shape) == 1:
            x = x.unsqueeze(0)  # [state_dim] -> [1, state_dim]
        x = torch.relu(self.fc1(x))  # [batch_size, hidden_dim]
        logits = self.fc2(x)  # [batch_size, action_dim]
        probs = self.softmax(logits)  # [batch_size, action_dim]
        return probs


# ----------------------------
# 2. Checkpoint 保存与加载
# ----------------------------
def save_checkpoint(policy, optimizer, episode, reward, path="checkpoint.pth"):
    torch.save({
        'policy_state_dict': policy.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'episode': episode,
        'reward': reward
    }, path)
    print(f"Checkpoint saved to {path} (Reward: {reward:.2f})")


def load_checkpoint(policy, optimizer, path="checkpoint.pth"):
    if os.path.exists(path):
        checkpoint = torch.load(path)
        policy.load_state_dict(checkpoint['policy_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print(f"Loaded checkpoint from {path} (Episode: {checkpoint['episode']}, Reward: {checkpoint['reward']:.2f})")
        return checkpoint['episode'], checkpoint['reward']
    else:
        print(f"No checkpoint found at {path}")
        return 0, 0


# ----------------------------
# 3. 训练函数（带Checkpoint和可视化）
# ----------------------------
def train(env_name="CartPole-v1", hidden_dim=128, lr=1e-3,    # backup  lr=1e-2
          gamma=0.99, max_episodes=1000, print_interval=2):
    env = gym.make(env_name, render_mode="human")  # 开启可视化
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    learner = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device) #负责学习，更新梯度
    worker = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)  #负责与环境交互
    optimizer = optim.Adam(learner.parameters(), lr=lr)

    # 尝试加载Checkpoint
    checkpoint_path = "./ppo_checkpoint_150_2955xx.0.pth"
    start_episode, _ = load_checkpoint(learner, optimizer, checkpoint_path)
    worker.load_state_dict(learner.state_dict())

    episode_rewards = []

    for episode in range(start_episode, max_episodes):

        #用worker收集若干个回合的轨迹
        states, actions, rewards = [], [], []
        while len(actions) < 64:
            state = env.reset()
            state = state[0]
            while True:
                state_tensor = torch.FloatTensor(state).to(device)
                state_tensor = state_tensor.unsqueeze(0)
                probs = worker(state_tensor)
                m = Categorical(probs) #根据各种action的概率值probs创建一个离散的概率分布
                action = m.sample() #使用该概率分布进行抽样，得到一个具体的action

                next_state, reward, done, _, _ = env.step(action.item())

                states.append(state_tensor.squeeze(0))
                actions.append(action.squeeze(0))
                rewards.append(reward)

                state = next_state
                if done:
                    break


        # 计算回报
        total_reward = sum(rewards)
        episode_rewards.append(total_reward)

        returns = compute_returns(rewards, gamma)
        returns = (returns - returns.mean()) / (returns.std() + 1e-9)

        # 反复利用收集到的数据，更新learner策略
        i = 0
        for i in range(5):
            loss, kl = train_policy_network(learner, worker, optimizer, states, actions, returns)
            if abs(kl) > 0.01: # kl散度比较大，说明两个模型之间分布差异偏大，不适合继续训练，需要重新跟环境交互
                break

        worker.load_state_dict(learner.state_dict())

        # 保存Checkpoint（如果回报>1000）
        if total_reward > 1000:
            checkpoint_path = f"./ppo_checkpoint_{episode}_{total_reward}.pth"
            save_checkpoint(learner, optimizer, episode, total_reward, checkpoint_path)



        # 打印进度
        if (episode + 1) % print_interval == 0:
            avg_reward = np.mean(episode_rewards[-print_interval:])
            print(f"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}, Loss: {loss:.4f}")

        # 提前终止
        if len(episode_rewards) >= 100 and np.mean(episode_rewards[-100:]) >= 2000:
            print(f"Solved at Episode {episode + 1}!")
            break

    env.close()
    return episode_rewards


# ----------------------------
# 4. 推理函数（演示训练好的模型）
# ----------------------------
def test(env_name="CartPole-v1", hidden_dim=128, checkpoint_path="checkpoint.pth"):
    env = gym.make(env_name, render_mode="human")  # 开启可视化
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)
    optimizer = optim.Adam(policy.parameters(), lr=0.001)  # 仅占位，实际不用于测试

    # 加载Checkpoint
    load_checkpoint(policy, optimizer, checkpoint_path)

    print("Starting inference...")
    while True:  # 无限运行直到手动停止
        state = env.reset()[0]
        total_reward = 0

        while True:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).to(device)
                probs = policy(state_tensor)
                action = torch.argmax(probs).item()  # 直接选最优动作

            next_state, reward, done, _, _ = env.step(action)
            total_reward += reward
            state = next_state

            if done:
                print(f"Inference Reward: {total_reward}")
                break


# ----------------------------
# 5. 辅助函数， 用于策略梯度更新（替代Q(s,a) 的蒙特卡洛估计）
# ----------------------------
def compute_returns(rewards, gamma=0.99):
    returns = []
    R = 0
    for r in reversed(rewards):
        R = r + gamma * R
        returns.insert(0, R)
    return torch.tensor(returns, device=device)

def importance_sample_and_kl(learner, worker, states, actions, epsilon=0.2):
    # 计算当前策略（learner）下的动作概率
    with torch.no_grad():
        probs = learner(states)  # [T, action_dim]
    pa1 = probs[torch.arange(probs.size(0)), actions]  # 当前策略下，动作a的概率

    # 计算旧策略（worker）下的动作概率
    with torch.no_grad():
        old_probs = worker(states)  # [T, action_dim]
    pa2 = old_probs[torch.arange(probs.size(0)), actions]  # 旧策略下，动作a的概率

    # 计算重要性采样比率: r_t = pi_new(a|s) / pi_old(a|s)
    importance_ratio = pa1 / (pa2 + 1e-6)

    # 计算KL散度
    log_pa1 = torch.log(pa1 + 1e-6)  # 当前策略下的log概率
    log_pa2 = torch.log(pa2 + 1e-6)  # 旧策略下的log概率
    kl_divergence = torch.mean( pa2 * (log_pa2 - log_pa1)  )  # KL散度

    return importance_ratio, kl_divergence



def train_policy_network(learner, worker, optimizer, states, actions, returns):
    # 将列表中的状态/动作/回报堆叠成张量， 假设一把游戏玩下来的状态个数是T
    states = torch.stack(states)  # [T, 4]
    actions = torch.stack(actions) # [T]
    returns = returns  # [T]
    # 1. 通过策略网络计算动作概率
    probs = learner(states)  # [T, action_dim]
    # 2. 创建分类分布（用于采样和计算对数概率）
    m = Categorical(probs)
    # 3. 计算所选动作的对数概率
    log_probs = m.log_prob(actions)  # [T,]
    importance_ratio, kl = importance_sample_and_kl(learner, worker, states, actions)
    importance_ratio = torch.clamp(importance_ratio, 1 - 0.2, 1 + 0.2)
    # 4. 因为基于策略的强化学习要使用梯度上升使得state-value函数的期望最大化，所以损失函数是期望值的负数
    # returns已经在函数外面进行了带折扣的汇总运算，也就是已经是U了，不是每一步的r
    loss = -(log_probs * returns*importance_ratio).mean() + 0.01 * kl
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss.item(), kl


# ----------------------------
# 6. 主函数（命令行参数解析）
# ----------------------------
def main(mode):
    if mode == "train":
        rewards = train()
        plt.plot(rewards)
        plt.xlabel("Episode")
        plt.ylabel("Total Reward")
        plt.title("Policy Gradient Training")
        plt.show()
    elif mode == "test":
        test(checkpoint_path="./pbrl_checkpoint_1001.0.pth")


if __name__ == "__main__":
    main("train")
```

感觉收敛会比之前的On Policy的方法要慢一些：

```shell
Episode 218, Avg Reward: 384.00, Loss: -0.0044
Checkpoint saved to ./ppo_checkpoint_219_1834.0.pth (Reward: 1834.00)
Episode 220, Avg Reward: 1230.50, Loss: -0.0003
```



### 8. imitation learning

![image-20250327142459868](img/RL/image-20250327142459868.png)

### 9. RLHF

![image-20250329203734345](img/RL/image-20250329203734345.png)

### 10. 马尔可夫决策过程(MDP)与动态规划(DP)

#### 10.1 基本概念

![image-20250331210825112](img/RL/image-20250331210825112.png)

#### 10.2 如何用动态规划进行预测与控制

![image-20250401103901779](img/RL/image-20250401103901779.png)

思考：

![image-20250403200338963](img/RL/image-20250403200338963.png)

##### 10.2.1 实操1  用动态规划实现GridWorld的预测与控制

![image-20250401090243435](img/RL/image-20250401090243435.png)

代码如下，包括预测与控制：

```python
import random
import torch
from tqdm import tqdm

# 这里环境的代码之所以我自己直接写，说明
# 1. 环境对我们是已知的，也就是状态变迁概率函数和奖励函数是完全掌握额
# 2. 说明我们不需要真的跟环境交互，用自己的代码来描述环境的状态变迁和奖励
class GridWorldEnv:
    def __init__(self):
        self.GRID_NUM = 5
        self.ACTION_SPACE = {1:'UP', 2:'DOWN', 3:'LEFT',4:'RIGHT'}
        self.current_x = 0
        self.current_y = 0
        self.A = (1, 0)
        self.B = (3, 0)
        self.AA = (1, 4)
        self.BB = (3, 2)
        self.gamma = 0.9

    def reset(self):
        self.current_x = random.randint(0, self.GRID_NUM-1)
        self.current_y = random.randint(0, self.GRID_NUM - 1)
        return (self.current_x, self.current_y)

    def step(self, action):
        reward = 0
        if not action in self.ACTION_SPACE:
            print(f"invalid action {action}")
            return (self.current_x, self.current_y), reward, False # new_state, reward, done
        if (self.current_x, self.current_y) == self.A:
            self.current_x, self.current_y = self.AA
            reward = 10
            return (self.current_x, self.current_y), reward, False
        if (self.current_x, self.current_y) == self.B:
            self.current_x, self.current_y = self.BB
            reward = 5
            return (self.current_x, self.current_y), reward, False
        if action == 1 and self.current_y == 0: # up but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 2 and self.current_y == (self.GRID_NUM-1): # down but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 3 and self.current_x == 0: # left but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 4 and self.current_x == (self.GRID_NUM-1): # left but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 1: # up
            self.current_y -= 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 2: # down
            self.current_y += 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 3: # left
            self.current_x -= 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 4: # right
            self.current_x += 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
	#额外赠送：蒙特卡洛方式进行预测
    def monte_carlo_update(self):
        v = torch.zeros(self.GRID_NUM, self.GRID_NUM, dtype=torch.float32)
        return_sum = torch.zeros(self.GRID_NUM, self.GRID_NUM, dtype=torch.float32)
        return_cnt = torch.zeros(self.GRID_NUM, self.GRID_NUM, dtype=torch.float32)

        for episode in range(1000): # 1000个回合
            self.reset()
            # 抽样很多了就换个起点，这是一个优化，可以去掉
            if return_cnt[self.current_y, self.current_x] > 1000:
                continue

            # walk 1000 steps，得到一个回合的步骤
            trajectory = []
            for i in range(500): #每个回合最多500步
                from_x, from_y = self.current_x, self.current_y
                action = random.randint(1,4)
                _, r, _ = self.step(action)
                trajectory.append((from_x, from_y, r))

                # 如果已经有了100步，且当前状态的抽样已经很多了。这是一个优化，可以去掉
                if len(trajectory) > 100 and return_cnt[self.current_y, self.current_x] > 200: #抽样很多了就可以不继续往下走
                    G = return_sum[self.current_y, self.current_x] / return_cnt[self.current_y, self.current_x]
                    trajectory.append((self.current_x, self.current_y, G ))
                    break

            # calculate returns
            G = 0
            for (x, y, r) in reversed(trajectory):
                G = r + G * self.gamma
                return_sum[y, x] += G
                return_cnt[y, x] += 1

        for y in range(self.GRID_NUM):
            for x in range(self.GRID_NUM):
                if return_cnt[y, x] > 0:
                    v[y, x] = return_sum[y, x] / return_cnt[y, x]

        for y in range(self.GRID_NUM):
            for x in range(self.GRID_NUM):
                print(f"{v[y, x]:.2f} ", end="")
            print("")
        return v
 ########################### 进行预测 ##################################
	#动态规划对 等概率随机动作的策略 进行预测
    def calculate_V(self):
        v = torch.zeros(self.GRID_NUM, self.GRID_NUM, dtype=torch.float32)

        for episode in range(1000):#最多迭代这么多次
            deltaBig = False
            self.reset()
            old_v = v.clone()
            #每一个状态
            for y in range(self.GRID_NUM):
                for x in range(self.GRID_NUM):
                    #每一个动作
                    newvalue = 0
                    for action in [1,2,3,4]:
                        #强制设置当前位置在(y,x)处，并做出一个行动
                        self.current_y = y
                        self.current_x = x
                        _, r, _ = self.step(action)
                        newvalue += 0.25 * (self.gamma * old_v[self.current_y, self.current_x] + r)
                    if (newvalue-old_v[y, x])*(newvalue-old_v[y, x]) > 0.000001:
                        deltaBig = True
                    v[y, x] = newvalue
            if not deltaBig: #早停
                break

        for y in range(self.GRID_NUM):
            for x in range(self.GRID_NUM):
                print(f"{v[y, x]:.2f} ", end="")
            print("")
        return v
################## 进行控制 #################################
    # Q*
    def calculate_Q(self):
        self.Q = torch.zeros(self.GRID_NUM, self.GRID_NUM, len(self.ACTION_SPACE))
        for episode in range(1000): #最多1000轮
            delta = 0#统计本轮最大的迭代变化绝对值
            old_Q = self.Q.clone()
            #每一个状态
            for y in range(self.GRID_NUM):
                for x in range(self.GRID_NUM):
                    #每一个动作
                    for action in range( len(self.ACTION_SPACE)):
                        # 强制设置当前位置在(y,x)处，并做出一个行动
                        self.current_y = y
                        self.current_x = x
                        (next_x, next_y), r, _ = self.step(action+1)
                        max_next_Q = old_Q[next_y, next_x].max().item()
                        self.Q[y, x, action] = self.gamma * max_next_Q + r

                        if (old_Q[y, x, action] - (self.gamma * max_next_Q + r)).abs() > delta:
                            delta = (old_Q[y, x, action] - (self.gamma * max_next_Q + r)).abs().item()

            if (delta * delta) < 0.000001:
                break

        for y in range(self.GRID_NUM):
            for x in range(self.GRID_NUM):
                print("(", end="")
                for action in range(len(self.ACTION_SPACE)):
                    print(f"{self.Q[y, x, action]:.1f},", end="")
                print(") ", end="")
            print("")
        return self.Q
    
#根据Q* 找到每个状态下回报最大的动作
    def extract_best_policy(self):
        self.Pi = torch.zeros(self.GRID_NUM, self.GRID_NUM, dtype=torch.int32)
        arrows = {0: '↑', 1: '↓', 2: '←', 3: '→'}

        for y in range(self.GRID_NUM):
            for x in range(self.GRID_NUM):
                max_Q_action = self.Q[y, x].argmax().item()
                self.Pi[y, x] = max_Q_action+1
                print(f"{arrows[max_Q_action]} ", end="")
            print("")


env = GridWorldEnv()
print("dynamic plan:")
env.calculate_V()
print("monte carlo:")
env.monte_carlo_update()
print("\nQ* :")
env.calculate_Q()
print("\nbest policy:")
env.extract_best_policy()
```

【思考】：

![image-20250403154309774](img/RL/image-20250403154309774.png)

##### 10.2.2 实操2 策略迭代法进行控制

还是实操1的toy problem，策略迭代法进行最优策略求解：

![image-20250401161829675](img/RL/image-20250401161829675.png)



代码如下：

```python
import random
import torch
from tqdm import tqdm

class GridWorldEnv:
    def __init__(self):
        self.GRID_NUM = 5
        self.ACTION_SPACE = {1:'UP', 2:'DOWN', 3:'LEFT',4:'RIGHT'}
        self.current_x = 0
        self.current_y = 0
        self.A = (1, 0)
        self.B = (3, 0)
        self.AA = (1, 4)
        self.BB = (3, 2)
        self.gamma = 0.9

    def reset(self):
        self.current_x = random.randint(0, self.GRID_NUM-1)
        self.current_y = random.randint(0, self.GRID_NUM - 1)
        return (self.current_x, self.current_y)

    def step(self, action):
        reward = 0
        if not action in self.ACTION_SPACE:
            print(f"invalid action {action}")
            return (self.current_x, self.current_y), reward, False # new_state, reward, done
        if (self.current_x, self.current_y) == self.A:
            self.current_x, self.current_y = self.AA
            reward = 10
            return (self.current_x, self.current_y), reward, False
        if (self.current_x, self.current_y) == self.B:
            self.current_x, self.current_y = self.BB
            reward = 5
            return (self.current_x, self.current_y), reward, False
        if action == 1 and self.current_y == 0: # up but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 2 and self.current_y == (self.GRID_NUM-1): # down but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 3 and self.current_x == 0: # left but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 4 and self.current_x == (self.GRID_NUM-1): # left but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 1: # up
            self.current_y -= 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 2: # down
            self.current_y += 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 3: # left
            self.current_x -= 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 4: # right
            self.current_x += 1
            reward = 0
            return (self.current_x, self.current_y), reward, False


    def policy_iterate(self, theta=1e-4):
        """ 使用策略迭代求解最优策略 """
        V = torch.zeros((self.GRID_NUM, self.GRID_NUM), dtype=torch.float32)  # 状态值函数 V(s)
        policy = torch.zeros((self.GRID_NUM, self.GRID_NUM), dtype=torch.int32)  # 初始随机策略

        while True:
            # 1. 策略评估 (Policy Evaluation)
            while True:
                delta = 0
                V_old = V.clone()

                for x in range(self.GRID_NUM):
                    for y in range(self.GRID_NUM):
                        self.current_x = x
                        self.current_y = y
                        action = policy[y, x].item()
                        (next_x, next_y), reward, _ = self.step( action+1)
                        V[y, x] = reward + self.gamma * V[next_y, next_x]
                        delta = max(delta, abs(V[y, x] - V_old[y, x]))

                if delta < theta:  # 价值函数收敛
                    break

            # 2. 策略改进 (Policy Improvement)
            policy_stable = True

            for y in range(self.GRID_NUM):
                for x in range(self.GRID_NUM):
                    old_action = policy[y, x].item()

                    # 计算 Q(s, a) 并选择最优动作
                    Q_values = []
                    for action in range(4):
                        self.current_x = x
                        self.current_y = y
                        (next_x, next_y), reward, _ = self.step( action+1)
                        Q_values.append(reward + self.gamma * V[next_y, next_x])

                    best_action = torch.tensor(Q_values).argmax().item()
                    policy[y, x] = best_action  # 选择最优动作

                    if old_action != best_action:
                        policy_stable = False  # 策略仍在变化

            if policy_stable:  # 当策略不再变化时，停止迭代
                break

        return policy, V

env = GridWorldEnv()
policy, v = env.policy_iterate()
arrows = {0: '↑', 1: '↓', 2: '←', 3: '→'}
for y in range(env.GRID_NUM):
    for x in range(env.GRID_NUM):
        action = policy[y, x].item()
        print(f"{arrows[action]} ", end="")
    print("")
```

##### 10.2.3 实操3 价值迭代法进行控制

还是实操1的toy problem，价值迭代法进行最优策略求解：

![image-20250401172732086](img/RL/image-20250401172732086.png)

代码如下：

```python
import random
import torch
from tqdm import tqdm

class GridWorldEnv:
    def __init__(self):
        self.GRID_NUM = 5
        self.ACTION_SPACE = {1:'UP', 2:'DOWN', 3:'LEFT',4:'RIGHT'}
        self.current_x = 0
        self.current_y = 0
        self.A = (1, 0)
        self.B = (3, 0)
        self.AA = (1, 4)
        self.BB = (3, 2)
        self.gamma = 0.9

    def reset(self):
        self.current_x = random.randint(0, self.GRID_NUM-1)
        self.current_y = random.randint(0, self.GRID_NUM - 1)
        return (self.current_x, self.current_y)

    def step(self, action):
        reward = 0
        if not action in self.ACTION_SPACE:
            print(f"invalid action {action}")
            return (self.current_x, self.current_y), reward, False # new_state, reward, done
        if (self.current_x, self.current_y) == self.A:
            self.current_x, self.current_y = self.AA
            reward = 10
            return (self.current_x, self.current_y), reward, False
        if (self.current_x, self.current_y) == self.B:
            self.current_x, self.current_y = self.BB
            reward = 5
            return (self.current_x, self.current_y), reward, False
        if action == 1 and self.current_y == 0: # up but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 2 and self.current_y == (self.GRID_NUM-1): # down but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 3 and self.current_x == 0: # left but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 4 and self.current_x == (self.GRID_NUM-1): # left but boundary
            reward = -1
            return (self.current_x, self.current_y), reward, False
        if action == 1: # up
            self.current_y -= 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 2: # down
            self.current_y += 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 3: # left
            self.current_x -= 1
            reward = 0
            return (self.current_x, self.current_y), reward, False
        if action == 4: # right
            self.current_x += 1
            reward = 0
            return (self.current_x, self.current_y), reward, False


    def value_iterate(self):
        V = torch.zeros((self.GRID_NUM, self.GRID_NUM), dtype=torch.float32)
        for k in range(100):
            Q = torch.zeros((self.GRID_NUM, self.GRID_NUM, 4), dtype=torch.float32)
            old_V = V.clone()
            for x in range(self.GRID_NUM):
                for y in range(self.GRID_NUM):
                    for action  in range(4):
                        self.current_x = x
                        self.current_y = y
                        (next_x, next_y), reward, _ = self.step(action + 1)
                        Q[y, x, action] = reward + self.gamma * V[next_y, next_x].item()
                    V[y,x] = Q[y,x].max().item()
            if torch.dist(V, old_V, p=2).item() < 0.000001:#这个对于性能提升很重要
                break
        policy = torch.zeros((self.GRID_NUM, self.GRID_NUM), dtype=torch.int32)
        for x in range(self.GRID_NUM):
            for y in range(self.GRID_NUM):
                vlist = []
                for action in range(4):
                    self.current_x = x
                    self.current_y = y
                    (next_x, next_y), reward, _ = self.step(action + 1)
                    vlist.append( reward + self.gamma * V[next_y, next_x].item() )
                a = torch.tensor(vlist).argmax().item()
                policy[y,x] = a
        return policy, V



env = GridWorldEnv()
print("value iterate result:")
policy, v = env.value_iterate()
arrows = {0: '↑', 1: '↓', 2: '←', 3: '→'}
for y in range(env.GRID_NUM):
    for x in range(env.GRID_NUM):
        action = policy[y, x].item()
        print(f"{arrows[action]} ", end="")
    print("")

```

#### 10.3 刻意练习

动态规划和马尔可夫决策过程，我一直没有入脑入心。所以安排刻意练习的环节

##### 10.3.1 练习1 配送pizza

学习到的一个经验就是：

1. Value的初始化会影响到是否收敛。不能照搬书上的算法把它初始化为0.
2. reward的设计会影响到是否收敛，例如下面的问题里，对于不能移动的情况的耗时，必须要比较大才会收敛。而前面cliffwalking的例子，跌下悬崖太大又会导致不能收敛...
3. reward不是说都要归一化，不是要都在同一个量级。有时候在同一量级，会导致不收敛。下面的练习题里，不能移动的情况下cost要比较大才能收敛



【练习的问题是：】

在一个n*m的矩阵里,只有一个格子是字符X,表示pizza店所在地。 $表示写字楼位置，需要把pizza送到写字楼。 

在矩阵里: 

1. 字符‘$'表示该格子是一座写字楼 ** (注意,在本题中,pizza店本身也是写字楼) ** ,里面的客户订 了一个pizza。
2. 字符'0’~'9'表示该格子是空地,同时表示该空地的高度。每个空地格子的高度范围为[0,9]。 送餐员可以从当前格子往 上、下、左、右四个方向移动一格。当然根据高度的不同,移动一次的时间也不同。 

你能从A格子进入B格子的条件是: 

1. A、B两个格子相邻 ** (有一条公共边) **; 
2. 关于移动方法： 
   1. 如果A、B均为空地: 
      1. 如果A、B高度相等,那么花费的时间是1; 
      2. 如果A、B高度差为1,那么花费的时间是3;
      3. 如果高度差大于1,则不能从A走到B。
   2. 如果A与B当中,至少有一个格子是写字楼(包括两个格子都是写字楼的情况， X也是写字楼) 花费的时间是2。(如果A或B当中有空地的话,不需要考虑A或B的高度) 注意:送餐员可以进入写字楼,即使他不是给该写字楼的客户送pizza(也就是可以借道)。

 矩阵如下，大小为3行7列： 

```shell
3442211 
34$221X 
3442211 
```

请实现了动态规划的策略迭代方法，为送餐员找到最合适的行路策略，使得送餐时间最短。代码如下：

```c++
#include <stdlib.h>
#include <stdio.h>
#include <string.h>
#include <vector>
#include <utility>
#include <string>
#include <stdint.h>
#include <cmath>
#include <climits>
using namespace std;

#define MAX_SZ (50)

int row, col; // 实际的地图大小
int start_x, start_y; // pizza店位置
int dest_x, dest_y; // 送餐的位置
char arr[MAX_SZ][MAX_SZ]; // 地图
char policy[MAX_SZ][MAX_SZ]; // 策略，action: 0-up, 1-down, 2-left, 3-right
float gamma_w = 1.0; // 折扣率设为1.0，因为我们想要准确的总成本
const int CANNOT_MOVE_COST = INT_MAX/2; // 不能移动时的成本（设为一个大数） 这个值也会影响到是否收敛！！

// action: 0-up, 1-down, 2-left, 3-right
int env_step(int from_x, int from_y, int action, int & next_x, int & next_y, int &cost)
{
    // 边界检查
    if (from_x < 0 || from_y < 0 || from_x >= col || from_y >= row || action < 0 || action > 3)
    {
        printf("invalid argument at %d ! %d, %d, %d\n", __LINE__, from_x, from_y, action);
        return -1;
    }

    // 检查是否不能移动
    if ((from_x == 0 && action == 2) ||  // 左边界且向左移动
        (from_y == 0 && action == 0) ||  // 上边界且向上移动
        (from_x == col-1 && action == 3) ||  // 右边界且向右移动
        (from_y == row-1 && action == 1))  // 下边界且向下移动
    {
        next_x = from_x;
        next_y = from_y;
        cost = CANNOT_MOVE_COST;
        return 1;
    }

    // 计算目标位置
    if (action == 0) next_y = from_y-1, next_x = from_x; // up
    else if (action == 1) next_y = from_y+1, next_x = from_x; // down
    else if (action == 2) next_x = from_x-1, next_y = from_y; // left
    else if (action == 3) next_x = from_x+1, next_y = from_y; // right

    char B = arr[next_y][next_x];
    char A = arr[from_y][from_x];

    // 处理写字楼情况（包括X和$）
    if (A == '$' || B == '$' || A == 'X' || B == 'X')
    {
        cost = 2; // 穿过写字楼耗时2
        return 0;
    }
    
    // 处理空地情况
    if ('0' <= A && A <= '9' && '0' <= B && B <= '9')
    {
        int aa = A - '0';
        int bb = B - '0';
        int diff = abs(aa - bb);

        if (diff == 0) {
            cost = 1; // 高度相同耗时1
            return 0;
        } else if (diff == 1) {
            cost = 3; // 高度差1耗时3
            return 0;
        } else {
            next_x = from_x;
            next_y = from_y;
            cost = CANNOT_MOVE_COST;
            return 1; // 不能移动
        }
    }

    printf("invalid state at [%d,%d] and [%d,%d]\n", from_y, from_x, next_y, next_x);
    return -1;
}

void print_arr()
{
    for (int i = 0; i < row; ++i)
    {
        for (int j = 0; j < col; ++j)
        {
            printf("%c", arr[i][j]);
        }
        printf("\n");
    }
    printf("\n");
}

void print_value(float v[MAX_SZ][MAX_SZ])
{
    for (int i = 0; i < row; ++i)
    {
        for (int j = 0; j < col; ++j)
        {
            if (v[i][j] > CANNOT_MOVE_COST/2) printf("  INF ");
            else printf("%5.1f ", v[i][j]);
        }
        printf("\n");
    }
    printf("\n");
}

void print_policy()
{
    for (int i = 0; i < row; ++i)
    {
        for (int j = 0; j < col; ++j)
        {
            if (i == dest_y && j == dest_x) {
                printf("  G "); // 目标位置
                continue;
            }
            
            switch(policy[i][j]) {
                case 0: printf("  ↑ "); break;
                case 1: printf("  ↓ "); break;
                case 2: printf("  ← "); break;
                case 3: printf("  → "); break;
                default: printf("  ? "); break;
            }
        }
        printf("\n");
    }
    printf("\n");
}

int policy_iteration()
{
    float value[MAX_SZ][MAX_SZ];
    // 初始化值函数为一个大数（类似无穷大），因为我们最后是选择成本最低的动作，而不是选择回报最大的动作。
    for (int i = 0; i < row; ++i) {
        for (int j = 0; j < col; ++j) {
            value[i][j] = CANNOT_MOVE_COST; //初始化为0也可以收敛，但这里初始化比较大会可靠一些
        }
    }
    // 终止状态值为0
    value[dest_y][dest_x] = 0;
    
    memset(policy, 0, sizeof(policy));
    
    int iteration = 0;
    while (1)
    {
        printf("Iteration %d:\n", ++iteration);
        
        // 策略评估
        float delta;
        do {
            delta = 0.0;
            for (int i = 0; i < row; ++i)
            {
                for (int j = 0; j < col; ++j)
                {
                    if (i == dest_y && j == dest_x) continue; // 跳过终止状态
                    
                    int from_x = j, from_y = i;
                    int next_x, next_y, cost;
                    float old_v = value[from_y][from_x];
                    
                    // 执行当前策略的动作
                    int action = policy[from_y][from_x];
                    int ret = env_step(from_x, from_y, action, next_x, next_y, cost);
                    
                    if (ret == 0) {
                        // 最小化总成本：当前成本 + 下一步的期望成本
                        value[from_y][from_x] = cost + gamma_w * value[next_y][next_x];
                        delta = max(delta, abs(value[from_y][from_x] - old_v));
                    } else {
                        value[from_y][from_x] = CANNOT_MOVE_COST;
                    }
                }
            }
        } while (delta > 0.01); // 收敛阈值

        printf("Value function after evaluation:\n");
        print_value(value);

        // 策略改进
        int stable = 1;
        for (int i = 0; i < row; ++i)
        {
            for (int j = 0; j < col; ++j)
            {
                if (i == dest_y && j == dest_x) continue; // 跳过终止状态
                
                int old_action = policy[i][j];
                int from_x = j, from_y = i;
                float min_q = CANNOT_MOVE_COST;
                int best_action = old_action;
                
                // 测试所有可能的动作，寻找最小成本的动作
                for (int a = 0; a < 4; ++a)
                {
                    int next_x, next_y, cost;
                    if (env_step(from_x, from_y, a, next_x, next_y, cost) == 0)
                    {
                        float q = cost + gamma_w * value[next_y][next_x];
                        if (q < min_q) { //不一样的地方，这里要取最小的值作为最佳动作
                            min_q = q;
                            best_action = a;
                        }
                    }
                }
                
                policy[i][j] = best_action;
                if (best_action != old_action) {
                    stable = 0;
                }
            }
        }

        printf("Policy after improvement:\n");
        print_policy();

        if (stable) break;
    }

    return 0;
}

int main()
{
    // 测试数据
    const char * map_data[] = {
        "3442211",
        "34$221X",
        "3442211"
    };
    
    row = 3;
    col = 7;
    
    // 初始化地图
    for (int i = 0; i < row; ++i)
    {
        for (int j = 0; j < col; ++j)
        {
            arr[i][j] = map_data[i][j];
            if (arr[i][j] == 'X') {
                start_x = j;
                start_y = i;
            } else if (arr[i][j] == '$') {
                dest_x = j;
                dest_y = i;
            }
        }
    }
    
    printf("Initial map:\n");
    print_arr();
    
    printf("Pizza shop at (%d, %d)\n", start_x, start_y);
    printf("Destination at (%d, %d)\n", dest_x, dest_y);
    
    policy_iteration();
    
    printf("Final optimal policy:\n");
    print_policy();
    
    // 打印从起点到终点的路径
    printf("Path from pizza shop to destination:\n");
    int x = start_x, y = start_y;
    while (!(x == dest_x && y == dest_y)) {
        printf("(%d, %d) -> ", x, y);
        int action = policy[y][x];
        if (action == 0) y--;
        else if (action == 1) y++;
        else if (action == 2) x--;
        else if (action == 3) x++;
    }
    printf("(%d, %d) [Destination]\n", dest_x, dest_y);
    
    return 0;
}
```

可能AI写的这份代码可读性更好：

```c++
#include <iostream>
#include <vector>
#include <climits>
#include <cmath>
#include <algorithm>
#include <iomanip>
#include <unordered_map>

using namespace std;

// 定义方向：上、下、左、右
const vector<pair<int, int>> DIRECTIONS = {{-1, 0}, {1, 0}, {0, -1}, {0, 1}};
const vector<string> DIRECTION_SYMBOLS = {"↑", "↓", "←", "→"};

struct Cell {
    char type;  // 'X': pizza店, '$': 写字楼, '0'-'9': 空地
    int height; // 如果是空地，存储高度
};

class PizzaDeliverySolver {
private:
    vector<vector<Cell>> grid;
    int rows;
    int cols;
    pair<int, int> pizza_shop;
    vector<pair<int, int>> office_buildings;
    vector<vector<double>> value_function;
    vector<vector<int>> policy;

public:
    PizzaDeliverySolver(const vector<vector<Cell>>& grid) : grid(grid) {
        rows = grid.size();
        cols = rows > 0 ? grid[0].size() : 0;
        findSpecialLocations();
        initializeValueAndPolicy();
    }

    void solve() {
        bool policy_stable = false;
        int iterations = 0;

        while (!policy_stable) {
            iterations++;
            policy_stable = true;

            // 策略评估
            evaluatePolicy();

            // 策略改进
            policy_stable = improvePolicy();
        }

        cout << "Policy Iteration completed in " << iterations << " iterations.\n";
    }

    void printResults() const {
        printPolicyTable();
        printOptimalPath();
    }

private:
    void findSpecialLocations() {
        for (int i = 0; i < rows; ++i) {
            for (int j = 0; j < cols; ++j) {
                if (grid[i][j].type == 'X') {
                    pizza_shop = {i, j};
                } else if (grid[i][j].type == '$') {
                    office_buildings.emplace_back(i, j);
                }
            }
        }
    }

    void initializeValueAndPolicy() {
        value_function.assign(rows, vector<double>(cols, INT_MAX)); //必须初始化比较大一点的值，0的话不收敛。
        policy.assign(rows, vector<int>(cols, -1));

        // 设置目标状态的值函数为0
        for (const auto& office : office_buildings) {
            value_function[office.first][office.second] = 0;
        }
    }

    void evaluatePolicy() {
        bool value_changed;
        do {
            value_changed = false;
            for (int i = 0; i < rows; ++i) {
                for (int j = 0; j < cols; ++j) {
                    if (isTargetLocation(i, j)) continue;

                    double old_value = value_function[i][j];
                    double new_value = INT_MAX;

                    for (int d = 0; d < DIRECTIONS.size(); ++d) {
                        auto di = DIRECTIONS[d].first;
                        auto dj = DIRECTIONS[d].second;
                        int ni = i + di;
                        int nj = j + dj;

                        if (!isValidMove(ni, nj)) continue;

                        double cost = calculateMoveCost(i, j, ni, nj);
                        double candidate_value = cost + value_function[ni][nj];
                        new_value = min(new_value, candidate_value);
                    }

                    if (new_value < old_value) {
                        value_function[i][j] = new_value;
                        value_changed = true;
                    }
                }
            }
        } while (value_changed);
    }

    bool improvePolicy() {
        bool policy_stable = true;

        for (int i = 0; i < rows; ++i) {
            for (int j = 0; j < cols; ++j) {
                if (isTargetLocation(i, j)) continue;

                int old_action = policy[i][j];
                int best_action = -1;
                double min_value = INT_MAX;

                for (int d = 0; d < DIRECTIONS.size(); ++d) {
                    auto di = DIRECTIONS[d].first;
                    auto dj = DIRECTIONS[d].second;
                    int ni = i + di;
                    int nj = j + dj;

                    if (!isValidMove(ni, nj)) continue;

                    double cost = calculateMoveCost(i, j, ni, nj);
                    double candidate_value = cost + value_function[ni][nj];

                    if (candidate_value < min_value) {
                        min_value = candidate_value;
                        best_action = d;
                    }
                }

                if (best_action != -1 && best_action != old_action) {
                    policy[i][j] = best_action;
                    policy_stable = false;
                }
            }
        }

        return policy_stable;
    }

    bool isTargetLocation(int i, int j) const {
        for (const auto& office : office_buildings) {
            if (i == office.first && j == office.second) {
                return true;
            }
        }
        return false;
    }

    bool isValidMove(int i, int j) const {
        return i >= 0 && i < rows && j >= 0 && j < cols;
    }

    double calculateMoveCost(int i, int j, int ni, int nj) const {
        // 如果任一方是写字楼或pizza店
        if (grid[i][j].type == '$' || grid[i][j].type == 'X' || 
            grid[ni][nj].type == '$' || grid[ni][nj].type == 'X') {
            return 2;
        }
        
        // 否则计算高度差
        int height_diff = abs(grid[i][j].height - grid[ni][nj].height);
        if (height_diff == 0) return 1;
        if (height_diff == 1) return 3;
        return INT_MAX; // 不能移动
    }

    void printPolicyTable() const {
        cout << "\nOptimal Policy Table:\n";
        cout << "Each cell shows the best action to take (↑, ↓, ←, →)\n";
        cout << "X: Pizza shop, $: Office building\n\n";

        for (int i = 0; i < rows; ++i) {
            for (int j = 0; j < cols; ++j) {
                if (grid[i][j].type == 'X') {
                    cout << " X ";
                } else if (grid[i][j].type == '$') {
                    cout << " $ ";
                } else {
                    if (policy[i][j] == -1) {
                        cout << " - ";
                    } else {
                        cout << " " << DIRECTION_SYMBOLS[policy[i][j]] << " ";
                    }
                }
            }
            cout << endl;
        }
    }

    void printOptimalPath() const {
        vector<pair<int, int>> path;
        vector<string> directions;
        
        int x = pizza_shop.first;
        int y = pizza_shop.second;
        path.emplace_back(x, y);

        while (!isTargetLocation(x, y)) {
            int action = policy[x][y];
            if (action == -1) {
                cout << "\nNo valid path found from pizza shop to office building!\n";
                return;
            }

    
            auto dx = DIRECTIONS[action].first;
            auto dy = DIRECTIONS[action].second;
            x += dx;
            y += dy;
            path.emplace_back(x, y);
            directions.push_back(DIRECTION_SYMBOLS[action]);
        }

        cout << "\nOptimal Path from Pizza Shop to Office Building:\n";
        for (size_t i = 0; i < path.size(); ++i) {
            cout << "(" << path[i].first << "," << path[i].second << ")";
            if (i < directions.size()) {
                cout << " -> " << directions[i] << " -> ";
            }
        }
        cout << "\nTotal delivery time: " << value_function[pizza_shop.first][pizza_shop.second] << endl;
    }
};

int main() {
    // 定义网格
    vector<string> grid_str = {
        "3442211",
        "34$221X",
        "3442211"
    };
    
    // 转换为Cell结构
    vector<vector<Cell>> grid(grid_str.size(), vector<Cell>(grid_str[0].size()));
    for (int i = 0; i < grid_str.size(); ++i) {
        for (int j = 0; j < grid_str[i].size(); ++j) {
            grid[i][j].type = grid_str[i][j];
            if (grid_str[i][j] >= '0' && grid_str[i][j] <= '9') {
                grid[i][j].height = grid_str[i][j] - '0';
            } else {
                grid[i][j].height = 0;
            }
        }
    }
    
    // 创建并运行求解器
    PizzaDeliverySolver solver(grid);
    solver.solve();
    solver.printResults();
    
    return 0;
}
```

##### 10.3.2 练习2 带终止状态的GridWorld

![image-20250404114946730](img/RL/image-20250404114946730.png)

代码如下，可以收敛：

```python
import torch


class GridWorld:
    def __init__(self):
        self.size = 4
        self.cannot_move_reward = -1
        self.terminator1 = (0, 0)
        self.terminator2 = (self.size-1, self.size-1)
        return

    def step(self, state, action):
        x,y = state
        if x < 0 or x >= self.size or y < 0 or y >= self.size or action <0 or action > 3:
            raise ValueError(f" invalid arguments")
        next_x = x
        next_y = y
        reward = -1
        if action == 0: # up
            next_y = y-1
        if action == 1: # down
            next_y = y+1
        if action == 2: # left
            next_x = x-1
        if action == 3: #right
            next_x = x+1
        if next_x < 0 or next_x >= self.size:
            next_x = x
            reward = self.cannot_move_reward
        if next_y < 0 or next_y >= self.size:
            next_y = y
            reward = self.cannot_move_reward

        done = False
        if (next_y, next_x) == self.terminator1 or (next_y, next_x) == self.terminator2:
            done = True
        return (next_x, next_y), reward, done
    def policy_iteration_initialize(self):
        self.policy = torch.zeros((self.size, self.size), dtype=torch.int32)
        self.value = torch.ones( (self.size, self.size), dtype=torch.float32) * 0
        self.value[self.terminator1[0], self.terminator1[1]] = 0
        self.value[self.terminator2[0], self.terminator2[1]] = 0
    def policy_iteration_evaluating(self):
        while True:
            delta = 0
            for h in range(self.size):
                for w in range(self.size):
                    old_v = self.value[h, w].item()
                    if (h,w) == self.terminator1 or (h, w) == self.terminator2:
                        continue
                    action = self.policy[h, w]
                    (next_x, next_y), r, done = self.step( (w,h), action)
                    if next_x != w or next_y != h : # moved
                        self.value[h, w] = r + self.value[next_y, next_x].item()
                        delta = max(delta, abs(self.value[h, w].item() - old_v) )
            if delta < 0.0001:
                break
        return self.value
    def policy_iteration_improving(self):
        stable = True
        for h in range(self.size):
            for w in range(self.size):
                if (h, w) == self.terminator1 or (h, w) == self.terminator2:
                    continue
                old_policy = self.policy[h,w].item()
                q_values = []
                for action in range(4):
                    (next_x, next_y), r, done = self.step((w, h), action)
                    if next_x != w or next_y != h:  # moved
                        q_values.append(r + self.value[next_y, next_x].item())
                    else: #如果不能移动，这个动作也要占位，否则后面求argmax的时候就出bug了
                        q_values.append(float('-inf'))
                self.policy[h, w] = torch.tensor(q_values).argmax().item()
                if self.policy[h, w].item() != old_policy:
                    stable = False
        return stable
    #价值迭代法
    def value_iteration(self):
        self.value = torch.ones((self.size, self.size), dtype=torch.float32) * 0
        self.value[self.terminator1[0], self.terminator1[1]] = 0
        self.value[self.terminator2[0], self.terminator2[1]] = 0
        while True:
            delta = 0
            for h in range(self.size):
                for w in range(self.size):
                    old_v = self.value[h, w].item()
                    if (h,w) == self.terminator1 or (h, w) == self.terminator2:
                        continue
                    q_values = []
                    for action in range(4):
                        (next_x, next_y), r, done = self.step( (w,h), action)
                        if next_x != w or next_y != h : # moved
                            q_values.append(r + self.value[next_y, next_x].item())
                    self.value[h, w] = torch.tensor(q_values).max().item()
                    delta = max(delta, abs(self.value[h, w].item() - old_v) )
            if delta < 0.0001:
                break
        # extract policy from v*
        self.policy = torch.zeros((self.size, self.size), dtype=torch.int32)
        for h in range(self.size):
            for w in range(self.size):
                old_v = self.value[h, w].item()
                if (h, w) == self.terminator1 or (h, w) == self.terminator2:
                    continue
                q_values = []
                for action in range(4):
                    (next_x, next_y), r, done = self.step((w, h), action)
                    if next_x != w or next_y != h:  # moved
                        q_values.append(r + self.value[next_y, next_x].item())
                    else:
                        q_values.append(float('-inf')) #占位，确保argmax正确
                self.policy[h,w] = torch.tensor(q_values).argmax().item()
        return
    #策略迭代法
    def policy_iteration(self):
        self.policy_iteration_initialize()
        while True:
            self.policy_iteration_evaluating()
            #print("iteration finished:")
            #env.print_value()
            stable = self.policy_iteration_improving()
            if stable:
                break

    def print_policy(self):
        arrows = {0: '↑', 1: '↓', 2: '←', 3: '→'}

        for y in range(self.size):
            for x in range(self.size):
                if (x,y) == self.terminator1 or (x,y) == self.terminator2:
                    print("X ", end="")
                else:
                    print(f"{arrows[self.policy[y,x].item()]} ", end="")
            print("")
    def print_value(self):
        for y in range(self.size):
            for x in range(self.size):
                print(f"{self.value[y,x].item():.2f} ", end="")
            print("")


env = GridWorld()
env.policy_iteration()
print("\noptimal prolicy and value:")
env.print_policy()
env.print_value()
env.value_iteration()
print("\noptimal prolicy and value:")
env.print_policy()
env.print_value()

```

##### 10.3.3 练习3 汽车门店间调度

杰克为一家全国性汽车租赁公司管理两个门店。每天都会有若干顾客到这两个门店租车。如果杰克门店有车可用，他将车辆租出并能从总公司获得10美元租金。若该门店车辆已租罄，则这笔生意就会流失。租出的车辆需在归还后的第二天才能重新投入使用。

为了确保车辆能合理调配到需求点，杰克可以在夜间将车辆在两个门店间转移，每转移一辆车需花费2美元成本。我们假设每个门店的车辆租用需求和归还数量均服从泊松分布，即数量为n的概率为( (n^λ) * e^(-λ) ) / (n!)，其中λ为期望值。

具体参数λ设置为：第一家门店的租车需求λ=3，还车λ=3；第二家门店租车需求λ=4，还车λ=2。

为简化问题，我们设定每个门店的车辆存储上限为20辆（超出部分将返还总公司，不再计入系统），且一夜间最多只能转移5辆车。取折扣率γ=0.9，将此问题建模为一个持续型有限马尔可夫决策过程（MDP）：时间步长为天，状态是每日营业结束时各门店的车辆库存数，动作为夜间在两个门店间调配的车辆净数量。

![泊松分布](img/RL/poison_dist.png)

从这个例子就可以看出动态规划方法，只适合处理很小的状态空间和动作空间的问题。这个练习的状态空间 441个，动作空间11个，V*的一次迭代计算就要16分钟，如果20次迭代可以收敛出V\*,  那就要320分钟也就是5个多小时。

我让AI帮我修改代码，变成GPU并行版本，不到10分钟可以跑出结果。

CPU版本代码如下：

```python
import os.path

import torch
from torch.distributions import Poisson
import math
from functools import lru_cache
from tqdm import  tqdm
import datetime as dt

# 预计算泊松分布表 (λ=2,3,4; x=0~20)， 用查表法加速
POISSON_TABLES = {
    2: [math.exp(-2) * (2 ** k) / math.factorial(k) for k in range(15)],
    3: [math.exp(-3) * (3 ** k) / math.factorial(k) for k in range(15)],
    4: [math.exp(-4) * (4 ** k) / math.factorial(k) for k in range(15)]
}
poisson_occur_cache = {} # 避免poisson_occur()函数重复计算 的cache
#直接查表得到泊松分布的 x值和概率  对的列表
def get_poisson_pairs(lambda_):
    k_values = [k for k in range(15)]
    probs = POISSON_TABLES[lambda_]
    return list(zip(k_values, probs))

class RentingCar:
    def __init__(self):
        self.min_action = -5
        self.max_action = 5
        self.gamma = 0.9
        self.max_car_num = 20
        self.value = torch.zeros((self.max_car_num+1, self.max_car_num+1),dtype=torch.float32)
        self.policy = torch.zeros((self.max_car_num+1, self.max_car_num+1),dtype=torch.int32)
        return

    #门店经过晚上移车后的早晨有car_num辆车，按照两个lambda参数发生还车(in)和借车(out)
    def poisson_occur(self, car_num, in_lambda, out_lambda):
        '''
        门店，现存有 x 辆车，当天还车的数量in 服从lambda=3的泊松分布，
        当天借出去的车的数量out 服从lambda=4的泊松分布，两个分布独立。
        y不能小于0.
        当天闭店时的车辆数 y 等于 x + in - out，求y的各种取值的概率：
        1）枚举所有 in的可能值和发生概率（in, in_p），得到列表1
        2）枚举所有out的可能值和发生概率 (out, out_p)，得到列表2
        3）两两组合列表1和列表2的所有可能， 计算y = x + in - out，和出现概率p =  in_p * out_p， 如果y < 0, 令y=0。
        4) 把 (y, p)放到一个列表里
        '''
        # 为了避免重复计算，计算的结果放到缓存里
        global  poisson_occur_cache
        key = (car_num, in_lambda, out_lambda)
        if poisson_occur_cache.__contains__( key ):
            #print(f"hit cache {key}")
            return poisson_occur_cache[key]
        #else:
            #print(f"miss cache {key}")

        result = [] #保存 三元组 (y值，挣的钱，发生概率)
        in_list = get_poisson_pairs(in_lambda)
        out_list = get_poisson_pairs(out_lambda)
        for in_c, in_p in in_list:
            for out_c, out_p in out_list:
                y =  car_num + in_c - out_c
                if y < 0: #车辆数 入不敷出
                    y = 0
                    out_c = car_num + in_c # 那最多就只能出租这么多车了
                if y > 20:
                    y = 20
                earn = out_c * 10 #挣的佣金
                p = in_p * out_p
                result.append( (y, earn, p) )
        psum = 0
        for (y, e, p) in result:
            psum += p
        assert(psum == 1.0, f"psum is not 1.0:{psum:.2f} in poisson_occur()")
        poisson_occur_cache[key] = result
        return result
    def poisson_result(self, next_num1, next_num2):
        shop1 = self.poisson_occur(next_num1, 3, 3)
        shop2 = self.poisson_occur(next_num2, 2, 4)
        psum = 0.0
        total_reward = 0
        for (y1, e1, p1) in shop1:
            for (y2, e2, p2) in shop2:
                p = p1 * p2
                psum += p
                reward = e1 + e2
                total_reward += p * (reward + self.gamma * self.value[y1, y2].item())
        assert(psum == 1.0, f"psum is not 1.0:{psum:.2f}")
        return total_reward


    def value_iteration(self, checkpoint="./rentcar_0.pth"):
        iterate_num = 0
        if os.path.exists(checkpoint):
            data = torch.load(checkpoint)
            self.value = data['value']
            iterate_num = data['iterate_num']

        while True:
            # 状态：门店1的日结车辆数 num1, 门店2的日结车辆数 num2
            # action: 当晚从门店1运走action量车到门店2， -5 <= action <= 5
            delta = 0
            iterate_num += 1
            for num1 in tqdm(range(self.max_car_num +1), "shop1"):
                for num2 in range(self.max_car_num +1):
                    max_reward_action = 0
                    max_reward = float('-inf')
                    old_value = self.value[num1, num2].item()
                    for action in range(self.min_action, self.max_action+1):
                        # 当晚搬运汽车
                        true_action = action
                        if num1 - action < 0:
                            true_action = num1
                        if num2 + action < 0:
                            true_action = num2 * (-1)
                        move_cost = abs(true_action) * 2 #挪车开销
                        next_num1 = min(num1 - true_action, self.max_car_num)
                        next_num2 = min(num2 + true_action, self.max_car_num)
                        # 第二天发生还车借车随机事件
                        #print(f"{dt.datetime.now()} begin")
                        total_reward = self.poisson_result(next_num1, next_num2)
                        #print(f"{dt.datetime.now()} end")
                        total_reward -= move_cost
                        if total_reward > max_reward:
                            max_reward = total_reward
                            max_reward_action = action
                    self.value[num1, num2] = max_reward
                    delta = max(delta, abs(old_value - self.value[num1, num2].item()))

            print(f"{iterate_num} finish one iteration, delta={delta:.4f}")

            # save checkpoint
            data = {}
            data['value'] = self.value
            data['iterate_num'] = iterate_num
            torch.save(data, f"./rentcar_{iterate_num}.pth")

            if delta < 0.0001:
                break
        # extract policy from v*
        for num1 in range(self.max_car_num+1):
            for num2 in range(self.max_car_num+1):
                max_reward_action = 0
                max_reward = float('-inf')
                for action in range(self.min_action, self.max_action+1):
                    # 当晚搬运汽车
                    true_action = 0
                    if num1 - action < 0:
                        true_action = num1
                    if num2 + action < 0:
                        true_action = num2 * (-1)
                    move_cost = abs(true_action) * 2  # 挪车开销
                    next_num1 = min(num1 - true_action, self.max_car_num)
                    next_num2 = min(num2 + true_action, self.max_car_num)
                    # 第二天发生还车借车随机事件
                    total_reward = self.poisson_result(next_num1, next_num2)
                    if total_reward > max_reward:
                        max_reward = total_reward
                        max_reward_action = action
                self.policy[num1, num2] = max_reward_action

    def print_policy(self):
        for num1 in range(self.max_car_num+1):
            for num2 in range(self.max_car_num+1):
                print(f"{self.policy[num1, num2].item()} ")
            print("")
    def print_value(self):
        for num1 in range(self.max_car_num+1):
            for num2 in range(self.max_car_num+1):
                print(f"{self.value[num1, num2].item():.2f} ")
            print("")


rc = RentingCar()
rc.value_iteration()
rc.print_policy()
rc.print_value()
```

GPU并行计算的版本：

```python
import os.path
import torch
import math
from tqdm import tqdm
import datetime as dt

# 检查是否有可用的CUDA设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 预计算泊松分布表 (λ=2,3,4; x=0~20)，用查表法加速
POISSON_TABLES = {
    2: torch.tensor([math.exp(-2) * (2 ** k) / math.factorial(k) for k in range(15)], device=device),
    3: torch.tensor([math.exp(-3) * (3 ** k) / math.factorial(k) for k in range(15)], device=device),
    4: torch.tensor([math.exp(-4) * (4 ** k) / math.factorial(k) for k in range(15)], device=device)
}


class RentingCar:
    def __init__(self):
        self.min_action = -5
        self.max_action = 5
        self.gamma = 0.9
        self.max_car_num = 20

        # 将value和policy张量移到GPU
        self.value = torch.zeros((self.max_car_num + 1, self.max_car_num + 1),
                                 dtype=torch.float32, device=device)
        self.policy = torch.zeros((self.max_car_num + 1, self.max_car_num + 1),
                                  dtype=torch.int32, device=device)

        # 预先生成所有可能的k值
        self.k_values = torch.arange(15, device=device)
        return

    def get_poisson_pairs(self, lambda_):
        # 使用PyTorch张量操作替代列表
        k_values = self.k_values
        probs = POISSON_TABLES[lambda_]
        return k_values, probs

    def poisson_occur(self, car_num, in_lambda, out_lambda):
        # 使用张量操作替代双重循环
        in_k, in_probs = self.get_poisson_pairs(in_lambda)
        out_k, out_probs = self.get_poisson_pairs(out_lambda)

        # 使用广播机制计算所有组合
        in_k_exp = in_k.unsqueeze(1)  # (in_size, 1)
        out_k_exp = out_k.unsqueeze(0)  # (1, out_size)
        in_probs_exp = in_probs.unsqueeze(1)  # (in_size, 1)
        out_probs_exp = out_probs.unsqueeze(0)  # (1, out_size)

        # 计算所有可能的y值
        y = car_num + in_k_exp - out_k_exp
        y = torch.clamp(y, 0, self.max_car_num)  # 限制在0到max_car_num之间

        # 计算所有可能的earn值
        actual_out = torch.minimum(out_k_exp, car_num + in_k_exp)
        earn = actual_out * 10

        # 计算联合概率
        probs = in_probs_exp * out_probs_exp

        return y, earn, probs

    def poisson_result(self, next_num1, next_num2):
        # 获取两个店铺的结果
        y1, e1, p1 = self.poisson_occur(next_num1, 3, 3)
        y2, e2, p2 = self.poisson_occur(next_num2, 2, 4)

        # 计算所有组合的奖励和概率
        reward = e1.unsqueeze(-1).unsqueeze(-1) + e2.unsqueeze(0).unsqueeze(0)
        joint_prob = p1.unsqueeze(-1).unsqueeze(-1) * p2.unsqueeze(0).unsqueeze(0)

        # 获取对应的value值
        value_matrix = self.value[y1.unsqueeze(-1).unsqueeze(-1), y2.unsqueeze(0).unsqueeze(0)]

        # 计算总奖励
        total_reward = torch.sum(joint_prob * (reward + self.gamma * value_matrix))

        return total_reward.item()

    def value_iteration(self, checkpoint="./rentcar_0.pth"):
        iterate_num = 0
        if os.path.exists(checkpoint):
            data = torch.load(checkpoint, map_location=device)
            self.value = data['value'].to(device)
            iterate_num = data['iterate_num']

        # 预先生成所有可能的动作
        actions = torch.arange(self.min_action, self.max_action + 1, device=device)

        while True:
            delta = 0
            iterate_num += 1

            # 创建网格坐标
            num1_grid, num2_grid = torch.meshgrid(
                torch.arange(self.max_car_num + 1, device=device),
                torch.arange(self.max_car_num + 1, device=device),
                indexing='ij'
            )

            # 初始化新value矩阵
            new_value = torch.zeros_like(self.value)
            new_policy = torch.zeros_like(self.policy)

            # 对每个状态进行并行处理
            for num1 in tqdm(range(self.max_car_num + 1), "shop1"):
                for num2 in range(self.max_car_num + 1):
                    old_value = self.value[num1, num2].item()

                    # 计算所有动作的奖励
                    action_rewards = []
                    for action in actions:
                        # 当晚搬运汽车
                        true_action = action
                        if num1 - action < 0:
                            true_action = num1
                        if num2 + action < 0:
                            true_action = -num2

                        move_cost = abs(true_action) * 2
                        next_num1 = min(num1 - true_action, self.max_car_num)
                        next_num2 = min(num2 + true_action, self.max_car_num)

                        # 计算总奖励
                        total_reward = self.poisson_result(next_num1, next_num2) - move_cost
                        action_rewards.append(total_reward)

                    # 找到最佳动作和最大奖励
                    action_rewards = torch.tensor(action_rewards, device=device)
                    max_reward, best_action_idx = torch.max(action_rewards, dim=0)
                    best_action = actions[best_action_idx]

                    new_value[num1, num2] = max_reward
                    new_policy[num1, num2] = best_action

                    delta = max(delta, abs(old_value - max_reward.item()))

            # 更新value
            self.value = new_value

            print(f"{iterate_num} finish one iteration, delta={delta:.4f}")

            # 保存检查点
            data = {
                'value': self.value.cpu(),
                'iterate_num': iterate_num
            }
            torch.save(data, f"./rentcar_{iterate_num}.pth")

            if delta < 0.01:
                break

        # 提取最终策略
        self.policy = new_policy

    def print_policy(self):
        policy_cpu = self.policy.cpu()
        for num1 in range(self.max_car_num + 1):
            for num2 in range(self.max_car_num + 1):
                print(f"{policy_cpu[num1, num2].item()} ", end="")
            print()

    def print_value(self):
        value_cpu = self.value.cpu()
        for num1 in range(self.max_car_num + 1):
            for num2 in range(self.max_car_num + 1):
                print(f"{value_cpu[num1, num2].item():.2f} ", end="")
            print()


rc = RentingCar()
rc.value_iteration()
rc.print_policy()
rc.print_value()
```

结果与书上的结果一致：

![image-20250404215757460](img/RL/image-20250404215757460.png)

##### 10.3.4 练习4 赌徒策略

一名赌徒有机会对一系列抛硬币的结果进行押注。若硬币为正面朝上，他将赢得与该次押注金额相等的美元；若为反面朝上，则损失所押注的金额。当赌徒达到100美元的目标时获胜，或资金耗尽时失败，游戏即告终止。每次抛硬币前，赌徒需决定以整数美元为单位押注其当前资本的多少比例。

该问题可被表述为一个**无折扣、分段式、有限马尔可夫决策过程（MDP）**。其状态为赌徒的资本额 s∈{ 1,2,...,99 }，动作为押注金额 a∈{ 1,...,min⁡(s,100−s)  }。除赌徒达成目标时获得+1奖励外，其他状态转移的奖励均为零。此时，状态价值函数表示从各状态出发的获胜概率。

策略是从资本额到押注金额的映射，最优策略则最大化达成目标的概率。设 p为硬币正面朝上的概率。若 p 已知，则可通过**价值迭代**等方法求解此问题。

```python
import os.path
import matplotlib.pyplot as plt
import torch


class Gamble:
    def __init__(self):
        self.min_action = 1
        self.p_head = 0.4
        self.gamma = 1
        self.min_state = 1
        self.max_state = 99
        self.value = torch.zeros((self.max_state+1,),dtype=torch.float32)
        self.policy = torch.zeros((self.max_state+1,),dtype=torch.int32)
        return

    def value_iteration(self, checkpoint="./gamble_0.pth"):
        iterate_num = 0
        if os.path.exists(checkpoint):
            data = torch.load(checkpoint)
            self.value = data['value']
            iterate_num = data['iterate_num']

        while True:
            delta = 0
            iterate_num += 1
            for state in range(self.min_state, self.max_state+1):
                max_reward_action = 0
                max_reward = float('-inf')
                old_value = self.value[state].item()
                max_action = min(state, 100-state)
                for action in range(self.min_action, max_action+1):
                    total_reward = 0
                    ################# 上概率 ####################
                    ######### head,win
                    next_state = state + action # 一定的概率赢
                    if next_state > self.max_state: #到达终态，终态的V = 0
                        total_reward += self.p_head * (1 + self.gamma * 0)
                    else:
                        total_reward += self.p_head * (0 + self.gamma * self.value[next_state].item())
                    ######### not head, lost
                    next_state = state - action
                    if next_state < 1: # 输光
                        total_reward += (1-self.p_head) * (0 + self.gamma * 0)
                    else:
                        total_reward += (1-self.p_head) * (0 + self.gamma * self.value[next_state].item())

                    if total_reward > max_reward:
                        max_reward = total_reward
                        max_reward_action = action
                    # 下面几行能看出最佳策略不是唯一的，奇怪的是action还是没有取偏小的那个
                    elif total_reward == max_reward:
                        print(f"hit, {action} and {max_reward_action} for {state}")
                        max_reward_action = min(max_reward_action, action)
                self.value[state] = max_reward
                self.policy[state] = max_reward_action
                delta = max(delta, abs(old_value - self.value[state].item()))

            print(f"{iterate_num} finish one iteration, delta={delta:.4f}")

            # save checkpoint
            data = {}
            data['value'] = self.value
            data['iterate_num'] = iterate_num
            #torch.save(data, f"./gamble_{iterate_num}.pth")

            if delta < 0.001:
                break
        

    def print_policy(self):
        """打印策略并绘制折线图"""
        print("policy:")
        policy_values = []
        for state in range(self.min_state, self.max_state + 1):
            val = self.policy[state].item()
            print(f"{state}:{val} ", end="")
            policy_values.append(val)
            if state % 10 == 0:
                print("")
        print("")

        # 绘制策略折线图
        plt.figure(figsize=(10, 5))
        plt.plot(range(self.min_state, self.max_state + 1), policy_values, 'b-o')
        plt.title("Policy Function")
        plt.xlabel("State (Capital)")
        plt.ylabel("Stake Amount")
        plt.grid(True)
        plt.show()

    def print_value(self):
        """打印价值函数并绘制折线图"""
        print("value:")
        value_values = []
        for state in range(self.min_state, self.max_state + 1):
            val = self.value[state].item()
            print(f"{state}:{val:.4f} ", end="")
            value_values.append(val)
            if state % 10 == 0:
                print("")
        print("")

        # 绘制价值函数折线图
        plt.figure(figsize=(10, 5))
        plt.plot(range(self.min_state, self.max_state + 1), value_values, 'r-o')
        plt.title("Value Function (Probability of Winning)")
        plt.xlabel("State (Capital)")
        plt.ylabel("Probability")
        plt.grid(True)
        plt.show()


g = Gamble()
g.value_iteration()

g.print_policy()
g.print_value()
```

我运行的结果（右边）与书上的答案（左边）不同，不过作者也说不只有一个最终的策略：

![image-20250404185702040](img/RL/image-20250404185702040.png)

#### 10.4 控制算法的CUDA化

##### 10.4.1 算法描述

前面汽车门店调度的问题，状态和动作空间并不大，用CPU串行计算需要4-5个小时，改为GPU矩阵运算后只需要不到10分钟。所以控制算法CUDA化非常有必要。

下面是AI帮我整理的CUDA化版本的两大控制算法。

策略迭代算法

```python
"""
    基于CUDA并行的策略迭代算法（含终止态处理）
    
    参数说明:
    ----------
    P : torch.Tensor, shape [|S|, |A|, |S|]
        转移概率张量，P[s, a, s'] 表示在状态s执行动作a后转移到状态s'的概率
        要求: 每对 (s,a) 的概率分布需归一化，即 Σ_{s'} P[s,a,s'] = 1
        
    R : torch.Tensor, shape [|S|, |A|]
        即时奖励张量，R[s, a] 表示在状态s执行动作a获得的即时奖励
        
    γ : float
        折扣因子（0 ≤ γ < 1），用于平衡当前奖励与未来奖励的权重
        
    max_iters : int
        最大外层迭代次数（策略改进轮次）
        
    terminal_mask : torch.Tensor, shape [|S|]
        终止态布尔掩码，terminal_mask[s] = True 表示状态s为终止态
        终止态的特性:
        1. 值函数固定为 V(s) = 0
        2. 无需策略动作（策略π[s]的值无意义）
        3. 不参与贝尔曼更新和收敛性检查
    
    输出:
    -------
    π : torch.Tensor, shape [|S|]
        最优策略，π[s] ∈ [0, |A|-1] 表示状态s下的最优动作索引
        注意: 终止态的策略值会被保留为初始值（实际无意义）
    
    内部张量说明:
    ----------
    V : torch.Tensor, shape [|S|]
        值函数，V[s] 表示状态s的长期期望回报
        终止态的V值始终被强制设为0
        
    Q : torch.Tensor, shape [|S|, |A|]
        Q值表，Q[s, a] 表示在状态s执行动作a的长期期望回报
        终止态的Q值会被设为 -inf，避免被argmax选中
    """
def policy_iteration_cuda(P, R, γ, max_iters, terminal_mask):
    # 初始化 (所有张量在GPU上)
    V = zeros(|S|, device='cuda')          # 价值函数
    π = randint(0, |A|, size=(|S|,), device='cuda')  # 随机策略
    
    while not converged:
        # -------------------- 策略评估 (并行化Bellman更新) -------------------
        for _ in range(max_eval_iters):
            V_old = V.clone()
            
            # 关键步骤1：选择当前策略的动作 π[s] -> a
            a_indices = π.unsqueeze(1).unsqueeze(2)  # [|S|, 1, 1]
            
            # 关键步骤2：广播选取P和R（跳过终止态）
            P_selected = torch.gather(P, 1, a_indices.expand(-1, -1, |S|)).squeeze(1)  # [|S|, |S|]
            R_selected = torch.gather(R, 1, π.unsqueeze(1)).squeeze(1)  # [|S|]
            
            # 关键步骤3：并行计算V(s)（终止态值固定为0）
            #   - P_selected: [|S|, |S|] 形状，表示在策略π下，从状态s转移到s'的概率 P(s'|s, π(s))
            #   - R_selected: [|S|] 形状，表示在策略π下的即时奖励 R(s, π(s))
            #   - V_old: [|S|] 形状，上一轮迭代的价值函数
            #   - γ: 折扣因子
            #   - terminal_mask: [|S|] 形状的布尔掩码，True表示终止态
            # 计算 V(s) = Σ_{s'} P(s'|s,π(s)) * [R(s,π(s)) + γ * V_old(s')]
            # 解释:
            #   - R_selected.unsqueeze(1): 将奖励从[|S|]扩展为[|S|, 1]，以便广播到所有s'
            #   - V_old.unsqueeze(0): 将V_old从[|S|]扩展为[1, |S|]，匹配P_selected的维度
            #   - R + γV_old: 广播后得到[|S|, |S|]，每个元素为 R(s,π(s)) + γ*V_old(s')
            #   - P_selected * (R + γV_old): 按概率加权，结果仍为[|S|, |S|]
            #   - .sum(dim=1): 对s'维度求和，得到[|S|]形状的V_update
            V_update = (P_selected * (R_selected.unsqueeze(1) + γ * V_old.unsqueeze(0))).sum(dim=1)
            # 处理终止态：强制设置终止态的V值为0
            V = torch.where(terminal_mask, torch.zeros_like(V), V_update)  # 终止态掩码覆盖
            
            # 提前终止检查（仅检查非终止态）
            non_terminal_diff = (V - V_old).abs() * (~terminal_mask)  # 掩码过滤
            if non_terminal_diff.max() < δ:
                break
		
        # ---------------- 策略改进 (并行化argmax) ----------------------------
        # 关键步骤1：计算所有动作的Q值（终止态Q值设为-inf）
        #   - P: [|S|, |A|, |S|] 形状，转移概率 P(s'|s,a)
        #   - R: [|S|, |A|] 形状，即时奖励 R(s,a)
        #   - V: [|S|] 形状，当前价值函数
        #   - γ: 折扣因子
        #   - terminal_mask: [|S|] 形状的布尔掩码
        # 计算 Q(s,a) = Σ_{s'} P(s'|s,a) * [R(s,a) + γ * V(s')]
        # 解释:
        #   - R.unsqueeze(2): 将R从[|S|, |A|]扩展为[|S|, |A|, 1]，以便广播到s'
        #   - V.unsqueeze(0).unsqueeze(0): 将V从[|S|]扩展为[1, 1, |S|]，匹配P的维度
        #   - R + γV: 广播后得到[|S|, |A|, |S|]，每个元素为 R(s,a) + γ*V(s')
        #   - P * (R + γV): 按概率加权，结果仍为[|S|, |A|, |S|]
        #   - .sum(dim=2): 对s'维度求和，得到[|S|, |A|]形状的Q值表
        Q = (P * (R.unsqueeze(2) + γ * V.unsqueeze(0).unsqueeze(0))).sum(dim=2)  # [|S|,|A|]
        # 处理终止态：将终止态的所有动作Q值设为-inf
        Q = torch.where(terminal_mask.unsqueeze(1), -torch.inf, Q)  # 终止态动作无效
        
        # 关键步骤2：并行argmax（终止态策略保持任意值）
        π_new = Q.argmax(dim=1)
        π_new = torch.where(terminal_mask, π, π_new)  # 终止态策略不变
        
        # 收敛检查（忽略终止态）
        if (π[~terminal_mask] == π_new[~terminal_mask]).all():
            break
        π = π_new
    
    return π
```

价值迭代算法

```python
# 输入: 
#   P: 转移概率矩阵，形状 [|S|, |A|, |S|], P[s,a,s'] = P(s'|s,a)
#   R: 即时奖励矩阵，形状 [|S|, |A|], R[s,a] = R(s,a)
#   γ: 折扣因子，标量
#   max_iters: 最大迭代次数，标量
#   terminal_mask: 终止态掩码，形状 [|S|], True表示终止态
# 输出:
#   V: 最优价值函数，形状 [|S|]
#   π: 最优策略，形状 [|S|]
def value_iteration_cuda(P, R, γ, max_iters, terminal_mask):
    # 初始化 (所有张量在GPU上)
    V = torch.zeros(|S|, device='cuda')  # 价值函数
    V = torch.where(terminal_mask, torch.zeros_like(V), V)  # 显式设置终止态V=0

    while not converged:
        V_old = V.clone()
        
        # --- 并行计算所有Q值 [|S|, |A|] ---
        # 关键步骤1: 广播计算 Q(s,a) = Σ P(s,a,s') * (R(s,a) + γV_old(s'))
        # 步骤1.1: 扩展R的维度以匹配P [广播准备] [|S|,|A|] → [|S|,|A|,1]
        # 步骤1.2: 扩展V_old的维度以匹配P [广播准备] 	[|S|] → [1,1,|S|]
        # 步骤1.3: 计算 R(s,a) + γV_old(s') [广播机制] ，R_expanded + γ * V_expanded  得到形状 [|S|,|A|,|S|]
        # 步骤1.4: 计算 P * (R + γV) 并按s'维度求和  [|S|,|A|,|S|] → [|S|,|A|]
        Q = (P * (R.unsqueeze(2) + γ * V_old.unsqueeze(0).unsqueeze(0))).sum(dim=2)  # [|S|, |A|]
        
        # 关键步骤2: 终止态处理 (Q值设为-inf，避免被max选中)
        Q = torch.where(terminal_mask.unsqueeze(1), -torch.inf, Q)  # [|S|, |A|]
        
        # --- 取每状态的最大Q值更新V ---
        V_new = Q.max(dim=1).values  # 并行规约
        V = torch.where(terminal_mask, torch.zeros_like(V), V_new)  # 终止态保持V=0
        
        # --- 收敛检查 (忽略终止态) ---
        non_terminal_diff = (V - V_old).abs() * (~terminal_mask)  # 掩码过滤
        if non_terminal_diff.max() < δ:
            break

    # --- 提取策略 (终止态策略设为任意值，如0) ---
    Q = (P * (R.unsqueeze(2) + γ * V.unsqueeze(0).unsqueeze(0))).sum(dim=2)
    Q = torch.where(terminal_mask.unsqueeze(1), -torch.inf, Q)  # 终止态屏蔽
    π = Q.argmax(dim=1)
    π = torch.where(terminal_mask, torch.zeros_like(π), π)  # 终止态策略设为0

    return V, π
```

##### 10.4.2 实操练习

还用汽车门店调度的例子

```python
#import torch
import torch
import math
from tqdm import tqdm

device = "cuda"
state_num = 21 * 21
action_num = 11 # [-5, 5]

def car_num_to_state(num1:int, num2:int):
    return num1 * 21 + num2

def state_to_car_num(state:int):
    return state // 21, state%21

def value_iteration(P: torch.Tensor, R: torch.Tensor, gamma: float, epsilon: float = 1e-6, max_iter: int = 1000):
    """
    Corrected value iteration implementation with proper tensor operations.

    Args:
        P: Transition matrix of shape (S, A, S)
        R: Reward matrix of shape (S, A)
        gamma: Discount factor
        epsilon: Convergence threshold
        max_iter: Maximum iterations

    Returns:
        policy: (S,) tensor of optimal actions
        V: (S,) tensor of state values
    """
    S, A = R.shape
    V = torch.zeros(S, dtype=torch.float32, device=P.device)

    for iter_num in range(1, max_iter + 1):
        V_prev = V.clone()

        # Correct way to compute expected future value:
        # For each state s and action a: sum over s' of P(s'|s,a)*V(s')
        # We can do this efficiently using matrix multiplication
        # Reshape V to (1, 1, S) for batch matrix multiplication
        EV = torch.bmm(P, V_prev.view(1, -1, 1).expand(S, -1, 1)).squeeze(-1)  # shape (S, A)

        # Compute Q-values
        Q = R + gamma * EV

        # Update value function
        V, _ = Q.max(dim=1)

        # Check convergence
        delta = torch.norm(V - V_prev, p=torch.inf).item()
        if iter_num % 10 == 0:
            print(f"Iter {iter_num}: delta = {delta:.6f}")

        if delta < epsilon:
            print(f"\nConverged after {iter_num} iterations")
            break

    # Compute final policy
    EV_final = torch.bmm(P, V.view(1, -1, 1).expand(S, -1, 1)).squeeze(-1)
    Q_final = R + gamma * EV_final
    policy = Q_final.argmax(dim=1)

    return policy, V

# 预计算泊松分布表 (λ=2,3,4; x=0~20)， 用查表法加速
POISSON_TABLES = {
    2: [math.exp(-2) * (2 ** k) / math.factorial(k) for k in range(15)],
    3: [math.exp(-3) * (3 ** k) / math.factorial(k) for k in range(15)],
    4: [math.exp(-4) * (4 ** k) / math.factorial(k) for k in range(15)]
}
poisson_occur_cache = {} # 避免poisson_occur()函数重复计算 的cache
#直接查表得到泊松分布的 x值和概率  对的列表
def get_poisson_pairs(lambda_):
    k_values = [k for k in range(15)]
    probs = POISSON_TABLES[lambda_]
    return list(zip(k_values, probs))
poisson_occur_cache = {} # 避免poisson_occur()函数重复计算 的cache

# 门店经过晚上移车后的早晨有car_num辆车，按照两个lambda参数发生还车(in)和借车(out)
def poisson_occur(car_num, in_lambda, out_lambda):
    '''
    门店，现存有 x 辆车，当天还车的数量in 服从lambda=3的泊松分布，
    当天借出去的车的数量out 服从lambda=4的泊松分布，两个分布独立。
    y不能小于0.
    当天闭店时的车辆数 y 等于 x + in - out，求y的各种取值的概率：
    1）枚举所有 in的可能值和发生概率（in, in_p），得到列表1
    2）枚举所有out的可能值和发生概率 (out, out_p)，得到列表2
    3）两两组合列表1和列表2的所有可能， 计算y = x + in - out，和出现概率p =  in_p * out_p， 如果y < 0, 令y=0。
    4) 把 (y, p)放到一个列表里
    '''
    # 为了避免重复计算，计算的结果放到缓存里
    global poisson_occur_cache
    key = (car_num, in_lambda, out_lambda)
    if poisson_occur_cache.__contains__(key):
        # print(f"hit cache {key}")
        return poisson_occur_cache[key]
    result = []  # 保存 三元组 (y值，挣的钱，发生概率)
    in_list = get_poisson_pairs(in_lambda)
    out_list = get_poisson_pairs(out_lambda)
    for in_c, in_p in in_list:
        for out_c, out_p in out_list:
            y = car_num + in_c - out_c
            if y < 0:  # 车辆数 入不敷出
                y = 0
                out_c = car_num + in_c  # 那最多就只能出租这么多车了
            if y > 20:
                y = 20
            earn = out_c * 10  # 挣的佣金
            p = in_p * out_p
            result.append((y, earn, p))
    psum = 0
    for (y, e, p) in result:
        psum += p
    assert ( abs(psum - 1.0) < 0.0001, f"psum is not 1.0:{psum:.2f} in poisson_occur()")
    poisson_occur_cache[key] = result
    return result


def print_policy(policy):
    for num1 in range(20 + 1):
        for num2 in range(20 + 1):
            state = car_num_to_state(num1, num2)
            print(f"{policy[state].item()-5} ",end="")
        print("")


def print_value(value):
    for num1 in range(20 + 1):
        for num2 in range(20 + 1):
            state = car_num_to_state(num1, num2)
            print(f"{value[state].item():.2f} ",end="")
        print("")
#这个本身特别耗时，需要2个小时，必须改为cuda
def prepare_arguments():
    P = torch.zeros((state_num, action_num, state_num), dtype=torch.float32, device=device)
    R = torch.zeros((state_num, action_num), dtype=torch.float32, device=device)
    for s in tqdm(range(state_num), "prepare argument"):
        for a in range(action_num):
            action = a - 5
            num1, num2 = state_to_car_num(s)
            # 当晚搬运汽车
            true_action = action
            if num1 - action < 0:
                true_action = num1
            if num2 + action < 0:
                true_action = num2 * (-1)
            move_cost = abs(true_action) * 2  # 挪车开销
            next_num1 = min(num1 - true_action, 20)
            next_num2 = min(num2 + true_action, 20)
            # 第二天发生还车借车随机事件
            shop1 = poisson_occur(next_num1, 3, 3)
            shop2 = poisson_occur(next_num2, 2, 4)
            psum = 0.0
            R[s, a] -= move_cost
            for (y1, e1, p1) in shop1:
                for (y2, e2, p2) in shop2:
                    p = p1 * p2
                    psum += p
                    reward = e1 + e2
                    next_state = car_num_to_state(y1, y2)
                    P[s, a, next_state] += p  # 指定s,a的情况下，next_state可能重复出现，所以要 +=
                    R[s, a] += p * reward #这里有点不好理解,因为每个reward都是一定概率发生，不是百分百确定，s,a会导致去向不同的s'
            assert ( abs(psum - 1.0) < 0.0001, f"psum is not 1.0:{psum:.2f} in prepare_arguments()")
    print(P)
    print(R)
    torch.save( (P, R), "./P_R.pth")  
    return P, R, 0.9 #0.9是gamma

def main():
    P,R,gamma = prepare_arguments()

    policy, value = value_iteration(P, R, gamma)
    print_policy(policy)
    print_value(value)

main()
```

AI写的用CUDA加速的数据准备过程：

```python
import torch
import math
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
state_num = 21 * 21
action_num = 11  # [-5, 5]

# 预计算泊松分布表 (λ=2,3,4; x=0~14)
POISSON_TABLES = {
    2: torch.tensor([math.exp(-2) * (2 ** k) / math.factorial(k) for k in range(15)], device=device),
    3: torch.tensor([math.exp(-3) * (3 ** k) / math.factorial(k) for k in range(15)], device=device),
    4: torch.tensor([math.exp(-4) * (4 ** k) / math.factorial(k) for k in range(15)], device=device)
}

def car_num_to_state(num1, num2):
    return int(num1 * 21 + num2)  # 确保返回整数

def state_to_car_num(state):
    return state // 21, state % 21

def get_poisson_pairs(lambda_):
    k_values = torch.arange(15, device=device)
    probs = POISSON_TABLES[lambda_]
    return torch.stack((k_values, probs), dim=1)

def compute_poisson_occur(car_num, in_lambda, out_lambda):
    in_pairs = get_poisson_pairs(in_lambda)
    out_pairs = get_poisson_pairs(out_lambda)
    
    in_c = in_pairs[:, 0].view(-1, 1)
    in_p = in_pairs[:, 1].view(-1, 1)
    out_c = out_pairs[:, 0].view(1, -1)
    out_p = out_pairs[:, 1].view(1, -1)
    
    y = car_num + in_c - out_c
    y = torch.clamp(y, min=0, max=20)
    out_c_actual = torch.where(car_num + in_c - out_c < 0, car_num + in_c, out_c)
    earn = out_c_actual * 10
    p = in_p * out_p
    
    unique_y = torch.unique(y)
    result = []
    for val in unique_y:
        mask = (y == val)
        total_p = p[mask].sum()
        total_earn = (earn[mask] * p[mask]).sum() / total_p if total_p > 0 else 0
        result.append((int(val.item()), float(total_earn), float(total_p)))  # 确保类型正确
    
    return result

# 预计算泊松缓存
poisson_cache = {}
for car_num in range(21):
    for in_lambda, out_lambda in [(3, 3), (2, 4)]:
        key = (car_num, in_lambda, out_lambda)
        poisson_cache[key] = compute_poisson_occur(car_num, in_lambda, out_lambda)

def prepare_arguments_cuda():
    P = torch.zeros((state_num, action_num, state_num), dtype=torch.float32, device=device)
    R = torch.zeros((state_num, action_num), dtype=torch.float32, device=device)
    
    states = torch.arange(state_num, device=device)
    actions = torch.arange(action_num, device=device) - 5
    
    num1, num2 = state_to_car_num(states)
    num1 = num1.view(-1, 1)
    num2 = num2.view(-1, 1)
    
    for a_idx in tqdm(range(action_num), desc="Processing actions"):
        action = actions[a_idx]
        
        true_action = torch.where(num1 - action < 0, num1, action)
        true_action = torch.where(num2 + true_action < 0, -num2, true_action)
        move_cost = torch.abs(true_action) * 2
        
        next_num1 = torch.clamp(num1 - true_action, 0, 20)
        next_num1 = next_num1.long()  # 确保是整数
        next_num2 = torch.clamp(num2 + true_action, 0, 20)
        next_num2 = next_num2.long()  # 确保是整数
        
        for s in range(state_num):
            n1 = next_num1[s].item()
            n2 = next_num2[s].item()
            
            shop1 = poisson_cache[(n1, 3, 3)]
            shop2 = poisson_cache[(n2, 2, 4)]
            
            for (y1, e1, p1) in shop1:
                for (y2, e2, p2) in shop2:
                    p = p1 * p2
                    reward = e1 + e2
                    next_state = car_num_to_state(y1, y2)  # 这里已经是整数
                    
                    P[s, a_idx, next_state] += p
                    R[s, a_idx] += p * reward
            
            R[s, a_idx] -= move_cost[s].item()
    
    # 验证概率总和
    for s in range(state_num):
        for a in range(action_num):
            prob_sum = P[s, a].sum().item()
            if abs(prob_sum - 1.0) > 1e-4:
                print(f"Warning: Probability sum is {prob_sum:.6f} for state {s}, action {a}")
                # 归一化处理
                P[s, a] /= prob_sum
    
    torch.save((P.cpu(), R.cpu()), "./P_R.pth")
    return P, R, 0.9

if __name__ == "__main__":
    P, R, gamma = prepare_arguments_cuda()
    print("Transition matrix P shape:", P.shape)
    print("Reward matrix R shape:", R.shape)
```

运行结果是正确的，同10.3.3的结果截图。

### 11. 表格型方法

#### 11.1 Q-Learning法实现CliffWalk

cliffwaling的游戏介绍在[这里](https://gymnasium.farama.org/environments/toy_text/cliff_walking/)

代码如下，可以收敛

```python
import torch
import random
import gym
import numpy as np

# 创建环境
env = gym.make('CliffWalking-v0')
state_num = env.observation_space.n
action_num = env.action_space.n
gamma = 0.95  # 折扣因子
lr = 0.001


# 产生动作，一定概率随机（epsilon-greedy）
def generate_action(epsilon, q:torch.Tensor, state):
    if random.random() < epsilon:
        return random.randint(0, action_num - 1)
    else:
        return q[state].argmax().item()  # 根据当前Q函数选择最优动作



def Q_learning(epsilon, q:torch.Tensor):
    episode = 0
    for episode in range(100):  # 迭代次数
        # 收集一条轨迹，存放到buffer里
        buffer = []
        state = env.reset()[0]  # 重置环境并获取初始状态
        while True:
            action = generate_action(epsilon, q, state)
            next_state, reward, done, _, _ = env.step(action)
            buffer.append((state, action, reward, done, next_state))  #
            if done or len(buffer) > 200:
                break
            state = next_state

		#重放buffer并更新Q
        for s, a, r,d, ss in buffer:
            predict = q[s, a].item()
            if d:
                target = r
            else:
                target = r + gamma * q[ss].max().item()
            q[s, a] += lr * (target - predict)

    return


# 根据Q函数表格得到最佳策略函数（也是一个表格）
def get_policy_from_Q(Q):
    policy = torch.zeros((state_num,), dtype=torch.int32)
    for s in range(state_num):
        policy[s] = torch.argmax(Q[s]).item()  # 每个状态选择Q值最大的动作
    return policy


# 打印策略的函数
def print_policy(policy):
    arrows = {0: '↑', 2: '↓', 3: '←', 1: '→'}
    for i in range(state_num):
        print(f"{arrows[policy[i].item()]} ", end="")
        if (i + 1) % 12 == 0:
            print("")


# 反复迭代计算Q
def main():
    epsilon = 1.0  # 初始epsilon
    Q = torch.zeros((state_num, action_num), dtype=torch.float32)  # 初始化Q表
    for episode in range(100):  # 迭代次数
        Q_learning(epsilon, Q)
        # epsilon衰减
        epsilon = max(0.1, epsilon * 0.90)  # 保证epsilon不会小于0.1，避免过早陷入确定性策略

        if (episode+1) % 10 == 0:  # 每10个回合打印一次策略
            print(f"Updated Policy at Episode {episode}:")
            policy = get_policy_from_Q(Q)  # 从Q值更新策略
            print_policy(policy)
            print()

    print("Final Q-table:")
    print(Q)
    print("Final Policy:")
    policy = get_policy_from_Q(Q)  # 从Q值更新策略
    print_policy(policy)


if __name__ == "__main__":
    main()

```

结果能够收敛，在第89次的时候得到了正确的策略：

```shell
Updated Policy at Episode 89:
→ → → → ↓ → → → → ← ↓ ↑ 
→ → ↑ ↑ → → → → → → ↓ ↓ 
→ → → → → → → → → → → ↓ 
↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ 

Updated Policy at Episode 99:
→ → → → → → → → → ↓ → ↓ 
→ → → ↓ → → → → → → → ↓ 
→ → → → → → → → → → → ↓ 
↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ 
```

#### 11.2 蒙特卡洛+策略迭代法实现CliffWalk



![image-20250401192434621](img/RL/image-20250401192434621.png)

我发现 env并不能随意指定state进行step，它内部有连续的状态管理，reset后我step(0)，它会返回next_state=36，把agent强行拉回七点。所以我不能完全按照上面的算法先求V再求Q，而是直接用蒙特卡洛方法求Q。

并且，在师兄的指导下，我把跌入悬崖的reward特殊处理了一下，改为-3。这样在第39次迭代的时候收敛到了最优策略。后面就稳定了

```python
Episode #39: avg_reword:-11.81, traj_num:10000
Updated Policy at Episode 39:
↑ ← ↓ ↓ → → ↑ ↓ → → → → 
→ → ↓ ↓ → ↓ ↓ → → → → ↓ 
→ → → → → → → → → → → ↓ 
↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ 
```

如果掉悬崖情况下保持reward原值（-100），几个小时候跑了300个迭代，也没有收敛。

代码如下：

```python
import torch
import random
import gym
import numpy as np

# 创建环境
env = gym.make('CliffWalking-v0')
state_num = env.observation_space.n
action_num = env.action_space.n
gamma = 0.95  # 折扣因子


# 产生动作，一定概率随机（epsilon-greedy）
def generate_action(epsilon, policy, state):
    if random.random() < epsilon:
        return random.randint(0, action_num - 1)
    else:
        return policy[state].item()  # 根据当前策略选择最优动作


# 基于当前的策略（外加一些随机 epsilon-greedy），使用蒙特卡洛法计算Q价值函数（一张表格）
def calculate_Q(epsilon, policy):
    Q = torch.zeros((state_num, action_num), dtype=torch.float32)  # 初始化Q表
    N = torch.zeros((state_num, action_num), dtype=torch.int32)  # 记录每个state-action对的访问次数

    total_reward = 0.0
    trajectory_count = 0

    traj_num = 0
    for traj_num in range(10000): #这个一万次不能改小，改小了可能不收敛
        # 收集一条轨迹
        trajectory = []
        state = env.reset()[0]  # 重置环境并获取初始状态
        while True:
            action = generate_action(epsilon, policy, state)
            next_state, reward, done, _, _ = env.step(action)
            if reward == -100:
                reward = -3
            trajectory.append((state, action, reward))  # 存储轨迹 (state, action, reward)
            if done or len(trajectory) > 500:
                break
            state = next_state

        # 计算每一步的回报，更新Q表
        G = 0  # 初始化回报
        for s, a, r in reversed(trajectory):
            G = r + gamma * G  # 计算折扣回报
            N[s, a] += 1
            Q[s, a] += (G - Q[s, a].item()) / N[s, a].item()  # 使用蒙特卡洛方法更新Q表

        total_reward += G
        trajectory_count += 1

        # 如果每个状态的每个动作都有足够的访问次数，可以提前结束蒙特卡洛计算
        tmp = N >= 10
        tmp = tmp.count_nonzero().item()
        if tmp >= (38*action_num):  # 只要每个状态-动作对都访问了至少若干次就结束
            print("early stop")
            break

    average_reward = total_reward / trajectory_count if trajectory_count > 0 else 0
    return Q, average_reward, traj_num+1


# 根据Q函数表格得到最优策略函数（也是一个表格）
def get_policy_from_Q(Q):
    policy = torch.zeros((state_num,), dtype=torch.int32)
    for s in range(state_num):
        policy[s] = torch.argmax(Q[s]).item()  # 每个状态选择Q值最大的动作
    return policy


# 打印策略的函数
def print_policy(policy):
    arrows = {0: '↑', 2: '↓', 3: '←', 1: '→'}
    for i in range(state_num):
        print(f"{arrows[policy[i].item()]} ", end="")
        if (i + 1) % 12 == 0:
            print("")


# 策略迭代法：根据当前policy计算Q -> 根据Q更新policy -> 根据新的policy计算Q-->
def main():
    epsilon = 1.0  # 初始epsilon
    policy = torch.zeros((state_num,), dtype=torch.int32)  # 初始化随机策略
    for episode in range(100):  # 迭代次数
        Q, avg_r, traj_num = calculate_Q(epsilon, policy)
        print(f"Episode #{episode}: avg_reword:{avg_r:.2f}, traj_num:{traj_num}")
        policy = get_policy_from_Q(Q)  # 从Q值更新策略

        # epsilon衰减
        epsilon = max(0.1, epsilon * 0.90)  # 保证epsilon不会小于0.1，避免过早陷入确定性策略

        if (episode+1) % 10 == 0:  # 每10个回合打印一次策略
            print(f"Updated Policy at Episode {episode}:")
            print_policy(policy)
            print()

    print("Final Q-table:")
    print(Q)
    print("Final Policy:")
    print_policy(policy)


if __name__ == "__main__":
    main()

```

### 12. DQN的各种优化方法

![image-20250405141817368](img/RL/image-20250405141817368.png)

### 13. 针对连续动作空间的DQN

#### 13.1 方法

![image-20250407102824611](img/RL/image-20250407102824611.png)

#### 13.2 练习

gym里钟摆小游戏的动作空间是连续的[-2, 2]，下面用Solution 1 尝试训练一个Q网络：

```python
import random
import gym
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from collections import deque
from torch.utils.tensorboard import SummaryWriter
import os
import datetime as dt

# 设备选择
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 超参数
gamma = 0.99  # 折扣因子
epsilon = 1.0  # 初始探索率
epsilon_min = 0.05  # 最低探索率
epsilon_decay = 0.995  # 探索率衰减
learning_rate = 1e-4  # 学习率
batch_size = 256  # 更合理的批量大小
memory_size = 100000  # 更大的经验池
target_update_freq = 100  # 目标网络更新频率
max_steps = 2000000  # 最大训练步数
eval_interval = 1000  # 评估间隔
num_candidates = 1000 #等间隔采样连续的动作的个数

writer = SummaryWriter("./logs")
env = gym.make("Pendulum-v1")

n_state = env.observation_space.shape[0]  # 状态维度
n_action = env.action_space.shape[0]
action_high = env.action_space.high[0]
action_low = env.action_space.low[0]

# 状态归一化参数
state_mean = np.array([0.0, 0.0, 0.0])
state_std = np.array([1.0, 1.0, 8.0])


# DQN 网络定义
class QNetwork(nn.Module):
    def __init__(self, state_dim=3, action_dim=1):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim + action_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, 1)

    def forward(self, state, action):
        x = torch.cat([state, action], dim=1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)


# 初始化网络
model = QNetwork(n_state, n_action).to(device)
target_model = QNetwork(n_state, n_action).to(device)
target_model.load_state_dict(model.state_dict())
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
memory = deque(maxlen=memory_size)


def normalize_state(state):
    return (state - state_mean) / state_std


def select_action(state, epsilon, noise_scale=0.1):
    """基于 ε-greedy 选择动作"""
    if random.random() < epsilon:
        # 随机动作加上一些噪声
        action = np.random.uniform(action_low, action_high)
    else:
        # 使用确定性策略选择动作
        state = torch.FloatTensor(normalize_state(state)).unsqueeze(0).to(device)

        # 生成候选动作 (更高效的方式)
        with torch.no_grad():
            # 生成一组候选动作
            actions = torch.linspace(action_low, action_high, steps=num_candidates, device=device).view(-1, 1)
            states = state.repeat(num_candidates, 1)
            q_values = model(states, actions)
            best_action = actions[q_values.argmax()]

            # 添加一些噪声增加探索
            noise = torch.randn_like(best_action) * noise_scale
            action = torch.clamp(best_action + noise, action_low, action_high).item()

    return np.array([action])


def train(step):
    if len(memory) < batch_size :
        return 0.0

    # 从经验池中采样
    batch = random.sample(memory, batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)

    # 转换为张量并归一化
    states = torch.FloatTensor(np.array(states)).to(device)
    states = (states - torch.FloatTensor(state_mean).to(device)) / torch.FloatTensor(state_std).to(device)

    actions = torch.FloatTensor(np.array(actions)).to(device)
    rewards = torch.FloatTensor(np.array(rewards)).unsqueeze(1).to(device)
    next_states = torch.FloatTensor(np.array(next_states)).to(device)
    next_states = (next_states - torch.FloatTensor(state_mean).to(device)) / torch.FloatTensor(state_std).to(device)
    dones = torch.FloatTensor(np.array(dones)).unsqueeze(1).to(device)

    # 计算当前Q值
    current_q = model(states, actions)

    # 计算目标Q值
    with torch.no_grad():
        # 生成候选动作

        candidate_actions = torch.linspace(action_low, action_high, steps=num_candidates, device=device).view(-1, 1)

        # 计算每个next_state对应的最大Q值
        next_q_values = []
        for next_state in next_states:
            next_state_repeated = next_state.unsqueeze(0).repeat(num_candidates, 1)
            q_values = target_model(next_state_repeated, candidate_actions)
            next_q_values.append(q_values.max())

        next_q_values = torch.stack(next_q_values).unsqueeze(1)
        target_q = rewards + gamma * next_q_values * (1 - dones)

    # 计算损失并更新
    loss = F.mse_loss(current_q, target_q)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # 记录日志
    writer.add_scalar("Train/Q_value", current_q.mean().item(), step)
    writer.add_scalar("Train/Loss", loss.item(), step)

    return loss.item()


def evaluate(step):
    total_reward = 0.0
    num_episodes = 1
    eval_env = gym.make("Pendulum-v1")
    for _ in range(num_episodes):
        state = eval_env.reset()
        if isinstance(state, tuple):
            state = state[0]
        episode_reward = 0.0
        done = False

        for i in range(100):
            action = select_action(state, 0.0)  # 无探索
            next_state, reward, done, _, _ = eval_env.step(action)
            episode_reward += reward
            state = next_state
            if done:
                break

        total_reward += episode_reward

    avg_reward = total_reward / num_episodes
    writer.add_scalar("Eval/Average Reward", avg_reward, step)
    eval_env.close()
    return avg_reward


def save_checkpoint(step, reward=None):
    os.makedirs("checkpoints", exist_ok=True)
    if reward is not None:
        path = f"checkpoints/dqn_{step}_reward_{reward:.1f}.pth"
    else:
        path = f"checkpoints/dqn_{step}.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'step': step,
        'epsilon': epsilon,
    }, path)
    #print(f"Checkpoint saved to {path}")


def load_checkpoint(path):
    if os.path.exists(path):
        checkpoint = torch.load(path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        global epsilon
        epsilon = checkpoint['epsilon']
        print(f"Checkpoint loaded from {path}, step {checkpoint['step']}")
        return checkpoint['step']
    else:
        print("No checkpoint found, starting from scratch.")
        return 0


def main(mode="train"):
    global epsilon
    global env
    if mode == "train":
        step_cnt = load_checkpoint("checkpoints/dqn_latest.pth")
        best_reward = -float('inf')

        state = env.reset()
        if isinstance(state, tuple):
            state = state[0]

        while step_cnt < max_steps:
            # 选择动作并执行

            action = select_action(state, epsilon)
            next_state, reward, done, _, _ = env.step(action)

            # 存储经验
            memory.append((state, action, reward, next_state, done))
            state = next_state
            step_cnt += 1

            # 训练模型

            loss = train(step_cnt)

            # 衰减探索率
            epsilon = max(epsilon_min, epsilon * epsilon_decay)

            # 更新目标网络
            if step_cnt % target_update_freq == 0:
                target_model.load_state_dict(model.state_dict())

            # 定期评估
            if step_cnt % eval_interval == 0:
                avg_reward = evaluate(step_cnt)
                print(f"{dt.datetime.now()} Step {step_cnt}, Eval Reward: {avg_reward:.1f}, Epsilon: {epsilon:.3f} best reward:{best_reward:.2f}")
                state = env.reset()
                if isinstance(state, tuple):
                    state = state[0]

                # 保存最佳模型
                if avg_reward > best_reward:
                    best_reward = avg_reward
                    save_checkpoint(step_cnt, best_reward)

                # 保存最新模型
                save_checkpoint(step_cnt)
                writer.flush()

            # 重置环境
            if done:
                state = env.reset()
                if isinstance(state, tuple):
                    state = state[0]

        writer.close()

    elif mode == "infer":
        load_checkpoint("checkpoints/dqn_36000_reward_-0.2.pth")
        env = gym.make("Pendulum-v1", render_mode='human')
        state = env.reset()
        if isinstance(state, tuple):
            state = state[0]
        total_reward = 0.0

        for i in range(200):
            env.render()
            action = select_action(state, 0.0)
            state, reward, done, _, _ = env.step(action)
            total_reward += reward

            if done:
                break

        print(f"Inference finished. Total reward: {total_reward:.1f}")


if __name__ == "__main__":
    main("infer")
```

结果能收敛，在3万次交互附近有两个特别大的reward的checkpoint，经验证确实效果很好：

![image-20250407215913159](img/RL/image-20250407215913159.png)

也录制了[一个5s的效果视频](img/RL/pendulum.mp4)



### 14. 针对连续动作空间的基于策略的方法

#### 14.1 DDPG（深度确定性策略梯度下降）方法

##### 14.1.1 原理

![image-20250407152417419](img/RL/image-20250407152417419.png)

【思考】

![image-20250407142207003](img/RL/image-20250407142207003.png)

##### 14.1.2 练习：玩转钟摆

下面这个代码是可以收敛的，把钟摆恰好的甩上去立起来

```python
import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import gym
from collections import deque, namedtuple


# 超参数配置
class Config:
    def __init__(self):
        self.env_name = "Pendulum-v1"  # 环境名称
        self.seed = 42  # 随机种子
        self.batch_size = 128  # 训练批次大小
        self.gamma = 0.99  # 折扣因子
        self.tau = 0.005  # 目标网络软更新系数
        self.actor_lr = 1e-4  # Actor学习率
        self.critic_lr = 1e-3  # Critic学习率
        self.buffer_size = 100000  # 经验回放缓冲区大小
        self.min_buffer_size = 1000  # 开始训练前的最小数据量
        self.max_episodes = 1000  # 最大训练回合数
        self.max_steps = 200  # 每回合最大步数
        self.exploration_noise = 0.1  # 动作探索噪声
        self.save_interval = 100  # 保存模型的间隔（回合数）
        self.checkpoint_dir = "./checkpoints"  # 模型保存路径
        self.log_dir = "./logs"  # TensorBoard日志路径


# 经验回放缓冲区
class ReplayBuffer:
    def __init__(self, buffer_size):
        self.buffer = deque(maxlen=buffer_size)
        self.experience = namedtuple("Experience", field_names=["state", "action", "reward", "next_state", "done"])

    def add(self, state, action, reward, next_state, done):
        e = self.experience(state, action, reward, next_state, done)
        self.buffer.append(e)

    def sample(self, batch_size):
        experiences = random.sample(self.buffer, batch_size)
        states = torch.FloatTensor(np.array([e.state for e in experiences]))
        actions = torch.FloatTensor(np.array([e.action for e in experiences]))
        rewards = torch.FloatTensor(np.array([e.reward for e in experiences])).unsqueeze(1)
        next_states = torch.FloatTensor(np.array([e.next_state for e in experiences]))
        dones = torch.FloatTensor(np.array([e.done for e in experiences])).unsqueeze(1)
        return states, actions, rewards, next_states, dones

    def __len__(self):
        return len(self.buffer)


# Actor网络（策略网络）
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, max_action):
        super(Actor, self).__init__()
        self.max_action = max_action
        self.fc1 = nn.Linear(state_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.tanh(self.fc3(x)) * self.max_action
        return x


# Critic网络（Q值网络）
class Critic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(Critic, self).__init__()
        self.fc1 = nn.Linear(state_dim + action_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, 1)

    def forward(self, x, a):
        x = torch.cat([x, a], dim=1)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x


# DDPG算法
class DDPG:
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # 初始化环境
        self.env = gym.make(config.env_name, render_mode="human")
        self.state_dim = self.env.observation_space.shape[0]
        self.action_dim = self.env.action_space.shape[0]
        self.max_action = float(self.env.action_space.high[0])

        # 初始化网络
        self.actor = Actor(self.state_dim, self.action_dim, self.max_action).to(self.device)
        self.actor_target = Actor(self.state_dim, self.action_dim, self.max_action).to(self.device)
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.actor_lr)

        self.critic = Critic(self.state_dim, self.action_dim).to(self.device)
        self.critic_target = Critic(self.state_dim, self.action_dim).to(self.device)
        self.critic_target.load_state_dict(self.critic.state_dict())
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.critic_lr)

        # 经验回放缓冲区
        self.buffer = ReplayBuffer(config.buffer_size)

        # 训练指标
        self.writer = SummaryWriter(config.log_dir)
        self.episode_rewards = []
        self.total_steps = 0

        # 随机种子
        self._set_seed()

    def _set_seed(self):
        torch.manual_seed(self.config.seed)
        np.random.seed(self.config.seed)
        random.seed(self.config.seed)
        #self.env.seed(self.config.seed)

    def select_action(self, state, add_noise=True):
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        state = state / torch.tensor([[1.0,1.0,8.0]]).to(self.device)
        action = self.actor(state).cpu().data.numpy().flatten()
        if add_noise:
            noise = np.random.normal(0, self.config.exploration_noise, size=self.action_dim)
            action = (action + noise).clip(-self.max_action, self.max_action)
        return action

    def train_step(self):
        if len(self.buffer) < self.config.min_buffer_size:
            return

        # 从缓冲区采样
        states, actions, rewards, next_states, dones = self.buffer.sample(self.config.batch_size)
        states = states.to(self.device)
        states = states / torch.tensor([[1.0, 1.0, 8.0]]).to(self.device) #归一化，避免不同维度的特征差距过大。除数应该会自动做广播
        actions = actions.to(self.device)
        rewards = rewards.to(self.device)
        next_states = next_states.to(self.device)
        next_states = next_states / torch.tensor([[1.0, 1.0, 8.0]]).to(self.device)  # 归一化，避免不同维度的特征差距过大。除数应该会自动做广播
        dones = dones.to(self.device)

        # 更新Critic
        with torch.no_grad():
            next_actions = self.actor_target(next_states)
            target_q = self.critic_target(next_states, next_actions)
            target_q = rewards + (1 - dones) * self.config.gamma * target_q

        current_q = self.critic(states, actions)
        critic_loss = nn.MSELoss()(current_q, target_q)

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # 更新Actor
        actor_loss = -self.critic(states, self.actor(states)).mean()

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # 软更新目标网络
        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):
            target_param.data.copy_(self.config.tau * param.data + (1 - self.config.tau) * target_param.data)
        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):
            target_param.data.copy_(self.config.tau * param.data + (1 - self.config.tau) * target_param.data)

        # 记录训练指标
        self.writer.add_scalar("Loss/Critic", critic_loss.item(), self.total_steps)
        self.writer.add_scalar("Loss/Actor", actor_loss.item(), self.total_steps)
        self.writer.add_scalar("Q_value", current_q.mean().item(), self.total_steps)

    def save_checkpoint(self, episode, reward):
        if not os.path.exists(self.config.checkpoint_dir):
            os.makedirs(self.config.checkpoint_dir)
        checkpoint_path = os.path.join(self.config.checkpoint_dir, f"ddpg_episode_{episode}_{int(reward)}.pth")
        torch.save({
            "episode": episode,
            "actor_state_dict": self.actor.state_dict(),
            "critic_state_dict": self.critic.state_dict(),
            "actor_target_state_dict": self.actor_target.state_dict(),
            "critic_target_state_dict": self.critic_target.state_dict(),
            "actor_optimizer_state_dict": self.actor_optimizer.state_dict(),
            "critic_optimizer_state_dict": self.critic_optimizer.state_dict(),
            #"buffer": self.buffer,
            "episode_rewards": self.episode_rewards,
            "total_steps": self.total_steps,
        }, checkpoint_path)
        print(f"Checkpoint saved at episode {episode}")

    def load_checkpoint(self, checkpoint_path):
        checkpoint = torch.load(checkpoint_path)
        self.actor.load_state_dict(checkpoint["actor_state_dict"])
        self.critic.load_state_dict(checkpoint["critic_state_dict"])
        self.actor_target.load_state_dict(checkpoint["actor_target_state_dict"])
        self.critic_target.load_state_dict(checkpoint["critic_target_state_dict"])
        self.actor_optimizer.load_state_dict(checkpoint["actor_optimizer_state_dict"])
        self.critic_optimizer.load_state_dict(checkpoint["critic_optimizer_state_dict"])
        #self.buffer = checkpoint["buffer"]
        self.episode_rewards = checkpoint["episode_rewards"]
        self.total_steps = checkpoint["total_steps"]
        print(f"Checkpoint loaded from {checkpoint_path}, resuming from episode {checkpoint['episode']}")
        return checkpoint["episode"] + 1

    def train(self, resume_checkpoint=None):
        if resume_checkpoint:
            start_episode = self.load_checkpoint(resume_checkpoint)
        else:
            start_episode = 0

        for episode in range(start_episode, self.config.max_episodes):
            state = self.env.reset()[0]
            episode_reward = 0

            for step in range(self.config.max_steps):
                action = self.select_action(state)
                next_state, reward, done, _, _ = self.env.step(action)
                self.buffer.add(state, action, reward, next_state, done)
                state = next_state
                episode_reward += reward
                self.total_steps += 1

                # 训练一步
                self.train_step()

                if done:
                    break

            self.episode_rewards.append(episode_reward)
            self.writer.add_scalar("Reward/Episode", episode_reward, episode)
            print(f"Episode: {episode}, Reward: {episode_reward:.2f}, Steps: {step + 1}")

            # 定期保存模型
            if (episode + 1) % self.config.save_interval == 0 or episode_reward > -1.0:
                self.save_checkpoint(episode + 1, episode_reward)

        self.env.close()
        self.writer.close()

    def showcase(self, checkpoint_path=None):
        """加载训练好的策略网络并展示10回合游戏"""
        if checkpoint_path:
            self.load_checkpoint(checkpoint_path)

        for episode in range(10):
            state = self.env.reset()[0]
            episode_reward = 0
            done = False

            for step in range(self.config.max_steps):
                action = self.select_action(state, add_noise=False)  # 关闭探索噪声
                #print(f"action#{step}:{action}")
                next_state, reward, done, _, _ = self.env.step(action)
                episode_reward += reward
                state = next_state
                self.env.render()  # 显示游戏画面
                if done:
                    break

            print(f"Showcase Episode {episode + 1}, Reward: {episode_reward:.2f}")

        self.env.close()


if __name__ == "__main__":
    config = Config()
    ddpg = DDPG(config)

    # 如果要恢复训练，可以指定checkpoint路径
    # ddpg.train(resume_checkpoint="./checkpoints/ddpg_episode_100.pth")
    #ddpg.train()

    ddpg.showcase("./checkpoints/ddpg_episode_97_0.pth")
    #ddpg.showcase("./checkpoints/ddpg_episode_100.pth")
```

##### 14.1.3 用于离散动作空间的环境？

想起了我前面4.2节到2025/4/7还不能收敛的问题，似乎Actor-Critic方法里面的Critic一定要是V函数不能是Q函数。但DDPG里面，critic是Q函数，让人不禁想要照葫芦画瓢的解决4.2不能收敛的问题。然后我把DDPG的代码改叭改叭，用于离散动作的CartPole游戏。

很苦恼，搞了很久，没有收敛。太玄学了。后来在AI的指导下修改为下面的代码，一个回合的reward最多只能到200多一点，不能像前面一些算法的几十万分。

AI对此的评价是：请注意，这种"离散化DDPG"的方法本质上是在重新发明轮子，标准的策略梯度方法（如PPO）或DQN变体在离散动作空间会更合适。

```python
import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import gym
from collections import deque, namedtuple
from torch.distributions import Categorical


# 超参数配置
class Config:
    def __init__(self):
        self.env_name = "CartPole-v1"  # 环境名称
        self.seed = 42  # 随机种子
        self.batch_size = 256  # 训练批次大小
        self.gamma = 0.95  # 折扣因子
        self.tau = 0.005  # 目标网络软更新系数
        self.actor_lr = 1e-4  # Actor学习率
        self.critic_lr = 1e-4  # Critic学习率
        self.buffer_size = 100000  # 经验回放缓冲区大小
        self.min_buffer_size = 1000  # 开始训练前的最小数据量
        self.max_episodes = 10000  # 最大训练回合数
        self.max_steps = 200000  # 每回合最大步数
        self.exploration_noise = 0.1  # 动作探索噪声
        self.save_interval = 100  # 保存模型的间隔（回合数）
        self.checkpoint_dir = "./checkpoints"  # 模型保存路径
        self.log_dir = "./logs"  # TensorBoard日志路径


# 经验回放缓冲区
class ReplayBuffer:
    def __init__(self, buffer_size):
        self.buffer = deque(maxlen=buffer_size)
        self.experience = namedtuple("Experience", field_names=["state", "action", "reward", "next_state", "done"])

    def add(self, state, action, reward, next_state, done):
        e = self.experience(state, action, reward, next_state, done)
        self.buffer.append(e)

    def sample(self, batch_size):
        experiences = random.sample(self.buffer, batch_size)
        states = torch.FloatTensor(np.array([e.state for e in experiences]))
        actions = torch.FloatTensor(np.array([e.action for e in experiences]))
        rewards = torch.FloatTensor(np.array([e.reward for e in experiences])).unsqueeze(1)
        next_states = torch.FloatTensor(np.array([e.next_state for e in experiences]))
        dones = torch.FloatTensor(np.array([e.done for e in experiences])).unsqueeze(1)
        return states, actions, rewards, next_states, dones

    def __len__(self):
        return len(self.buffer)


# Actor网络（策略网络）
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return torch.softmax(self.fc3(x), dim=1)


# Critic网络（Q值网络）
class Critic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        # 对每个离散动作创建一个独立的Q值流
        self.fc1 = nn.Linear(state_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, action_dim)  # 输出每个动作的Q值

    def forward(self, x, a=None):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        if a is not None:
            return x.gather(1, a.long().unsqueeze(1))  # 选择特定动作的Q值
        return x


# DDPG算法
class AC:
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # 初始化环境
        self.env = gym.make(config.env_name, render_mode="human")
        self.state_dim = self.env.observation_space.shape[0]
        self.action_dim = self.env.action_space.n


        # 初始化网络
        self.actor = Actor(self.state_dim, self.action_dim).to(self.device)
        self.actor_target = Actor(self.state_dim, self.action_dim).to(self.device)
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.actor_lr)

        self.critic = Critic(self.state_dim, self.action_dim).to(self.device)
        self.critic_target = Critic(self.state_dim, self.action_dim).to(self.device)
        self.critic_target.load_state_dict(self.critic.state_dict())
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.critic_lr)

        # 经验回放缓冲区
        self.buffer = ReplayBuffer(config.buffer_size)

        # 训练指标
        self.writer = SummaryWriter(config.log_dir)
        self.episode_rewards = []
        self.total_steps = 0

        # 随机种子
        self._set_seed()

    def _set_seed(self):
        torch.manual_seed(self.config.seed)
        np.random.seed(self.config.seed)
        random.seed(self.config.seed)

    def select_action(self, state, explore=True):
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        probs = self.actor(state).cpu().detach().numpy().flatten()

        if explore:
            action = np.random.choice(len(probs), p=probs)  # 按概率探索
        else:
            action = np.argmax(probs)
        return action


    def train_step(self):
        if len(self.buffer) < self.config.batch_size:
            return

        # 从缓冲区采样
        states, actions, rewards, next_states, dones = self.buffer.sample(self.config.batch_size)
        states = states.to(self.device)
        actions = actions.to(self.device)
        rewards = rewards.to(self.device)
        next_states = next_states.to(self.device)
        dones = dones.to(self.device)

        # Critic更新
        with torch.no_grad():
            next_probs = self.actor_target(next_states)
            next_q = self.critic_target(next_states)
            target_q = rewards + (1 - dones) * self.config.gamma * \
                       (next_probs * next_q).sum(1, keepdim=True)

        current_q = self.critic(states).gather(1, actions.long().unsqueeze(1))
        critic_loss = nn.MSELoss()(current_q, target_q)

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # Actor更新（使用可导的probability-based更新）
        probs = self.actor(states)
        q_values = self.critic(states)
        actor_loss = -(probs * q_values).sum(1).mean()  # 可导的更新

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # 软更新目标网络，#不是完全用actor网络更新，只用一点点比例
        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):
            target_param.data.copy_(self.config.tau * param.data + (1 - self.config.tau) * target_param.data)
        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):
            target_param.data.copy_(self.config.tau * param.data + (1 - self.config.tau) * target_param.data)

        # 记录训练指标
        self.writer.add_scalar("Loss/Critic", critic_loss.item(), self.total_steps)
        self.writer.add_scalar("Loss/Actor", actor_loss.item(), self.total_steps)
        self.writer.add_scalar("Perf/Q_value", current_q.mean().item(), self.total_steps)

    def save_checkpoint(self, episode, reward):
        if not os.path.exists(self.config.checkpoint_dir):
            os.makedirs(self.config.checkpoint_dir)
        checkpoint_path = os.path.join(self.config.checkpoint_dir, f"ac_episode_{episode}_{int(reward)}.pth")
        torch.save({
            "episode": episode,
            "actor_state_dict": self.actor.state_dict(),
            "critic_state_dict": self.critic.state_dict(),
            "actor_target_state_dict": self.actor_target.state_dict(),
            "critic_target_state_dict": self.critic_target.state_dict(),
            "actor_optimizer_state_dict": self.actor_optimizer.state_dict(),
            "critic_optimizer_state_dict": self.critic_optimizer.state_dict(),
            #"buffer": self.buffer,
            "episode_rewards": self.episode_rewards,
            "total_steps": self.total_steps,
        }, checkpoint_path)
        print(f"Checkpoint saved at episode {episode}")

    def load_checkpoint(self, checkpoint_path):
        checkpoint = torch.load(checkpoint_path)
        self.actor.load_state_dict(checkpoint["actor_state_dict"])
        self.critic.load_state_dict(checkpoint["critic_state_dict"])
        self.actor_target.load_state_dict(checkpoint["actor_target_state_dict"])
        self.critic_target.load_state_dict(checkpoint["critic_target_state_dict"])
        self.actor_optimizer.load_state_dict(checkpoint["actor_optimizer_state_dict"])
        self.critic_optimizer.load_state_dict(checkpoint["critic_optimizer_state_dict"])
        #self.buffer = checkpoint["buffer"]
        self.episode_rewards = checkpoint["episode_rewards"]
        self.total_steps = checkpoint["total_steps"]
        print(f"Checkpoint loaded from {checkpoint_path}, resuming from episode {checkpoint['episode']}")
        return checkpoint["episode"] + 1

    def train(self, resume_checkpoint=None):
        if resume_checkpoint:
            start_episode = self.load_checkpoint(resume_checkpoint)
        else:
            start_episode = 0

        for episode in range(start_episode, self.config.max_episodes):
            state = self.env.reset()[0]
            episode_reward = 0

            for step in range(self.config.max_steps):
                action = self.select_action(state)
                next_state, reward, done, _, _ = self.env.step(action)
                self.buffer.add(state, action, reward, next_state, done)
                state = next_state
                episode_reward += reward
                self.total_steps += 1

                # 训练一步
                self.train_step()

                if done:
                    break

            self.episode_rewards.append(episode_reward)
            self.writer.add_scalar("Perf/Reward", episode_reward, episode)
            print(f"Episode: {episode}, Reward: {episode_reward:.2f}, Steps: {step + 1}")

            # 定期保存模型
            if (episode + 1) % self.config.save_interval == 0 or episode_reward > 1000:
                self.save_checkpoint(episode + 1, episode_reward)

        self.env.close()
        self.writer.close()

    def showcase(self, checkpoint_path=None):
        """加载训练好的策略网络并展示10回合游戏"""
        if checkpoint_path:
            self.load_checkpoint(checkpoint_path)

        for episode in range(10):
            state = self.env.reset()[0]
            episode_reward = 0
            done = False

            for step in range(self.config.max_steps):
                action = self.select_action(state, add_noise=False)  # 关闭探索噪声
                #print(f"action#{step}:{action}")
                next_state, reward, done, _, _ = self.env.step(action)
                episode_reward += reward
                state = next_state
                self.env.render()  # 显示游戏画面
                if done:
                    break

            print(f"Showcase Episode {episode + 1}, Reward: {episode_reward:.2f}")

        self.env.close()


if __name__ == "__main__":
    config = Config()
    ac = AC(config)

    # 如果要恢复训练，可以指定checkpoint路径
    # ac.train(resume_checkpoint="./checkpoints/ac_episode_100.pth")
    ac.train()

    ac.showcase("./checkpoints/ac_episode_97_0.pth")
    
```

每个回合的reward走势，横轴是回合：

![image-20250407182942211](img/RL/image-20250407182942211.png)