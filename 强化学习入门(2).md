## 强化学习入门（2）

由于文件太大会导致加载缓慢、编辑的时候卡顿，所以另外开一个文件，不在原来的强化学习入门.md文件上继续追加内容。

### 1、 高维像素输入

#### 1.1 要考虑的问题

**问题1：延迟奖励带来的虚假奖励问题**

以gym里的breakout（小球敲砖块）的小游戏来理解：

先不考虑跳帧，如下场景： 

1. agent逐帧对环境进行观察，做出动作选择，每一帧都对环境施加了动作，也就是每一帧都会调用env.step(action)。虽然很多时候的action移动托盘，但不会接触到球，没有真正影响到环境，当然这样的移动对于下一次托举球做准备是有意义的。 
2. 在t-5帧的时候，托盘很好的托住了球，球反弹飞向砖块，而达到砖块还需要时间 
3. 此后的8帧时间内，agent继续保持对环境的每一帧都施加一个动作，也就是调用step(action)，这段时间内托盘都与球不发生接触，还在飞行，step返回0作为reward。reward和action会保存在同一个元组tuple里 
4. 在t时刻也就是t帧的时候，agent又对环境执行了step(action)，同时环境里发生了球击中砖块的事件，所以这一次的step（）会返回1 作为reward。那么这个等于1的reward会记录在与这次action相同的tuple元组里，表示这个时候对环境施加了action同时发生了球击中砖块事件，虽然这个事情因果关系并不强烈（因为t-5时刻以后的action对于这次击中砖块是没有帮助的）
5. DQN等算法在利用上述过程中收集的样本，会回溯，以折扣回报的方式奖励 t-5时刻的动作。**但算法也会错误的奖励 t-3, t-4等时刻的action。**
6. replay buffer里随机打散和随机抽样样本的方式进行训练，会不会导致 t 时刻的值为1的奖励不能很好的回馈 t-5时刻的托举动作？ 不会，因为通过TD误差的传播，在某个批次，reward 1 会影响Q(t)值，在某个批次，Q(t)值会影响Q(t-1)，依次类推， reward 1会影响到 Q(t-5, 托举)
7. 但不得不说，**这样的错误归因和奖励，会导致训练的agent收敛慢很多，甚至不能收敛。**

我觉得解决方案可能有：

1. 需要修改奖励机制，例如托举到球，要奖励；击中砖块，要回溯奖励最近一次托举和上一次托举之间的动作
2. 层级式强化学习？（HRL）



**问题2：逐帧处理有什么问题？跳帧处理又有什么问题？**

逐帧处理的问题：

1. 逐帧处理的话，要求Agent的处理速度必须非常高，以60帧每秒的游戏刷新率为例，那就要求agent的策略网络/价值网络的计算和动作施加都要控制在1/60秒内
2. 很多动作是冗余的微小的，紧接着的帧冗余，没有意义，加大了运算负担。
3. 从模拟人玩游戏的角度来看，逐帧处理没有太多必要，决策不需要这么高频的去观察和采取动作
4. 相同的堆叠数下，不容易发现运动趋势，因为帧与帧之间的差异不够大

跳帧的设计，我理解有下面的问题： 

1. 可能错过关键时刻，例如小球飞过去了没有能够托住
2.  动作重复的方式，如果动作是增量的相对的，那么动作可能过量 
3. 可能错失奖励，例如某一帧小球击中了砖块，但是这一帧被跳过了



CleanRL开源项目的代码中breakout游戏的处理，是粗暴的跳过4帧、动作重复执行4次，返回最后两次的观测的最大值和4次reward的累加。

```python
    def step(self, action: int) -> AtariStepReturn:
        """
        Step the environment with the given action
        Repeat action, sum reward, and max over last observations.

        :param action: the action
        :return: observation, reward, terminated, truncated, information
        """
        total_reward = 0.0
        terminated = truncated = False
        for i in range(self._skip):
            obs, reward, terminated, truncated, info = self.env.step(action)
            done = terminated or truncated
            if i == self._skip - 2:
                self._obs_buffer[0] = obs
            if i == self._skip - 1:
                self._obs_buffer[1] = obs
            total_reward += float(reward)
            if done:
                break
        # Note that the observation on the done=True frame
        # doesn't matter
        max_frame = self._obs_buffer.max(axis=0)

        return max_frame, total_reward, terminated, truncated, info
```

![image-20250430151447951](img/RL/image-20250430151447951.png)

CleanRL中对环境的处理有很多有用的组件，用于包装环境：

![image-20250430151902087](img/RL/image-20250430151902087.png)

#### 1.2 实操1：马里奥跳过毒乌龟 / 马里奥跳吃金币

先从简单的场景入手，有一只毒乌龟左右移动，头顶悬着一些金币。马里奥可以跳起来得到金币，遇到乌龟要跳起来躲避他，否则就会受伤。

#### 1.3 实操2：breakout游戏



### 2 机器人手臂的仿真模拟环境

![image-20250501200451197](img/RL/image-20250501200451197.png)

### 3 高并发环境的并行仿真

并行化环境交互是强化学习**提速**、**稳健**与**可扩展**的基础手段：

1. 提升样本吞吐，缩短调参/收敛时间。尤其对于交互时延很大/计算量很大的环境
2. 打散数据相关性，稳定梯度估计
3. 更好地利用 CPU/GPU 资源，例如可以异步的同时进行样本收集（侧重CPU+网络）和网络更新（侧重GPU）
4. 支撑异步、大规模分布式算法。例如当下很火的LLM，每次交互时延/计算量都很大，为了高效收集样本数据，必须进行分布式部署



```
# stable_baseline3
https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html

# ray rllib
https://docs.ray.io/en/latest/cluster/getting-started.html

# envpool
https://envpool.readthedocs.io/en/latest/
```

#### 3.1 开发库 Stable-Baselines3

[SB3](https://stable-baselines3.readthedocs.io/en/master/index.html)是一个基于 PyTorch 的强化学习（RL）库，提供可靠且易于使用的 RL 算法实现，特性有：

1. 实现了多种常用的强化学习算法，所有算法遵循统一的接口和代码结构，便于快速上手和切换不同算法
2. 通过向量化环境（VecEnv）支持并行训练，加速实验过程
3. 提供详细的用户指南、API 参考和教程；充分测试（95%的覆盖率）
4. RL Baselines3 Zoo提供预训练的代理、超参数调优脚本、评估工具和结果可视化
5. 支持用户定义自己的特征提取器和网络结构，满足特定需求；提供灵活的回调机制，便于在训练过程中插入自定义逻辑，如早停、模型保存等
6. atari环境的wrappers，例如ClipRewardEnv / EpisodicLifeEnv / FireResetEnv等， 还有各种工具类，让开发变得简单方便。



思考：

stable-baseline3提供了对很多RL算法的封装，对于典型的gym 游戏的训练只需要3-5行代码，极其方便：

1. 从知其所以然的角度来看，封装的太好不利于我这样的初学者洞悉背后的细节，很多时候我希望手搓代码
2. 但是从有一个好的开始而提振信心（尤其是对于高维像素输入方式下的游戏agent训练）来说，有一个先成功跑起来的代码是有较大价值的。下面的3.4就让人很激动
3. 另外，研究中用作实验对比的基准，需要一个可靠的RL算法x环境的实现。毕竟RL很玄学，有时候即使训练出来一个可以work的agent，也拿不准是不是充分收敛的agent



**具体到SB3中提供的VecEnv来实现并行多环境交互：**

VecEnv有两个具体实现：DummyVecEnv和SubprocVecEnv，前者实际上是串行的在多个env上执行交互，后者才是真正的多进程并发。

使用make_vec_env()函数可以创建VecEnv并明确指定具体的实现，也可以直接调用他们的构造函数。

```python
vec_env = make_vec_env(env_id='CartPole-v1', n_envs=4, vec_env_cls=SubprocVecEnv)
```

VecEnv的使用注意事项：

![image-20250516123835826](img/RL/image-20250516123835826.png)

根据上面的文档，**回合终止的处理需要额外注意**：

1. 当一个 episode 正常终止（terminated），我们就知道它的最终结果，比如 agent 掉落悬崖，得了 -100 分 —— 这是明确的，我们就不需要估计接下来的价值（因为 episode 已经结束，没有下一步）。但如果 episode 是因为 **时间限制（timeout）被截断（truncated）**，那其实 agent 可能还没死、还可以继续获得 reward，只是我们强制停止了。在这种情况下：
   1. 我们**不能简单地认为 episode 到此就结束了**；
   2. 所以我们**应当用当前状态的值函数来估算未来**，这就是 bootstrap。
2. 另外，当一个回合终止后，由于并行环境会自动reset，那么step函数返回的state其实是reset后新的回合的开始，并非上一个回合真的最后一个state，所以要通过infos字段来获得真正的最后一个state，放入到replaybuffer里。



#### 3.2 实操一：用VecEnv+手搓DQN实现并行交互

手搓代码总是让人更好的理解背后的机制，下面的代码能够取得不错的训练效果：

![image-20250516121322765](img/RL/image-20250516121322765.png)

```
Inference finished. Total reward: 369.0
```

代码如下：

```python
import random
import gym
import numpy
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from collections import deque
import argparse
import os
from stable_baselines3.common.vec_env import SubprocVecEnv
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime as dt

# 设备选择
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
writer = SummaryWriter(f"logs/parrallel_{dt.now().strftime('%y%m%d_%H%M%S')}")

# 超参数
gamma = 0.99  # 折扣因子
epsilon = 1.0  # 初始探索率
epsilon_min = 0.01  # 最低探索率
epsilon_decay = 0.95  # 探索率衰减
learning_rate = 1e-3  # 学习率
batch_size = 64  # 经验回放的批量大小
memory_size = 100_000  # 经验池大小
target_update_freq = 10  # 目标网络更新频率
n_envs = 4

# 创建一个返回 CartPole 环境的函数（每个子环境都需要一个独立实例）
def make_env(seed, index):
    def _init():
        env = gym.make("CartPole-v1", max_episode_steps=None) #max_episode_steps这样修改没有用，内部还是500步最大，需要继续研究
        return env
    return _init

single_env = gym.make("CartPole-v1", render_mode="human")
n_state = single_env.observation_space.shape[0]  # 状态维度
n_action = single_env.action_space.n  # 动作数量

# DQN 网络定义
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)


# 初始化网络
model = DQN(n_state, n_action).to(device)
target_model = DQN(n_state, n_action).to(device)
target_model.load_state_dict(model.state_dict())
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
memory = deque(maxlen=memory_size)


def select_action(states:numpy.ndarray, epsilon):

    """基于 ε-greedy 选择动作"""
    if random.random() < epsilon:
        actions = torch.zeros(states.shape[0], dtype=torch.long).to(device)
        for i in range(states.shape[0]):
            actions[i] = random.randint(0, n_action - 1)
        return   actions.cpu().numpy()
    else:
        states = torch.tensor(states).to(device)
        if states.shape.__len__() < 2:
            states = states.unsqueeze(0)
        return model(states).argmax(1).cpu().numpy()  # 选取 Q 值最大的动作


def train():
    if len(memory) < batch_size*10:
        return 99.0  # 经验池数据不足时不训练

    batch = random.sample(memory, batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)

    states = torch.FloatTensor(states).to(device)  # (batch_size, 4)
    actions = torch.LongTensor(actions).unsqueeze(1).to(device)  # (batch_size,) -> (batch_size, 1)
    rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)  # (batch_size,) -> (batch_size, 1)
    next_states = torch.FloatTensor(next_states).to(device)  # (batch_size, 4)
    dones = torch.FloatTensor(dones).unsqueeze(1).to(device)  # (batch_size,) -> (batch_size, 1)

    # 计算当前 Q 值
    q_values = model(states).gather(1, actions)  # 从 Q(s, a) 选取执行的动作 Q 值

    # 计算目标 Q 值
    next_q_values = target_model(next_states).max(1, keepdim=True)[0]  # 选取 Q(s', a') 的最大值
    target_q_values = rewards + gamma * next_q_values * (1 - dones)  # TD 目标

    # 计算损失
    loss = F.mse_loss(q_values, target_q_values.detach())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()


def save_checkpoint(id):
    path=f"checkpoints/dqn_checkpoint_{id}.pth"
    torch.save(model.state_dict(), path)
    print(f"Checkpoint saved to {path}")


def load_checkpoint(path):
    if os.path.exists(path):
        model.load_state_dict(torch.load(path, map_location=device))
        print(f"Checkpoint loaded from {path}")
    else:
        print("No checkpoint found, starting from scratch.")


def main(mode, env):
    global epsilon

    if mode == "train":
        epoches = 500
        states = env.reset()
        episode_len = 0 #跟踪0号环境的回合长度
        episode_cnt = 0 # 跟踪0号环境的回合个数
        step_cnt = 0 #总的交互次数，没有乘以环境并发数
        max_episode_len = -1 #跟踪0号环境最大的回合长度
        for epoch in range(epoches):
            # 本来是可以一直执行下去，因为并行环境会自动reset，但这里做一个控制：
            # 每300步要跳出来做一点其他事情，例如更新epsilon。
            # 300这个参数很关键的，直接影响epsilon的节奏
            for _ in range(300):
                actions = select_action(states, epsilon)
                next_states, rewards, dones, infos = env.step(actions)
                step_cnt += 1
                if dones[0]:
                    # 处理episode_len数据
                    episode_cnt += 1
                    writer.add_scalar("a_train/episode_len", episode_len, episode_cnt) #加上a_让这个排在前面
                    if episode_len >= max_episode_len: #等于也保存，觉得后面训练时间更长，可能效果更好
                        max_episode_len = episode_len
                        save_checkpoint(max_episode_len)
                        if  episode_len == 499:
                            print("reach 499")
                    episode_len = 0
                else:
                    episode_len += 1

                # 经验回放缓存
                for env_idx in range(n_envs):
                    if not dones[env_idx]:
                        memory.append((states[env_idx], actions[env_idx], rewards[env_idx], next_states[env_idx], dones[env_idx]))
                        continue
                    if infos[env_idx]["TimeLimit.truncated"]: #被截断了，不是真的结束，那么这个时间步的done flag应该为false
                        memory.append( (states[env_idx], actions[env_idx], rewards[env_idx], next_states[env_idx], False))
                    else: # 是真的一个回合结束了，由于环境自动reset，所以next_states并不是真的随后一个状态，而是reset后下一回合的新状态
                        next_stt = infos[env_idx]["terminal_observation"]
                        memory.append( (states[env_idx], actions[env_idx], rewards[env_idx], next_stt, dones[env_idx]))

                states = next_states


                # 训练 DQN
                loss = train()
                if step_cnt % 10 == 0:
                    writer.add_scalar("loss/loss", loss, step_cnt)

            # 逐步降低 epsilon，减少随机探索，提高利用率
            epsilon = max(epsilon_min, epsilon * epsilon_decay)

            # 定期更新目标网络，提高稳定性
            if epoch % target_update_freq == 0:
                target_model.load_state_dict(model.state_dict())



            writer.add_scalar("epoch/epsilon", epsilon, epoch)
            print(f"Finished epoch#{epoch}")

    elif mode == "infer":
        load_checkpoint("./checkpoints/dqn_checkpoint_499.pth")
        state = single_env.reset()
        state = state[0]
        total_reward = 0

        while True:
            single_env.render()
            action = select_action(state, 0)  # 纯利用，epsilon=0
            state, reward, done, _, _ = single_env.step(action[0])
            total_reward += reward

            if done:
                break

        print(f"Inference finished. Total reward: {total_reward}")


if __name__ == "__main__":
    # 使用 SubprocVecEnv 实现多进程并行环境（注意：在 Windows 中建议使用 if __name__ == "__main__"）
    env_fns = [make_env(seed=i, index=i) for i in range(n_envs)]
    envs = SubprocVecEnv(env_fns)
    main("train", envs)
```

上面的SubprocVecEnv的创建方式默认有限制一个回合的最大步数为500，也可以通过下面的代码去掉：

```python
import gym
from gym.wrappers import TimeLimit
from stable_baselines3.common.env_util import make_vec_env

def make_unlimited_cartpole():
    env = gym.make("CartPole-v1")
    # 如果环境被 TimeLimit 包装，移除它
    if isinstance(env, TimeLimit):
        env = env.env  # unwrap TimeLimit
    return env

# 使用 make_vec_env 构造并行环境
envs = make_vec_env(make_unlimited_cartpole, n_envs=4)

```

这样果然会出现回合长度超过10000的回合。

#### 3.3 实操二：直接用SB3训练DQN（CartPole）

感觉上面3.2的自己手搓的代码还不够好，因为训练的回合长度没有一路单调增长，中间波动很大，但又不知道问题出在哪里。

所以直接用SB3这样的专家代码来训练DQN，发现回合平均长度也波动很大。

下面代码直接使用SB3库解决CartPole任务，可以看到回合平均长度波动很大且后面都变得很糟糕。但通过及时的保存best model，得到的模型在评估中表现很好。

![image-20250517154356656](img/RL/image-20250517154356656.png)

评估时候输出的回合长度：

```
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
```

代码：

```python
from stable_baselines3 import DQN
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback


eval_env = make_vec_env("CartPole-v1", n_envs=1)
# 创建评估回调
eval_callback = EvalCallback(
    eval_env,
    best_model_save_path="./best_model/",
    log_path="./logs/eval/",
    eval_freq=50_000,                  # 每隔多少步评估一次
    n_eval_episodes=5,                 # 每次评估使用多少个 episode
    deterministic=True,
    render=False,
    verbose=1                          # 日志显示
)

if __name__ == "__main__":
    #下面函数默认是创建DummyVecEnv，不是真的并行，除非明确指定vec_env_cls=SubprocVecEnv，但对于cartpole这样简单的
    # 环境，后者反而更慢，因为增加了额外的进程间通信开销
    vec_env = make_vec_env(env_id='CartPole-v1', n_envs=4,vec_env_cls=SubprocVecEnv)
    model = DQN("MlpPolicy", vec_env,
                    verbose=1,
                    tensorboard_log="logs") #这里可以写很多参数，包括replaybuffer大小、衰减率、学习率等等
    model.learn(total_timesteps=5_000_000, callback=eval_callback)
    del model

    model = DQN.load("./best_model/best_model.zip")

    obs = vec_env.reset()
    for i in range(10):
        done = False
        episode_len = 0
        while not done:
            action, _states = model.predict(obs, deterministic=False)
            obs, rewards, dones, info = vec_env.step(action)
            done = dones[0]
            episode_len += 1
            vec_env.render("human")
        print(f"episode len:{episode_len}")

```



#### 3.4 实操三：用SB3库搞定像素输入（Breakout）

效果很好，可以说是惊艳，可以上分100多，见[视频](img/RL/SB3_breakout.mp4)：

```
一开始：
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 703      |
|    ep_rew_mean      | 1.13     |
|    exploration_rate | 0.923    |
| time/               |          |
|    episodes         | 1000     |
|    fps              | 475      |
|    time_elapsed     | 68       |
|    total_timesteps  | 32512    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00183  |
|    n_updates        | 1406     |
----------------------------------
经过4个小时后如下:
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 5.22e+03 |
|    ep_rew_mean      | 69.9     |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 27000    |
|    fps              | 295      |
|    time_elapsed     | 13172    |
|    total_timesteps  | 3898952  |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00214  |
|    n_updates        | 243059   |
----------------------------------
```



代码如下（来自SB3文档）：

```python
import gymnasium as gym
from stable_baselines3 import DQN
from stable_baselines3.common.atari_wrappers import AtariWrapper
from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv
import ale_py

# There already exists an environment generator
# that will make and wrap atari environments correctly.
# Here we are also multi-worker training (n_envs=4 => 4 environments)
vec_env = make_atari_env("BreakoutNoFrameskip-v4", n_envs=4, seed=0)
# Frame-stacking with 4 frames
vec_env = VecFrameStack(vec_env, n_stack=4)

# 使用 CNN 网络进行训练
model = DQN("CnnPolicy", vec_env, verbose=1, buffer_size=10_000, learning_rate=1e-4, batch_size=32,
            learning_starts=10_000, target_update_interval=1_000, train_freq=4)

# 训练 100k steps（可以增加）
model.learn(total_timesteps=4_000_000, log_interval=1_000)

# 保存模型
model.save("dqn_breakout")
del model

model = DQN.load("dqn_breakout")

obs = vec_env.reset()
while True:
    action, _states = model.predict(obs, deterministic=False)
    obs, rewards, dones, info = vec_env.step(action)
    vec_env.render("human")

```

#### 3.5 开发库 EnvPool

Here are EnvPool’s several highlights:

- Compatible with OpenAI `gym` APIs and DeepMind `dm_env` APIs;
- Manage a pool of envs, interact with the envs in batched APIs by default;
- Support both synchronous execution and asynchronous execution;
- Support both single player and multi-player environment;
- Easy C++ developer API to add new envs: [Customized C++ environment integration](https://envpool.readthedocs.io/en/latest/content/new_env.html);
- High performance
  - Free **~2x** speedup with only single environment;
  - **1 Million** Atari frames / **3 Million** Mujoco steps per second simulation with 256 CPU cores, **~20x** throughput of Python subprocess-based vector env;
  - **~3x** throughput of Python subprocess-based vector env on low resource setup like 12 CPU cores;
- Compatible and general
  - XLA support with JAX jit function;
  - Comparing with the existing GPU-based solution ([Brax](https://github.com/google/brax) / [Isaac-gym](https://developer.nvidia.com/isaac-gym)), EnvPool is a **general** solution for various kinds of speeding-up RL environment parallelization;
  - Compatible with some existing RL libraries, e.g., [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3), [Tianshou](https://github.com/thu-ml/tianshou), [ACME](https://github.com/deepmind/acme), [CleanRL](https://github.com/vwxyzjn/cleanrl) ([Solving Pong in 5 mins](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#solving-pong-in-5-minutes-with-ppo--envpool)), [rl_games](https://github.com/Denys88/rl_games) (2 mins [Pong](https://colab.research.google.com/drive/1iWFv0g67mWqJONoFKNWUmu3hdxn_qUf8?usp=sharing), 15 mins [Breakout](https://colab.research.google.com/drive/1U_NxL6gSs0yRVhfl0cKl9ttRmcmMCiBS?usp=sharing), 5 mins [Ant](https://colab.research.google.com/drive/1C9yULxU_ahQ_i6NUHCvOLoeSwJovQjdz?usp=sharing) and [HalfCheetah](https://colab.research.google.com/drive/1bser52bpItzmlME00IA0bbmPdp1Xm0fy?usp=sharing)).

比较操蛋：不支持windows，只支持类unix环境。

#### 3.6 用EnvPool+手搓DQN实现并行交互



待调试代码如下，等我有钱了买个云主机跑起来

```python
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime as dt
import os
import envpool  # ✅ 使用 EnvPool 替代 SubprocVecEnv
import gym

# 设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
writer = SummaryWriter(f"logs/envpool_{dt.now().strftime('%y%m%d_%H%M%S')}")

# 超参数
gamma = 0.99
epsilon = 1.0
epsilon_min = 0.01
epsilon_decay = 0.95
learning_rate = 1e-3
batch_size = 64
memory_size = 100_000
target_update_freq = 10
n_envs = 4

# 单个环境用于推理
single_env = gym.make("CartPole-v1", render_mode="human")
n_state = single_env.observation_space.shape[0]
n_action = single_env.action_space.n

# 网络结构
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

# 初始化模型
model = DQN(n_state, n_action).to(device)
target_model = DQN(n_state, n_action).to(device)
target_model.load_state_dict(model.state_dict())
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
memory = deque(maxlen=memory_size)

# ε-greedy 动作选择
def select_action(states: np.ndarray, epsilon):
    if random.random() < epsilon:
        actions = np.random.randint(0, n_action, size=states.shape[0])
        return actions
    else:
        states = torch.tensor(states, dtype=torch.float32).to(device)
        if states.ndim == 1:
            states = states.unsqueeze(0)
        with torch.no_grad():
            return model(states).argmax(1).cpu().numpy()

# 训练过程
def train():
    if len(memory) < batch_size * 10:
        return 99.0

    batch = random.sample(memory, batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)

    states = torch.FloatTensor(states).to(device)
    actions = torch.LongTensor(actions).unsqueeze(1).to(device)
    rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)
    next_states = torch.FloatTensor(next_states).to(device)
    dones = torch.FloatTensor(dones).unsqueeze(1).to(device)

    q_values = model(states).gather(1, actions)
    next_q_values = target_model(next_states).max(1, keepdim=True)[0]
    target_q_values = rewards + gamma * next_q_values * (1 - dones)

    loss = F.mse_loss(q_values, target_q_values.detach())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()

# 保存模型
def save_checkpoint(id):
    os.makedirs("checkpoints", exist_ok=True)
    path = f"checkpoints/dqn_checkpoint_{id}.pth"
    torch.save(model.state_dict(), path)
    print(f"Checkpoint saved to {path}")

# 加载模型
def load_checkpoint(path):
    if os.path.exists(path):
        model.load_state_dict(torch.load(path, map_location=device))
        print(f"Checkpoint loaded from {path}")
    else:
        print("No checkpoint found, starting from scratch.")

# 主函数
def main(mode, env):
    global epsilon

    if mode == "train":
        epoches = 500
        obs = env.reset()
        states = obs["obs"]
        episode_len = 0
        episode_cnt = 0
        step_cnt = 0
        max_episode_len = -1

        for epoch in range(epoches):
            for _ in range(300):
                actions = select_action(states, epsilon)
                next_obs, rewards, dones, truncs, infos = env.step(actions)
                next_states = next_obs["obs"]
                step_cnt += 1

                # 处理0号环境的episode长度
                if dones[0] or truncs[0]:
                    episode_cnt += 1
                    writer.add_scalar("a_train/episode_len", episode_len, episode_cnt)
                    if episode_len >= max_episode_len:
                        max_episode_len = episode_len
                        save_checkpoint(max_episode_len)
                        if episode_len == 499:
                            print("reach 499")
                    episode_len = 0
                else:
                    episode_len += 1

                # 存入经验池
                for env_idx in range(n_envs):
                    if not dones[env_idx] and not truncs[env_idx]:
                        memory.append((states[env_idx], actions[env_idx], rewards[env_idx], next_states[env_idx], False))
                    elif truncs[env_idx] and "terminal_observation" in infos[env_idx]:
                        next_stt = infos[env_idx]["terminal_observation"]
                        memory.append((states[env_idx], actions[env_idx], rewards[env_idx], next_stt, False))
                    else:
                        memory.append((states[env_idx], actions[env_idx], rewards[env_idx], next_states[env_idx], dones[env_idx]))

                states = next_states

                # 训练网络
                loss = train()
                if step_cnt % 10 == 0:
                    writer.add_scalar("loss/loss", loss, step_cnt)

            epsilon = max(epsilon_min, epsilon * epsilon_decay)

            if epoch % target_update_freq == 0:
                target_model.load_state_dict(model.state_dict())

            writer.add_scalar("epoch/epsilon", epsilon, epoch)
            print(f"Finished epoch#{epoch}")

    elif mode == "infer":
        load_checkpoint("./checkpoints/dqn_checkpoint_499.pth")
        state = single_env.reset()[0]
        total_reward = 0

        while True:
            single_env.render()
            action = select_action(state, 0)
            state, reward, done, _, _ = single_env.step(action[0])
            total_reward += reward
            if done:
                break
        print(f"Inference finished. Total reward: {total_reward}")

if __name__ == "__main__":
    # 初始化并行环境
    envs = envpool.make("CartPole-v1", num_envs=n_envs, seed=42)
    main("train", envs)

```

