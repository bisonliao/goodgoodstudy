## 强化学习入门（2）

由于文件太大会导致加载缓慢、编辑的时候卡顿，所以另外开一个文件，不在原来的强化学习入门.md文件上继续追加内容。

### 1、 高维像素输入

#### 1.1 要考虑的问题

**问题1：延迟奖励带来的虚假奖励问题**

以gym里的breakout（小球敲砖块）的小游戏来理解：

先不考虑跳帧，如下场景： 

1. agent逐帧对环境进行观察，做出动作选择，每一帧都对环境施加了动作，也就是每一帧都会调用env.step(action)。虽然很多时候的action移动托盘，但不会接触到球，没有真正影响到环境，当然这样的移动对于下一次托举球做准备是有意义的。 
2. 在t-5帧的时候，托盘很好的托住了球，球反弹飞向砖块，而达到砖块还需要时间 
3. 此后的8帧时间内，agent继续保持对环境的每一帧都施加一个动作，也就是调用step(action)，这段时间内托盘都与球不发生接触，还在飞行，step返回0作为reward。reward和action会保存在同一个元组tuple里 
4. 在t时刻也就是t帧的时候，agent又对环境执行了step(action)，同时环境里发生了球击中砖块的事件，所以这一次的step（）会返回1 作为reward。那么这个等于1的reward会记录在与这次action相同的tuple元组里，表示这个时候对环境施加了action同时发生了球击中砖块事件，虽然这个事情因果关系并不强烈（因为t-5时刻以后的action对于这次击中砖块是没有帮助的）
5. DQN等算法在利用上述过程中收集的样本，会回溯，以折扣回报的方式奖励 t-5时刻的动作。**但算法也会错误的奖励 t-3, t-4等时刻的action。**
6. replay buffer里随机打散和随机抽样样本的方式进行训练，会不会导致 t 时刻的值为1的奖励不能很好的回馈 t-5时刻的托举动作？ 不会，因为通过TD误差的传播，在某个批次，reward 1 会影响Q(t)值，在某个批次，Q(t)值会影响Q(t-1)，依次类推， reward 1会影响到 Q(t-5, 托举)
7. 但不得不说，**这样的错误归因和奖励，会导致训练的agent收敛慢很多，甚至不能收敛。**

我觉得解决方案可能有：

1. 需要修改奖励机制，例如托举到球，要奖励；击中砖块，要回溯奖励最近一次托举和上一次托举之间的动作
2. 层级式强化学习？（HRL）



**问题2：逐帧处理有什么问题？跳帧处理又有什么问题？**

逐帧处理的问题：

1. 逐帧处理的话，要求Agent的处理速度必须非常高，以60帧每秒的游戏刷新率为例，那就要求agent的策略网络/价值网络的计算和动作施加都要控制在1/60秒内
2. 很多动作是冗余的微小的，紧接着的帧冗余，没有意义，加大了运算负担。
3. 从模拟人玩游戏的角度来看，逐帧处理没有太多必要，决策不需要这么高频的去观察和采取动作
4. 相同的堆叠数下，不容易发现运动趋势，因为帧与帧之间的差异不够大

跳帧的设计，我理解有下面的问题： 

1. 可能错过关键时刻，例如小球飞过去了没有能够托住
2.  动作重复的方式，如果动作是增量的相对的，那么动作可能过量 
3. 可能错失奖励，例如某一帧小球击中了砖块，但是这一帧被跳过了



CleanRL开源项目的代码中breakout游戏的处理，是粗暴的跳过4帧、动作重复执行4次，返回最后两次的观测的最大值和4次reward的累加。

```python
    def step(self, action: int) -> AtariStepReturn:
        """
        Step the environment with the given action
        Repeat action, sum reward, and max over last observations.

        :param action: the action
        :return: observation, reward, terminated, truncated, information
        """
        total_reward = 0.0
        terminated = truncated = False
        for i in range(self._skip):
            obs, reward, terminated, truncated, info = self.env.step(action)
            done = terminated or truncated
            if i == self._skip - 2:
                self._obs_buffer[0] = obs
            if i == self._skip - 1:
                self._obs_buffer[1] = obs
            total_reward += float(reward)
            if done:
                break
        # Note that the observation on the done=True frame
        # doesn't matter
        max_frame = self._obs_buffer.max(axis=0)

        return max_frame, total_reward, terminated, truncated, info
```

![image-20250430151447951](img/RL/image-20250430151447951.png)

CleanRL中对环境的处理有很多有用的组件，用于包装环境：

![image-20250430151902087](img/RL/image-20250430151902087.png)

#### 1.2 实操1：breakout游戏

训练效果很好，经过5个小时1x3万个回合后，能够很稳定的弹球，见评估的[效果视频](img/RL/my_breakout.mp4)。

![image-20250520074803950](img/RL/image-20250520074803950.png)

注意，这还不是完全的只依赖像素输入的，因为奖励/回合结束等信息，还是来自模拟器返回的结构化数据。

代码如下：

```python
import os
import random

import numpy
import numpy as np
from collections import deque, namedtuple
from typing import List, Tuple, Optional


import ale_py
import os

from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack


import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime


# 设备配置 (优先使用CUDA)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class CNN_DQN(nn.Module):
    """处理像素输入的深度Q网络"""

    def __init__(self, stack_size: int, action_dim: int):
        super(CNN_DQN, self).__init__()
        self.conv1 = nn.Conv2d(stack_size, 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
        self.fc1 = nn.Linear(64 * 7 * 7, 512)
        self.fc2 = nn.Linear(512, action_dim)

        # 初始化权重
        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    m.bias.data.zero_()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """前向传播"""
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.contiguous().view(x.size(0), -1)  # 展平
        x = F.relu(self.fc1(x))
        return self.fc2(x)


class DQNAgent:
    """DQN智能体，处理像素输入"""

    def __init__(self, env, stack_size: int = 4, buffer_capacity: int = 10_000,
                 batch_size: int = 32, gamma: float = 0.99, lr: float = 1e-4,
                 tau: float = 1.0, initial_epsilon: float = 1.0,
                 final_epsilon: float = 0.01, num_episodes=100_000, n_envs=4):
        self.env = env
        self.n_envs = n_envs
        self.stack_size = stack_size
        self.action_dim = env.action_space.n
        print(f"action_dim:{self.action_dim}")
        self.batch_size = batch_size
        self.gamma = gamma
        self.tau = tau
        self.epsilon = initial_epsilon
        self.final_epsilon = final_epsilon
        self.num_episodes = num_episodes

        # 初始化网络
        self.policy_net = CNN_DQN(stack_size, self.action_dim).to(device)
        self.target_net = CNN_DQN(stack_size, self.action_dim).to(device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()  # 目标网络不训练

        # 优化器
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)

        # 经验回放
        self.memory = deque(maxlen=buffer_capacity)  # 10万个step就是11个GB

        # TensorBoard
        dt = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.writer = SummaryWriter(log_dir=f"logs/raw_dqn_{dt}")
        self.steps_done = 1

    def select_action(self, state: np.ndarray, evaluate: bool = False) -> int:

        """根据ε-greedy策略选择动作"""
        if not evaluate and random.random() < self.epsilon:
            result = numpy.ndarray((self.n_envs,), dtype=numpy.int32)
            for i in range(self.n_envs):
                result[i] = random.randint(0, self.action_dim - 1)
            return result

        # 转换为torch张量并添加batch维度
        state_tensor = torch.FloatTensor(state).to(device)
        assert state_tensor.shape.__len__() == 4

        with torch.no_grad():
            q_values = self.policy_net(state_tensor)

        return q_values.argmax(dim=1).numpy()

    def update_model(self) -> float:
        """更新策略网络"""
        if len(self.memory) < self.batch_size*100:
            return 0.0  # 如果经验不足，不更新

        # 从回放缓冲区采样


        batch = random.sample(self.memory, k=self.batch_size)
        s, a, s_, r, d = zip(*batch)

        # 转换为张量
        state_batch = torch.FloatTensor(np.array(s)).to(device)
        action_batch = torch.LongTensor(a).unsqueeze(1).to(device)
        next_state_batch = torch.FloatTensor(np.array(s_)).to(device)
        reward_batch = torch.FloatTensor(r).to(device)
        done_batch = torch.FloatTensor(d).to(device)

        # 计算当前Q值
        current_q = self.policy_net(state_batch).gather(1, action_batch)
        self.writer.add_scalar(f"steps/current_Q", current_q.mean().item(), self.steps_done)
        # 计算目标Q值
        with torch.no_grad():
            next_q = self.target_net(next_state_batch).max(1)[0].detach()
            target_q = reward_batch + (1 - done_batch) * self.gamma * next_q
        self.writer.add_scalar(f"steps/target_Q", target_q.mean().item(), self.steps_done)

        loss = F.mse_loss(current_q.squeeze(), target_q)

        # 反向传播
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)  # 新增梯度裁剪
        '''if self.steps_done % 10000 == 0:
            # 检查梯度是否存在
            for name, param in self.policy_net.named_parameters():
                if param.grad is None:
                    self.writer.add_scalar(f"grad/{name}", 0, self.steps_done)
                else:
                    self.writer.add_scalar(f"grad/{name}", param.grad.mean().item(), self.steps_done)'''
        self.optimizer.step()

        # 更新目标网络 (软更新)
        if self.steps_done % 1000 == 0:
            for target_param, policy_param in zip(self.target_net.parameters(),
                                                  self.policy_net.parameters()):
                target_param.data.copy_(
                    self.tau * policy_param.data + (1 - self.tau) * target_param.data
                )

        return loss.item()

    def decay_epsilon(self, episode):
        """衰减探索率"""
        stop_pointer = 5000
        decay_rate = (1.0 - self.final_epsilon) / stop_pointer  # 在1-stop pointer，epsilon就从1线性下降到final_epsilon
        if episode  < stop_pointer:
            self.epsilon = 1 - episode * decay_rate
        else:
            self.epsilon = self.final_epsilon

    def save_checkpoint(self, episode: int, total_reward, checkpoint_dir: str = "checkpoints"):
        """保存模型检查点"""
        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)

        checkpoint_path = os.path.join(checkpoint_dir, f"dqn_episode_{episode}_{total_reward}.pth")
        torch.save({
            'episode': episode,
            'policy_state_dict': self.policy_net.state_dict(),
            'target_state_dict': self.target_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'steps_done': self.steps_done
        }, checkpoint_path)

        # print(f"Checkpoint saved to {checkpoint_path}")

    def load_checkpoint(self, checkpoint_path: str):
        """加载模型检查点"""
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"Checkpoint file {checkpoint_path} not found")

        checkpoint = torch.load(checkpoint_path)
        self.policy_net.load_state_dict(checkpoint['policy_state_dict'])
        self.target_net.load_state_dict(checkpoint['target_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint['epsilon']
        self.steps_done = checkpoint['steps_done']

        print(f"Checkpoint loaded from {checkpoint_path}, "
              f"resuming from episode {checkpoint['episode']}")

        return checkpoint['episode']



    def train(self, checkpoint_interval: int = 1000,
              checkpoint_dir: str = "checkpoints", resume_checkpoint: Optional[str] = None):
        """训练循环"""
        start_episode = 0

        # 如果提供了检查点，从中恢复
        if resume_checkpoint is not None:
            start_episode = self.load_checkpoint(resume_checkpoint) + 1


        episode_cnt = 0
        max_episode_len = -1
        while episode_cnt < 10_000:
            states = self.env.reset()
            states = states.transpose(0, 3, 1, 2)
            total_reward = 0
            episode_len = 1e-4
            for _ in range(300):
                actions = self.select_action(states)
                next_states, rewards, dones, infos = self.env.step(actions)
                next_states = next_states.transpose(0, 3, 1, 2)
                self.steps_done += 1
                if dones[0]:
                    # 处理episode_len数据
                    episode_cnt += 1
                    self.writer.add_scalar("episode/episode_len", episode_len, episode_cnt)
                    self.writer.add_scalar('episode/total_reward', total_reward, episode_cnt)
                    if episode_len >= max_episode_len:  # 等于也保存，觉得后面训练时间更长，可能效果更好
                        max_episode_len = episode_len
                        self.save_checkpoint(episode_cnt, episode_len, "./checkpoints")
                    episode_len = 1e-4
                    total_reward = 0
                else:
                    episode_len += 1
                    total_reward += rewards[0]

                # 经验回放缓存
                for env_idx in range(self.n_envs):
                    if dones[env_idx]:
                        if infos[env_idx]["TimeLimit.truncated"]:  # 被截断了，不是真的结束，那么这个时间步的done flag应该为false
                            self.memory.append(
                                (states[env_idx], actions[env_idx], next_states[env_idx], rewards[env_idx], False))
                        else:  # 是真的一个回合结束了，由于环境自动reset，所以next_states并不是真的随后一个状态，而是reset后下一回合的新状态
                            next_stt = infos[env_idx]["terminal_observation"]
                            next_stt = next_stt.transpose(2, 0, 1)
                            self.memory.append( (states[env_idx], actions[env_idx],  next_stt, rewards[env_idx],  dones[env_idx]) )
                    else:
                        self.memory.append(
                            (states[env_idx], actions[env_idx], next_states[env_idx], rewards[env_idx], dones[env_idx]))

                states = next_states

                # 更新模型
                if  self.steps_done % 4 == 0:
                    loss = self.update_model()
                    self.writer.add_scalar('episode/loss', loss, self.steps_done)


            # 衰减探索率
            self.decay_epsilon(episode_cnt)

            # 记录到TensorBoard


            self.writer.add_scalar('episode/epsilon', self.epsilon, episode_cnt)


            # 定期保存检查点
            if (episode_cnt + 1) % checkpoint_interval == 0:
                self.save_checkpoint(episode_cnt, 0, checkpoint_dir)

        self.writer.close()

    def evaluate(self, vec_env, num_episodes: int = 10, checkpoint_path: Optional[str] = None):
        """评估模型性能"""
        # 使用 render_mode="human" ，再单独调用 env.render() 不能获取 RGB 数组

        if checkpoint_path is not None:
            self.load_checkpoint(checkpoint_path)


        total_rewards = []

        for episode in range(num_episodes):
            states = vec_env.reset()
            states = states.transpose(0, 3, 1, 2)
            total_reward = 0
            done = False

            while not done:
                # 选择动作 (评估时不使用探索)
                actions = self.select_action(states, evaluate=True)

                states, rewards, dones, _ = vec_env.step(actions)
                done = dones[0]
                states = states.transpose(0, 3, 1, 2)
                total_reward += rewards[0]
                vec_env.render("human")

            total_rewards.append(total_reward)
            print(f"Evaluation Episode {episode + 1}, Reward: {total_reward:.1f}")

        avg_reward = np.mean(total_rewards)
        print(f"Average reward over {num_episodes} episodes: {avg_reward:.1f}")
        return avg_reward


if __name__ == "__main__":

    # 创建环境 (训练时不渲染)
    n_envs = 3
    vec_env = make_atari_env('BreakoutNoFrameskip-v4', n_envs=n_envs, seed=0) #这里会进行灰度化和裁剪为 84x84
    # Frame-stacking with 4 frames
    vec_env = VecFrameStack(vec_env, n_stack=4)

    # 初始化智能体
    agent = DQNAgent(vec_env, n_envs=n_envs)

    agent.train(checkpoint_interval=500, checkpoint_dir="./checkpoints")
    agent.evaluate(vec_env, 10,"./checkpoints/dqn_episode_9226_299.0001.pth")

```

#### 1.3 实操2：Pong游戏

Pong游戏看似和Breakout游戏很类似，似乎把上面的代码拿过来把env_id改一下就ok，但实际上Pong游戏的reward非常稀疏，而且是对抗型的游戏。

使用SB3的DQN封装训练50万步也没有收敛的趋势，可能要改算法？

三个AI都说从适合稀疏奖励的的角度， PPO > A2C > DQN，所以换算法试试。

##### 1.3.1 PPO方法(没搞定)

不收敛。

我对手搓的PPO一向没有什么信心，实现细节太多，下面的代码把环境改为 ALE/Breakout-v5也不能收敛，几万把游戏下来也没有什么长进，可能代码/超参数还是有问题。

用SB3的PPO实现训练Breakout任务，可以看到明显收敛趋势。

```python
import os
import numpy as np
import torch
import torch.nn as nn
from torch.distributions.categorical import Categorical
from stable_baselines3.common.atari_wrappers import (
    ClipRewardEnv,
    EpisodicLifeEnv,
    FireResetEnv,
    MaxAndSkipEnv,
    NoopResetEnv,
)
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.torch_layers import NatureCNN
from gymnasium.wrappers import ResizeObservation, TransformObservation, GrayscaleObservation
from gymnasium import spaces
import gymnasium as gym
from typing import Optional, Dict, Any, Tuple
from datetime import datetime
import warnings
import ale_py
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime as dt

# 忽略警告
warnings.filterwarnings("ignore")

logTimestamp = dt.now().strftime("%y%m%d_%H%M%S")
writer = SummaryWriter(f"logs/ppo_{logTimestamp}")


class PPOConfig:
    """PPO算法的超参数配置类"""

    def __init__(self):
        # 环境参数
        self.env_id = "ALE/Pong-v5"
        self.seed = 42
        self.capture_video = True

        # PPO超参数
        self.total_timesteps = 1_000_000  # 总训练步数
        self.learning_rate = 2.5e-4
        self.num_steps = 128  # 每次收集的步数
        self.gamma = 0.99  # 折扣因子
        self.gae_lambda = 0.95  # GAE参数
        self.num_minibatches = 4  # 小批量数
        self.update_epochs = 4  # 每次更新的epoch数
        self.clip_coef = 0.1  # PPO的clip系数
        self.ent_coef = 0.01  # 熵系数
        self.vf_coef = 0.5  # value函数系数
        self.max_grad_norm = 0.5  # 最大梯度范数

        # 网络架构
        self.hidden_size = 512
        self.cnn_features_dim = 512

        # 其他参数
        self.save_freq = 50_000  # 保存模型的频率
        self.checkpoint_path = "ppo_pong_checkpoints"  # 模型保存路径


class PPONetwork(nn.Module):
    """简化的PPO网络架构"""

    def __init__(self, env: gym.Env, config: PPOConfig):
        super().__init__()
        self.config = config

        # 特征提取器 (使用NatureCNN)
        self.feature_extractor = NatureCNN(
            env.observation_space,
            features_dim=config.cnn_features_dim,
            normalized_image=True
        )

        # Actor网络
        self.actor = nn.Sequential(
            nn.Linear(config.cnn_features_dim, config.hidden_size),
            nn.ReLU(),
            nn.Linear(config.hidden_size, env.action_space.n),
        )

        # Critic网络
        self.critic = nn.Sequential(
            nn.Linear(config.cnn_features_dim, config.hidden_size),
            nn.ReLU(),
            nn.Linear(config.hidden_size, 1),
        )

    def get_value(self, x: torch.Tensor) -> torch.Tensor:
        """获取状态价值"""
        return self.critic(self.feature_extractor(x))

    def get_action_and_value(
            self,
            x: torch.Tensor,
            action: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """获取动作、对数概率、熵和状态价值"""
        hidden = self.feature_extractor(x)
        logits = self.actor(hidden)
        probs = Categorical(logits=logits)

        if action is None:
            action = probs.sample()

        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)


class PPOBuffer:
    """简化的经验回放缓冲区"""

    def __init__(self, config: PPOConfig, obs_shape: tuple, action_shape: tuple, device: torch.device):
        self.config = config
        self.device = device

        # 初始化缓冲区
        self.obs = torch.zeros((config.num_steps,) + obs_shape).to(device)
        self.actions = torch.zeros((config.num_steps,) + action_shape).to(device)
        self.logprobs = torch.zeros((config.num_steps,)).to(device)
        self.rewards = torch.zeros((config.num_steps,)).to(device)
        self.dones = torch.zeros((config.num_steps,)).to(device)
        self.values = torch.zeros((config.num_steps,)).to(device)

        self.reset()

    def reset(self):
        """重置缓冲区指针"""
        self.step = 0

    def add(
            self,
            obs: torch.Tensor,
            action: torch.Tensor,
            logprob: torch.Tensor,
            reward: torch.Tensor,
            done: torch.Tensor,
            value: torch.Tensor,
    ):
        """向缓冲区添加经验"""
        self.obs[self.step] = obs
        self.actions[self.step] = action
        self.logprobs[self.step] = logprob
        self.rewards[self.step] = reward
        self.dones[self.step] = done
        self.values[self.step] = value
        self.step += 1

    def get(self, last_value) -> Dict[str, torch.Tensor]:
        """获取所有经验并计算GAE和回报"""
        # 计算GAE和回报
        advantages = torch.zeros_like(self.rewards).to(self.device)
        lastgaelam = 0

        with torch.no_grad():
            for t in reversed(range(self.config.num_steps)):
                if t == self.config.num_steps - 1:
                    nextnonterminal = 1.0 - self.dones[t]
                    #nextvalues = self.values[t]
                    nextvalues = last_value
                else:
                    nextnonterminal = 1.0 - self.dones[t]
                    nextvalues = self.values[t + 1]

                delta = self.rewards[t] + self.config.gamma * nextvalues * nextnonterminal - self.values[t]
                advantages[
                    t] = lastgaelam = delta + self.config.gamma * self.config.gae_lambda * nextnonterminal * lastgaelam

            returns = advantages + self.values

        return {
            "obs": self.obs,
            "actions": self.actions,
            "logprobs": self.logprobs,
            "advantages": advantages,
            "returns": returns,
        }


def make_env(env_id: str, seed: int, capture_video: bool = False, render_mode="rgb_array") -> gym.Env:
    """创建并包装单个环境"""
    env = gym.make(env_id, render_mode=render_mode)
    env = gym.wrappers.RecordEpisodeStatistics(env)
    env.reset(seed=seed)
    env.action_space.seed(seed)
    env.observation_space.seed(seed)

    # Atari预处理
    env = NoopResetEnv(env, noop_max=30)
    #env = MaxAndSkipEnv(env, skip=4)  # ALE已经跳过4帧，这里再跳过4帧(总共8帧)
    env = EpisodicLifeEnv(env)
    env = FireResetEnv(env)
    env = ClipRewardEnv(env)

    # 图像预处理
    env = GrayscaleObservation(env, keep_dim=False)
    env = ResizeObservation(env, shape=(84, 84))
    env = TransformObservation(env, lambda obs: obs.astype(np.float32) / 255.0,
                               observation_space=gym.spaces.Box(0.0, 1.0, shape=env.observation_space.shape, dtype=np.float32))

    # 帧堆叠
    env = gym.wrappers.FrameStackObservation(env, 4)

    return env


class PPOTrainer:
    """简化的PPO训练器"""

    def __init__(self, config: PPOConfig):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # 创建环境
        self.env = make_env(config.env_id, config.seed)

        # 初始化网络和优化器
        self.agent = PPONetwork(self.env, config).to(self.device)
        self.optimizer = torch.optim.Adam(self.agent.parameters(), lr=config.learning_rate, eps=1e-5)

        # 创建缓冲区
        obs_shape = self.env.observation_space.shape
        action_shape = self.env.action_space.shape
        self.buffer = PPOBuffer(config, obs_shape, action_shape, self.device)

        # 创建检查点目录
        os.makedirs(config.checkpoint_path, exist_ok=True)

        # 训练统计
        self.global_step = 0
        self.episode_reward = 0
        self.episode_length = 0

    def train(self):
        """训练循环"""
        while self.global_step < self.config.total_timesteps:
            # 收集经验
            self.buffer.reset()
            obs, _ = self.env.reset()
            obs = torch.Tensor(np.array(obs)).to(self.device)

            for _ in range(self.config.num_steps):
                self.global_step += 1

                with torch.no_grad():
                    action, logprob, _, value = self.agent.get_action_and_value(obs.unsqueeze(0))

                # 执行动作
                next_obs, reward, terminated, truncated, info = self.env.step(action.cpu().numpy()[0])
                done = terminated or truncated

                # 存储经验
                self.buffer.add(
                    obs,
                    action,
                    logprob,
                    torch.tensor(reward).to(self.device),
                    torch.tensor(done).to(self.device),
                    value.flatten(),
                )
                # 关键修复2：回合中途结束时立即重置环境
                if done:
                    next_obs, _ = self.env.reset()

                # 更新观察
                obs = torch.Tensor(np.array(next_obs)).to(self.device)

                # 记录回合信息
                if "episode" in info:
                    print(f"Episode len: {info['episode']['l']}: reward={info['episode']['r']:.2f}")
                    writer.add_scalar("episode/episode_len", info['episode']['l'], self.global_step)
                    writer.add_scalar("episode/episode_rew", info['episode']['r'], self.global_step)

            # 计算最后一个观察的价值
            with torch.no_grad():
                last_value = self.agent.get_value(obs.unsqueeze(0)).reshape(1, -1)

            # 获取批量数据
            batch = self.buffer.get(last_value)

            # 优化网络
            for epoch in range(self.config.update_epochs):
                # 随机打乱数据
                indices = torch.randperm(self.config.num_steps, device=self.device)

                # 小批量更新
                for start in range(0, self.config.num_steps, self.config.num_steps // self.config.num_minibatches):
                    end = start + (self.config.num_steps // self.config.num_minibatches)
                    mb_indices = indices[start:end]

                    # 获取小批量数据
                    mb_obs = batch["obs"][mb_indices]
                    mb_actions = batch["actions"][mb_indices]
                    mb_logprobs = batch["logprobs"][mb_indices]
                    mb_advantages = batch["advantages"][mb_indices]
                    mb_returns = batch["returns"][mb_indices]

                    # 计算新的动作概率和价值
                    _, newlogprob, entropy, newvalue = self.agent.get_action_and_value(mb_obs, mb_actions)

                    # 计算比率
                    logratio = newlogprob - mb_logprobs
                    ratio = logratio.exp()

                    # 标准化优势
                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)

                    # 策略损失
                    pg_loss1 = -mb_advantages * ratio
                    pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - self.config.clip_coef, 1 + self.config.clip_coef)
                    pg_loss = torch.max(pg_loss1, pg_loss2).mean()

                    # 价值损失
                    v_loss = 0.5 * ((newvalue - mb_returns) ** 2).mean()

                    # 熵损失
                    entropy_loss = entropy.mean()

                    # 总损失
                    loss = pg_loss - self.config.ent_coef * entropy_loss + v_loss * self.config.vf_coef

                    # 优化步骤
                    self.optimizer.zero_grad()
                    loss.backward()
                    nn.utils.clip_grad_norm_(self.agent.parameters(), self.config.max_grad_norm)
                    self.optimizer.step()

            # 保存检查点
            if self.global_step % self.config.save_freq == 0:
                self.save_checkpoint(f"ppo_pong_{self.global_step}.pt")

    def save_checkpoint(self, filename: str):
        """保存模型检查点"""
        checkpoint_path = os.path.join(self.config.checkpoint_path, filename)
        torch.save({
            "global_step": self.global_step,
            "model_state_dict": self.agent.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
        }, checkpoint_path)
        print(f"Saved checkpoint to {checkpoint_path}")

    def load_checkpoint(self, filename: str):
        """加载模型检查点"""
        checkpoint_path = os.path.join(self.config.checkpoint_path, filename)
        checkpoint = torch.load(checkpoint_path)

        self.agent.load_state_dict(checkpoint["model_state_dict"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        self.global_step = checkpoint["global_step"]

        print(f"Loaded checkpoint from {checkpoint_path}")

    def evaluate(self, num_episodes: int = 5, render: bool = True):
        """评估模型性能"""
        eval_env = make_env(self.config.env_id, self.config.seed + 1, render_mode="human")

        episode_rewards = []

        for i in range(num_episodes):
            obs, _ = eval_env.reset()
            done = False
            episode_reward = 0

            while not done:
                obs_tensor = torch.Tensor(np.array(obs)).to(self.device).unsqueeze(0)

                with torch.no_grad():
                    action, _, _, _ = self.agent.get_action_and_value(obs_tensor)

                obs, reward, terminated, truncated, _ = eval_env.step(action.cpu().numpy()[0])
                done = terminated or truncated
                episode_reward += reward

            episode_rewards.append(episode_reward)
            print(f"Evaluation Episode {i + 1}: Reward = {episode_reward}")

        print(f"\nMean Reward over {num_episodes} episodes: {np.mean(episode_rewards):.2f}")


if __name__ == "__main__":
    config = PPOConfig()
    trainer = PPOTrainer(config)

    # 训练模型
    trainer.train()

    # 评估模型
    trainer.evaluate(render=True)
```

##### 1.3.2 Actor-Critic方法

**不收敛**，没有明显的收敛趋势。

但如果把env_id改为Breakout就有收敛的趋势（如下图），说明代码本身没有问题，主要还是环境的差异。

![image-20250520181508945](img/RL/image-20250520181508945.png)

```python
import os
import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
from torch.distributions import Categorical
from stable_baselines3.common.vec_env import  SubprocVecEnv, VecFrameStack,VecTransposeImage
from stable_baselines3.common.atari_wrappers import (
    NoopResetEnv, EpisodicLifeEnv, FireResetEnv, ClipRewardEnv
)
from gymnasium.wrappers import ResizeObservation, TransformObservation, GrayscaleObservation
from stable_baselines3.common.torch_layers import NatureCNN
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
import ale_py

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

log_dir = "logs"
os.makedirs(log_dir, exist_ok=True)
writer = SummaryWriter(log_dir=os.path.join(log_dir, datetime.now().strftime("%Y%m%d-%H%M%S")))

# ---------------------------
# 环境封装
# ---------------------------
def make_env(env_id, seed):
    def thunk():
        env = gym.make(env_id, render_mode="rgb_array")
        env = gym.wrappers.RecordEpisodeStatistics(env)
        env.reset(seed=seed)
        env.action_space.seed(seed)
        env.observation_space.seed(seed)

        # Atari预处理
        env = NoopResetEnv(env, noop_max=30)
        env = EpisodicLifeEnv(env)
        env = FireResetEnv(env)
        env = ClipRewardEnv(env)

        # 图像预处理
        env = GrayscaleObservation(env, keep_dim=False)
        env = ResizeObservation(env, shape=(84, 84))
        env = TransformObservation(env, lambda obs: obs.astype(np.float32) / 255.0,
                                   observation_space=gym.spaces.Box(0.0, 1.0, shape=env.observation_space.shape,
                                                                    dtype=np.float32))

        # 帧堆叠
        env = gym.wrappers.FrameStackObservation(env, 4)
        return env
    return thunk


def make_vec_env(env_id, num_envs=4, seed=0, frame_stack=4):
    envs = SubprocVecEnv([make_env(env_id, seed + i) for i in range(num_envs)])

    return envs


# ---------------------------
# 模型结构定义
# ---------------------------
class ActorCriticNet(nn.Module):
    def __init__(self, observation_space, action_space):
        super().__init__()
        self.shared_cnn = NatureCNN(observation_space, features_dim=512, normalized_image=True)
        self.actor = nn.Linear(512, action_space)
        self.critic = nn.Linear(512, 1)

    def forward(self, x):
        features = self.shared_cnn(x)
        logits = self.actor(features)
        value = self.critic(features).squeeze()
        return logits, value


# ---------------------------
# Agent 封装
# ---------------------------
class ActorCriticAgent:
    def __init__(self, observation_space, action_space, lr=1e-4, gamma=0.99):
        self.gamma = gamma
        self.net = ActorCriticNet(observation_space, action_space).to(device)
        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=lr)

    def select_action(self, obs):
        logits, value = self.net(obs)
        probs = Categorical(logits=logits)
        action = probs.sample()
        log_prob = probs.log_prob(action)
        return action, log_prob, value, probs.entropy()

    def train_step(self, obs, action, log_prob, value, reward, next_obs, done, entropy):
        with torch.no_grad():
            _, next_value = self.net(next_obs)
            target = reward + self.gamma * next_value.detach() * (1 - done)

        advantage = target - value
        actor_loss = -(log_prob * advantage.detach()).mean()
        critic_loss = advantage.pow(2).mean()
        loss = actor_loss + critic_loss - 0.1 * entropy.mean()

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        return loss.item(), actor_loss.item(), critic_loss.item()

def report_infos(infos, timestep):
    reward_list = []
    length_list = []
    for info in infos:
        if "episode" in info:
            reward_list.append(info['episode']["r"])
            length_list.append(info['episode']["l"])

    if reward_list.__len__() > 0:
        avg_reward = np.mean(reward_list)
        writer.add_scalar("steps/AverageReward", avg_reward, timestep)
    if length_list.__len__() > 0:
        avg_len = np.mean(length_list)
        writer.add_scalar("steps/AverageLength", avg_len, timestep)

# ---------------------------
# 主训练循环
# ---------------------------
def train():
    env_id = "ALE/Pong-v5"
    #env_id = "ALE/Breakout-v5"
    num_envs = 13
    total_timesteps = 1_000_000
    log_interval = 100
    save_interval = 100_000

    envs = make_vec_env(env_id, num_envs=num_envs, seed=42)
    agent = ActorCriticAgent(envs.observation_space, envs.action_space.n)


    model_dir = "checkpoints"
    os.makedirs(model_dir, exist_ok=True)

    obs = envs.reset()
    obs = torch.tensor(obs, dtype=torch.float32).to(device)
    print(f"obs shape:{obs.shape}")



    for timestep in range(1, total_timesteps + 1):
        action, log_prob, value, entropy = agent.select_action(obs)

        next_obs, reward, done, infos = envs.step(action.cpu().numpy())
        next_obs_t = torch.tensor(next_obs, dtype=torch.float32).to(device)
        if timestep == 1: print(f"next_obs shape:{next_obs.shape}")
        reward_t = torch.tensor(reward, dtype=torch.float32).to(device)
        done_t = torch.tensor(done, dtype=torch.float32).to(device)

        #并行环境下的next_obs和done是需要特殊处理的
        true_next_obs = []
        true_done = []
        for env_idx in range(envs.num_envs):
            if done_t[env_idx].item() > 0:
                if infos[env_idx]["TimeLimit.truncated"]:  # 被截断了，不是真的结束，那么这个时间步的done flag应该为false
                    true_done.append(torch.tensor(0.0))
                    true_next_obs.append(next_obs_t[env_idx])
                else:  # 是真的一个回合结束了，由于环境自动reset，所以next_states并不是真的随后一个状态，而是reset后下一回合的新状态
                    next_stt = infos[env_idx]["terminal_observation"]
                    true_next_obs.append(torch.tensor(next_stt, dtype=torch.float32))
                    true_done.append(done_t[env_idx])
            else:
                true_next_obs.append(next_obs_t[env_idx])
                true_done.append(done_t[env_idx])
        true_next_obs = torch.stack(true_next_obs).to(device)
        true_done = torch.stack(true_done).to(device)


        loss, actor_loss, critic_loss = agent.train_step(
            obs, action, log_prob, value, reward_t, true_next_obs, true_done, entropy )

        if timestep % log_interval == 0:
            writer.add_scalar("steps/ActorLoss", actor_loss, timestep)
            writer.add_scalar("steps/CriticLoss", critic_loss, timestep)

        obs = next_obs_t

        report_infos(infos, timestep)





        if timestep % save_interval == 0:
            torch.save(agent.net.state_dict(), os.path.join(model_dir, f"ac_{timestep}.pt"))
            print(f"Model saved at timestep {timestep}")


if __name__ == "__main__":
    train()

```

##### 1.3.3 SB3实现的A2C方式

下面SB3的pong的训练也不收敛。训练了100万步，看不到收敛的趋势。

把env_id换成 "BreakoutNoFrameskip-v4"能看到明显的收敛趋势（如下图），演示效果也很好。

![image-20250521085052566](img/RL/image-20250521085052566.png)



```python
from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack
from stable_baselines3 import A2C
import ale_py

game="PongNoFrameskip-v4"

vec_env = make_atari_env(game, n_envs=4, seed=0)
vec_env = VecFrameStack(vec_env, n_stack=4)

model = A2C("CnnPolicy", vec_env, verbose=1, tensorboard_log="logs")
model.learn(total_timesteps=1_000_000)
model.save("a2c_pong")
del model

# 测试环境用单个环境，模型要开启确定性预测，否则表现会很糟糕
vec_env = make_atari_env(game, n_envs=1, seed=0)
vec_env = VecFrameStack(vec_env, n_stack=4)
model = A2c.load("a2c_pong")

obs = vec_env.reset()
while True:
    action, _states = model.predict(obs, deterministic=True)
    obs, rewards, dones, info = vec_env.step(action)
    vec_env.render("human")
    if dones:
        obs = vec_env.reset()
```

##### 1.3.4 SB3实现的PPO方式

能够收敛，[实际演示的效果也挺好的](img/RL/pong_ppo.mp4)。

![image-20250521140449956](img/RL/image-20250521140449956.png)

可见算法之间的适应能力还是差别比较大的，同样的任务，A2C不能收敛，PPO收敛得很好。

代码如下：

```python
from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack
from stable_baselines3 import A2C, PPO
import ale_py
from stable_baselines3.common.callbacks import CheckpointCallback

game = "PongNoFrameskip-v4"
checkpoint_callback = CheckpointCallback(
    save_freq=100_000,               # 每隔多少 environment steps 保存一次
    save_path="./checkpoints/",    # 模型保存路径

    name_prefix="ppo_pong"   # 保存的文件名前缀
)

vec_env = make_atari_env(game, n_envs=6, seed=0)
vec_env = VecFrameStack(vec_env, n_stack=4)

model = PPO("CnnPolicy", vec_env, verbose=1, tensorboard_log="logs")
model.learn(total_timesteps=1_000_000, callback=checkpoint_callback)
model.save("ppo_pong")
del model

# 测试环境用单个环境，模型要开启确定性预测，否则表现会很糟糕
test_env = make_atari_env(game, n_envs=1, seed=42)
test_env = VecFrameStack(test_env, n_stack=4)
model = PPO.load("ppo_pong", env=test_env)

obs = test_env.reset()
while True:
    action, _states = model.predict(obs, deterministic=True)  # 开启确定性预测
    obs, reward, done, info = test_env.step(action)
    test_env.render("human")
    if done:
        obs = test_env.reset()
```

#### 1.4 实操3：CartPole游戏

我在“强化学习入门.md”笔记中有手搓DQN+像素输入的方式来训练agent玩CartPole，agent有明显学习和提升，但相比结构化输入的效果总是远远不及。现在有了SB3，我看看用SB3重做这个实验会不会效果更好。



### 2 机器人手臂的仿真模拟环境

![image-20250501200451197](img/RL/image-20250501200451197.png)

### 3 高并发环境的并行仿真

并行化环境交互是强化学习**提速**、**稳健**与**可扩展**的基础手段：

1. 提升样本吞吐，缩短调参/收敛时间。尤其对于交互时延很大/计算量很大的环境
2. 打散数据相关性，稳定梯度估计
3. 更好地利用 CPU/GPU 资源，例如可以异步的同时进行样本收集（侧重CPU+网络）和网络更新（侧重GPU）
4. 支撑异步、大规模分布式算法。例如当下很火的LLM，每次交互时延/计算量都很大，为了高效收集样本数据，必须进行分布式部署

快速研究了一下三个并发库，结论是SB3很好，其他两个有或多或少的问题，不好用。

```
# stable_baseline3
https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html

# ray rllib
https://docs.ray.io/en/latest/cluster/getting-started.html

# envpool
https://envpool.readthedocs.io/en/latest/
```

#### 3.1 开发库 Stable-Baselines3

[SB3](https://stable-baselines3.readthedocs.io/en/master/index.html)是一个基于 PyTorch 的强化学习（RL）库，提供可靠且易于使用的 RL 算法实现，特性有：

1. 实现了多种常用的强化学习算法，所有算法遵循统一的接口和代码结构，便于快速上手和切换不同算法
2. 通过向量化环境（VecEnv）支持并行训练，加速实验过程
3. 提供详细的用户指南、API 参考和教程；充分测试（95%的覆盖率）
4. RL Baselines3 Zoo提供预训练的代理、超参数调优脚本、评估工具和结果可视化
5. 支持用户定义自己的特征提取器和网络结构，满足特定需求；提供灵活的回调机制，便于在训练过程中插入自定义逻辑，如早停、模型保存等
6. atari环境的wrappers，例如ClipRewardEnv / EpisodicLifeEnv / FireResetEnv等， 还有各种工具类，让开发变得简单方便。



思考：

stable-baseline3提供了对很多RL算法的封装，对于典型的gym 游戏的训练只需要3-5行代码，极其方便：

1. 从知其所以然的角度来看，封装的太好不利于我这样的初学者洞悉背后的细节，很多时候我希望手搓代码
2. 但是从有一个好的开始而提振信心（尤其是对于高维像素输入方式下的游戏agent训练）来说，有一个先成功跑起来的代码是有较大价值的。下面的3.4就让人很激动
3. 另外，研究中用作实验对比的基准，需要一个可靠的RL算法x环境的实现。毕竟RL很玄学，有时候即使训练出来一个可以work的agent，也拿不准是不是充分收敛的agent



**具体到SB3中提供的VecEnv来实现并行多环境交互：**

VecEnv有两个具体实现：DummyVecEnv和SubprocVecEnv，前者实际上是串行的在多个env上执行交互，后者才是真正的多进程并发。

使用make_vec_env()函数可以创建VecEnv并明确指定具体的实现，也可以直接调用他们的构造函数。

```python
vec_env = make_vec_env(env_id='CartPole-v1', n_envs=4, vec_env_cls=SubprocVecEnv)
```

VecEnv的使用注意事项：

![image-20250516123835826](img/RL/image-20250516123835826.png)

根据上面的文档，**回合终止的处理需要额外注意**：

1. 当一个 episode 正常终止（terminated），我们就知道它的最终结果，比如 agent 掉落悬崖，得了 -100 分 —— 这是明确的，我们就不需要估计接下来的价值（因为 episode 已经结束，没有下一步）。但如果 episode 是因为 **时间限制（timeout）被截断（truncated）**，那其实 agent 可能还没死、还可以继续获得 reward，只是我们强制停止了。在这种情况下：
   1. 我们**不能简单地认为 episode 到此就结束了**；
   2. 所以我们**应当用当前状态的值函数来估算未来**，这就是 bootstrap。
2. 另外，当一个回合终止后，由于并行环境会自动reset，那么step函数返回的state其实是reset后新的回合的开始，并非上一个回合真的最后一个state，所以要通过infos字段来获得真正的最后一个state，放入到replaybuffer里。



#### 3.2 实操一：用VecEnv+手搓DQN实现并行交互

手搓代码总是让人更好的理解背后的机制，下面的代码能够取得不错的训练效果：

![image-20250516121322765](img/RL/image-20250516121322765.png)

```
Inference finished. Total reward: 369.0
```

代码如下：

```python
import random
import gym
import numpy
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from collections import deque
import argparse
import os
from stable_baselines3.common.vec_env import SubprocVecEnv
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime as dt

# 设备选择
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
writer = SummaryWriter(f"logs/parrallel_{dt.now().strftime('%y%m%d_%H%M%S')}")

# 超参数
gamma = 0.99  # 折扣因子
epsilon = 1.0  # 初始探索率
epsilon_min = 0.01  # 最低探索率
epsilon_decay = 0.95  # 探索率衰减
learning_rate = 1e-3  # 学习率
batch_size = 64  # 经验回放的批量大小
memory_size = 100_000  # 经验池大小
target_update_freq = 10  # 目标网络更新频率
n_envs = 4

# 创建一个返回 CartPole 环境的函数（每个子环境都需要一个独立实例）
def make_env(seed, index):
    def _init():
        env = gym.make("CartPole-v1", max_episode_steps=None) #max_episode_steps这样修改没有用，内部还是500步最大，需要继续研究
        return env
    return _init

single_env = gym.make("CartPole-v1", render_mode="human")
n_state = single_env.observation_space.shape[0]  # 状态维度
n_action = single_env.action_space.n  # 动作数量

# DQN 网络定义
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)


# 初始化网络
model = DQN(n_state, n_action).to(device)
target_model = DQN(n_state, n_action).to(device)
target_model.load_state_dict(model.state_dict())
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
memory = deque(maxlen=memory_size)


def select_action(states:numpy.ndarray, epsilon):

    """基于 ε-greedy 选择动作"""
    if random.random() < epsilon:
        actions = torch.zeros(states.shape[0], dtype=torch.long).to(device)
        for i in range(states.shape[0]):
            actions[i] = random.randint(0, n_action - 1)
        return   actions.cpu().numpy()
    else:
        states = torch.tensor(states).to(device)
        if states.shape.__len__() < 2:
            states = states.unsqueeze(0)
        return model(states).argmax(1).cpu().numpy()  # 选取 Q 值最大的动作


def train():
    if len(memory) < batch_size*10:
        return 99.0  # 经验池数据不足时不训练

    batch = random.sample(memory, batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)

    states = torch.FloatTensor(states).to(device)  # (batch_size, 4)
    actions = torch.LongTensor(actions).unsqueeze(1).to(device)  # (batch_size,) -> (batch_size, 1)
    rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)  # (batch_size,) -> (batch_size, 1)
    next_states = torch.FloatTensor(next_states).to(device)  # (batch_size, 4)
    dones = torch.FloatTensor(dones).unsqueeze(1).to(device)  # (batch_size,) -> (batch_size, 1)

    # 计算当前 Q 值
    q_values = model(states).gather(1, actions)  # 从 Q(s, a) 选取执行的动作 Q 值

    # 计算目标 Q 值
    next_q_values = target_model(next_states).max(1, keepdim=True)[0]  # 选取 Q(s', a') 的最大值
    target_q_values = rewards + gamma * next_q_values * (1 - dones)  # TD 目标

    # 计算损失
    loss = F.mse_loss(q_values, target_q_values.detach())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()


def save_checkpoint(id):
    path=f"checkpoints/dqn_checkpoint_{id}.pth"
    torch.save(model.state_dict(), path)
    print(f"Checkpoint saved to {path}")


def load_checkpoint(path):
    if os.path.exists(path):
        model.load_state_dict(torch.load(path, map_location=device))
        print(f"Checkpoint loaded from {path}")
    else:
        print("No checkpoint found, starting from scratch.")


def main(mode, env):
    global epsilon

    if mode == "train":
        epoches = 500
        states = env.reset() 
        episode_len = 0 #跟踪0号环境的回合长度
        episode_cnt = 0 # 跟踪0号环境的回合个数
        step_cnt = 0 #总的交互次数，没有乘以环境并发数
        max_episode_len = -1 #跟踪0号环境最大的回合长度
        for epoch in range(epoches):
            # 本来是可以一直执行下去，因为并行环境会自动reset，但这里做一个控制：
            # 每300步要跳出来做一点其他事情，例如更新epsilon。
            # 300这个参数很关键的，直接影响epsilon的节奏
            for _ in range(300):
                actions = select_action(states, epsilon)
                next_states, rewards, dones, infos = env.step(actions) 
                step_cnt += 1
                if dones[0]:
                    # 处理episode_len数据
                    episode_cnt += 1
                    writer.add_scalar("a_train/episode_len", episode_len, episode_cnt) #加上a_让这个排在前面
                    if episode_len >= max_episode_len: #等于也保存，觉得后面训练时间更长，可能效果更好
                        max_episode_len = episode_len
                        save_checkpoint(max_episode_len)
                        if  episode_len == 499:
                            print("reach 499")
                    episode_len = 0
                else:
                    episode_len += 1

                # 经验回放缓存
                for env_idx in range(n_envs):
                    if not dones[env_idx]:
                        memory.append((states[env_idx], actions[env_idx], rewards[env_idx], next_states[env_idx], dones[env_idx]))
                        continue
                    if infos[env_idx]["TimeLimit.truncated"]: #被截断了，不是真的结束，那么这个时间步的done flag应该为false
                        memory.append( (states[env_idx], actions[env_idx], rewards[env_idx], next_states[env_idx], False))
                    else: # 是真的一个回合结束了，由于环境自动reset，所以next_states并不是真的随后一个状态，而是reset后下一回合的新状态
                        next_stt = infos[env_idx]["terminal_observation"] 
                        memory.append( (states[env_idx], actions[env_idx], rewards[env_idx], next_stt, dones[env_idx]))

                states = next_states


                # 训练 DQN
                loss = train()
                if step_cnt % 10 == 0:
                    writer.add_scalar("loss/loss", loss, step_cnt)

            # 逐步降低 epsilon，减少随机探索，提高利用率
            epsilon = max(epsilon_min, epsilon * epsilon_decay)

            # 定期更新目标网络，提高稳定性
            if epoch % target_update_freq == 0:
                target_model.load_state_dict(model.state_dict())



            writer.add_scalar("epoch/epsilon", epsilon, epoch)
            print(f"Finished epoch#{epoch}")

    elif mode == "infer":
        load_checkpoint("./checkpoints/dqn_checkpoint_499.pth")
        state = single_env.reset()
        state = state[0]
        total_reward = 0

        while True:
            single_env.render()
            action = select_action(state, 0)  # 纯利用，epsilon=0
            state, reward, done, _, _ = single_env.step(action[0])
            total_reward += reward

            if done:
                break

        print(f"Inference finished. Total reward: {total_reward}")


if __name__ == "__main__":
    # 使用 SubprocVecEnv 实现多进程并行环境（注意：在 Windows 中建议使用 if __name__ == "__main__"）
    env_fns = [make_env(seed=i, index=i) for i in range(n_envs)]
    envs = SubprocVecEnv(env_fns)
    main("train", envs)
```

上面的SubprocVecEnv的创建方式默认有限制一个回合的最大步数为500，也可以通过下面的代码去掉：

```python
import gym
from gym.wrappers import TimeLimit
from stable_baselines3.common.env_util import make_vec_env

def make_unlimited_cartpole():
    env = gym.make("CartPole-v1")
    # 如果环境被 TimeLimit 包装，移除它
    if isinstance(env, TimeLimit):
        env = env.env  # unwrap TimeLimit
    return env

# 使用 make_vec_env 构造并行环境
envs = make_vec_env(make_unlimited_cartpole, n_envs=4)

```

这样果然会出现回合长度超过10000的回合。

#### 3.3 实操二：直接用SB3训练DQN（CartPole）

感觉上面3.2的自己手搓的代码还不够好，因为训练的回合长度没有一路单调增长，中间波动很大，但又不知道问题出在哪里。

所以直接用SB3这样的专家代码来训练DQN，发现回合平均长度也波动很大。

下面代码直接使用SB3库解决CartPole任务，可以看到回合平均长度波动很大且后面都变得很糟糕。但通过及时的保存best model，得到的模型在评估中表现很好。

![image-20250517154356656](img/RL/image-20250517154356656.png)

评估时候输出的回合长度：

```
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
```

代码：

```python
from stable_baselines3 import DQN
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback


eval_env = make_vec_env("CartPole-v1", n_envs=1)
# 创建评估回调
eval_callback = EvalCallback(
    eval_env,
    best_model_save_path="./best_model/",
    log_path="./logs/eval/",
    eval_freq=50_000,                  # 每隔多少步评估一次
    n_eval_episodes=5,                 # 每次评估使用多少个 episode
    deterministic=True,
    render=False,
    verbose=1                          # 日志显示
)

if __name__ == "__main__":
    #下面函数默认是创建DummyVecEnv，不是真的并行，除非明确指定vec_env_cls=SubprocVecEnv，但对于cartpole这样简单的
    # 环境，后者反而更慢，因为增加了额外的进程间通信开销
    vec_env = make_vec_env(env_id='CartPole-v1', n_envs=4,vec_env_cls=SubprocVecEnv)
    model = DQN("MlpPolicy", vec_env,
                    verbose=1,
                    tensorboard_log="logs") #这里可以写很多参数，包括replaybuffer大小、衰减率、学习率等等
    model.learn(total_timesteps=5_000_000, callback=eval_callback)
    del model

    model = DQN.load("./best_model/best_model.zip")

    obs = vec_env.reset()
    for i in range(10):
        done = False
        episode_len = 0
        while not done:
            action, _states = model.predict(obs, deterministic=False)
            obs, rewards, dones, info = vec_env.step(action)
            done = dones[0]
            episode_len += 1
            vec_env.render("human")
        print(f"episode len:{episode_len}")

```



#### 3.4 实操三：用SB3库搞定像素输入（Breakout）

效果很好，可以说是惊艳，可以上分100多，见[视频](img/RL/SB3_breakout.mp4)：

```
一开始：
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 703      |
|    ep_rew_mean      | 1.13     |
|    exploration_rate | 0.923    |
| time/               |          |
|    episodes         | 1000     |
|    fps              | 475      |
|    time_elapsed     | 68       |
|    total_timesteps  | 32512    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00183  |
|    n_updates        | 1406     |
----------------------------------
经过4个小时后如下:
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 5.22e+03 |
|    ep_rew_mean      | 69.9     |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 27000    |
|    fps              | 295      |
|    time_elapsed     | 13172    |
|    total_timesteps  | 3898952  |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00214  |
|    n_updates        | 243059   |
----------------------------------
```



代码如下（来自SB3文档）：

```python
import gymnasium as gym
from stable_baselines3 import DQN
from stable_baselines3.common.atari_wrappers import AtariWrapper
from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv
import ale_py

# There already exists an environment generator
# that will make and wrap atari environments correctly.
# Here we are also multi-worker training (n_envs=4 => 4 environments)
vec_env = make_atari_env("BreakoutNoFrameskip-v4", n_envs=4, seed=0)
# Frame-stacking with 4 frames
vec_env = VecFrameStack(vec_env, n_stack=4)

# 使用 CNN 网络进行训练
model = DQN("CnnPolicy", vec_env, verbose=1, buffer_size=10_000, learning_rate=1e-4, batch_size=32,
            learning_starts=10_000, target_update_interval=1_000, train_freq=4)

# 训练 100k steps（可以增加）
model.learn(total_timesteps=4_000_000, log_interval=1_000)

# 保存模型
model.save("dqn_breakout")
del model

model = DQN.load("dqn_breakout")

obs = vec_env.reset()
while True:
    action, _states = model.predict(obs, deterministic=False)
    obs, rewards, dones, info = vec_env.step(action)
    vec_env.render("human")

```

#### 3.5 开发库 EnvPool（不好用有bug）

Here are EnvPool’s several highlights:

- Compatible with OpenAI `gym` APIs and DeepMind `dm_env` APIs;
- Manage a pool of envs, interact with the envs in batched APIs by default;
- Support both synchronous execution and asynchronous execution;
- Support both single player and multi-player environment;
- Easy C++ developer API to add new envs: [Customized C++ environment integration](https://envpool.readthedocs.io/en/latest/content/new_env.html);
- High performance
  - Free **~2x** speedup with only single environment;
  - **1 Million** Atari frames / **3 Million** Mujoco steps per second simulation with 256 CPU cores, **~20x** throughput of Python subprocess-based vector env;
  - **~3x** throughput of Python subprocess-based vector env on low resource setup like 12 CPU cores;
- Compatible and general
  - XLA support with JAX jit function;
  - Comparing with the existing GPU-based solution ([Brax](https://github.com/google/brax) / [Isaac-gym](https://developer.nvidia.com/isaac-gym)), EnvPool is a **general** solution for various kinds of speeding-up RL environment parallelization;
  - Compatible with some existing RL libraries, e.g., [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3), [Tianshou](https://github.com/thu-ml/tianshou), [ACME](https://github.com/deepmind/acme), [CleanRL](https://github.com/vwxyzjn/cleanrl) ([Solving Pong in 5 mins](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#solving-pong-in-5-minutes-with-ppo--envpool)), [rl_games](https://github.com/Denys88/rl_games) (2 mins [Pong](https://colab.research.google.com/drive/1iWFv0g67mWqJONoFKNWUmu3hdxn_qUf8?usp=sharing), 15 mins [Breakout](https://colab.research.google.com/drive/1U_NxL6gSs0yRVhfl0cKl9ttRmcmMCiBS?usp=sharing), 5 mins [Ant](https://colab.research.google.com/drive/1C9yULxU_ahQ_i6NUHCvOLoeSwJovQjdz?usp=sharing) and [HalfCheetah](https://colab.research.google.com/drive/1bser52bpItzmlME00IA0bbmPdp1Xm0fy?usp=sharing)).

**envpool比较操蛋，果断弃用**：

1. 不支持windows，只支持类unix环境。
2. 文档几乎没有，找个想要的示例代码都没有
3. 大模型对于envpool的问题，几乎回答不了，说明envpool在网上的信息很少或者质量不行
4. 下面3.6的代码，发现envpool有bug，8个环境reset出来都是一样的，env_id都是0，step为每个环境输入的action不一样得到的next_obs也是一样的。换Pool-v5也这样



#### 3.6 实操四：用EnvPool+手搓DQN实现并行交互

下面的代码在额外购买的linux云主机上可以跑，但实际上只有一个环境，envpool有bug

官方代码库还有很多[没有关闭的Bug Issue](https://github.com/sail-sg/envpool/issues)：

代码如下：

```python
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime as dt
import os
import envpool  
import gym

# 设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
writer = SummaryWriter(f"logs/envpool_{dt.now().strftime('%y%m%d_%H%M%S')}")

# 超参数
gamma = 0.99
epsilon = 1.0
epsilon_min = 0.01
epsilon_decay = 0.95
learning_rate = 1e-3
batch_size = 64
memory_size = 100_000
target_update_freq = 10
n_envs = 8

# 单个环境用于推理
single_env = gym.make("CartPole-v1", render_mode="human")
n_state = single_env.observation_space.shape[0]
n_action = single_env.action_space.n

# 网络结构
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

# 初始化模型
model = DQN(n_state, n_action).to(device)
target_model = DQN(n_state, n_action).to(device)
target_model.load_state_dict(model.state_dict())
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
memory = deque(maxlen=memory_size)

# ε-greedy 动作选择
def select_action(states: np.ndarray, epsilon):
    if random.random() < epsilon:
        actions = np.random.randint(0, n_action, size=states.shape[0])
        return actions
    else:
        states = torch.tensor(states, dtype=torch.float32).to(device)
        if states.ndim == 1:
            states = states.unsqueeze(0)
        with torch.no_grad():
            return model(states).argmax(1).cpu().numpy()

# 训练过程
def train():
    if len(memory) < batch_size * 10:
        return 99.0

    batch = random.sample(memory, batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)

    states = torch.FloatTensor(states).to(device)
    actions = torch.LongTensor(actions).unsqueeze(1).to(device)
    rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)
    next_states = torch.FloatTensor(next_states).to(device)
    dones = torch.FloatTensor(dones).unsqueeze(1).to(device)

    q_values = model(states).gather(1, actions)
    next_q_values = target_model(next_states).max(1, keepdim=True)[0]
    target_q_values = rewards + gamma * next_q_values * (1 - dones)

    loss = F.mse_loss(q_values, target_q_values.detach())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()

# 保存模型
def save_checkpoint(id):
    os.makedirs("checkpoints", exist_ok=True)
    path = f"checkpoints/dqn_checkpoint_{id}.pth"
    torch.save(model.state_dict(), path)
    print(f"Checkpoint saved to {path}")

# 加载模型
def load_checkpoint(path):
    if os.path.exists(path):
        model.load_state_dict(torch.load(path, map_location=device))
        print(f"Checkpoint loaded from {path}")
    else:
        print("No checkpoint found, starting from scratch.")

# 主函数
def main(mode, env):
    global epsilon

    if mode == "train":
        epoches = 500
        obs = env.reset()
        print("reset returns:", obs)
        states = obs[0]
        episode_len = 0
        episode_cnt = 0
        step_cnt = 0
        max_episode_len = -1

        for epoch in range(epoches):
            for _ in range(300):
                actions = select_action(states, epsilon)
                if step_cnt == 0: print("actions:", actions)
                next_obs, rewards, dones, truncs, infos = env.step(actions)
                next_states = next_obs
                if step_cnt == 0: print('Next_states:', next_states)
                step_cnt += 1

                # 处理0号环境的episode长度
                if dones[0] or truncs[0]:
                    episode_cnt += 1
                    writer.add_scalar("a_train/episode_len", episode_len, episode_cnt)
                    if episode_len >= max_episode_len:
                        max_episode_len = episode_len
                        save_checkpoint(max_episode_len)
                        if episode_len == 499:
                            print("reach 499")
                    episode_len = 0
                else:
                    episode_len += 1

                # 存入经验池，按环境逐一处理
                for env_idx in range(n_envs):
                    s = states[env_idx]
                    a = actions[env_idx]
                    r = rewards[env_idx]

                    # 情况 1：未终止也未截断，正常步
                    if not dones[env_idx] and not truncs[env_idx]:
                        next_s = next_states[env_idx]
                        done_flag = False

                    # 情况 2：被截断（如超时终止），使用 terminal_observation
                    elif truncs[env_idx] and "terminal_observation" in infos[env_idx]:
                        next_s = infos[env_idx]["terminal_observation"]
                        print(">>>>>>>>>>", next_s.shape)
                        done_flag = False  # 截断并不意味着任务完成

                    # 情况 3：真正 episode 结束（自然结束或其他终止）
                    else:
                        next_s = next_states[env_idx]
                        done_flag = True

                    memory.append((s, a, r, next_s, done_flag))

                states = next_states

                # 训练网络
                loss = train()
                if step_cnt % 10 == 0:
                    writer.add_scalar("loss/loss", loss, step_cnt)

            epsilon = max(epsilon_min, epsilon * epsilon_decay)

            if epoch % target_update_freq == 0:
                target_model.load_state_dict(model.state_dict())

            writer.add_scalar("epoch/epsilon", epsilon, epoch)
            print(f"Finished epoch#{epoch}")

    elif mode == "infer":
        load_checkpoint("./checkpoints/dqn_checkpoint_499.pth")
        state = single_env.reset()[0]
        total_reward = 0

        while True:
            single_env.render()
            action = select_action(state, 0)
            state, reward, done, _, _ = single_env.step(action[0])
            total_reward += reward
            if done:
                break
        print(f"Inference finished. Total reward: {total_reward}")

if __name__ == "__main__":
    # 初始化并行环境
    print("envpool version:", envpool.__version__)
    envs = envpool.make("CartPole-v1", num_envs=n_envs, seed=43,  env_type="gymnasium", batch_size=n_envs)
    main("train", envs)

```

cleanRL项目里有使用envpool：

```
https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_atari_envpool.py
https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_rnd_envpool.py
```

#### 3.7 开发库 Ray RLlib

**问了工业界的朋友，他们大规模生产中就是用Ray**。

Ray is an open source unified framework for scaling AI and Python applications. It provides a simple, universal API for building distributed applications that can scale from a laptop to a cluster. It includes:

1. Ray Data: Scalable Dataset for ML。分布式数据集
2. Ray Train: Distributed Model Training。分布式的模型训练
3. Ray Tune: Hyperparameter Tuning 。超参数搜索
4. Ray Serve: Scalable Model Serving。模型分布式部署用于推理
5. Ray RLlib: Industry-Grade Reinforcement Learning。强化学习库

具体到强化学习库RLlib:

![image-20250519114310130](img/RL/image-20250519114310130.png)

如上图，主要的概念有：

1. 算法
2. （算法）配置
3. EnvRunner，即分布式的环境交互，完成经验收集
4. Learner， 即分布式的agent训练，利用收集的经验进行agent的训练

#### 3.8 实操五：用RLlib训练DQN（CartPole任务）

很简单的从官方抄下来的几行代码，在windows下异常：启动后报错，又停不下来，只能完全退出pycharm。

```
2025-05-19 14:17:22,351	ERROR services.py:1362 -- Failed to start the dashboard , return code 3221226505
2025-05-19 14:17:22,351	ERROR services.py:1387 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure' to find where the log file is.
2025-05-19 14:17:22,352	ERROR services.py:1397 -- Couldn't read dashboard.log file. Error: 'utf-8' codec can't decode byte 0xbe in position 22: invalid start byte. It means the dashboard is broken even before it initializes the logger (mostly dependency issues). Reading the dashboard.err file which contains stdout/stderr.
2025-05-19 14:17:22,352	ERROR services.py:1431 -- 
The last 20 lines of C:\Users\11228\AppData\Local\Temp\ray\session_2025-05-19_14-17-18_283568_7704\logs\dashboard.err (it contains the error message from the dashboard): 
2025-05-19 14:17:22,511	INFO worker.py:1888 -- Started a local Ray instance.
[2025-05-19 14:17:24,403 E 7704 5736] core_worker.cc:513: Failed to register worker to Raylet: IOError: [RayletClient] Unable to register worker with raylet. Unknown error worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff

```

在linux下可以正常运行：

```
Episode complete:
  Steps taken: 72
  Total reward: 72.00
```

代码如下：

```python
import gymnasium as gym
import numpy as np
import torch
from typing import Dict, Tuple, Any, Optional
from ray.rllib.algorithms.dqn import DQNConfig

print("Setting up the DQN configuration...")
config = (
    DQNConfig().environment("CartPole-v1")
    .training( lr=1e-4 )
    # Parallelize environment rollouts for faster training.
    .env_runners(num_env_runners=3)
)

# Construct the actual PPO algorithm object from the config.
algo = config.build_algo()
rl_module = algo.get_module()


print("\nStarting training loop...")
for i in range(8):
    results = algo.train()

    # Log the metrics from training results
    print(f"Iteration {i+1}")
    print(f"  Training metrics: {results['env_runners']}")

# Save the trained algorithm (optional)
checkpoint_dir = algo.save()
print(f"\nSaved model checkpoint to: {checkpoint_dir}")

print("\nRunning inference with the trained policy...")
env = gym.make("CartPole-v1", render_mode="human")
# Get the initial observation (should be: [0.0] for the starting position).
obs, info = env.reset()
terminated = truncated = False
total_reward = 0.0
step_count = 0

# Play one episode and track the agent's trajectory
print("\nAgent trajectory:")

while not terminated and not truncated:
    # Compute an action given the current observation
    actions = rl_module.forward_inference(
        {"obs": torch.from_numpy(obs).unsqueeze(0)}
    )
    action = actions['actions'].item()

    # Log the agent's decision
    action_name = "LEFT" if action == 0 else "RIGHT"
    print(f"  Step {step_count}: Action: {action_name}")

    # Apply the computed action in the environment
    obs, reward, terminated, truncated, info = env.step(action)

    # Sum up rewards
    total_reward += reward
    step_count += 1

# Report final results
print(f"\nEpisode complete:")
print(f"  Steps taken: {step_count}")
print(f"  Total reward: {total_reward:.2f}")

```

#### 3.9 实操：只使用RLLib的EnvRunnerGroup组件

尝试只使用EnvRunnerGroup进行并发的与CartPole-v1环境进行交互、收集经验数据，用于off policy训练，例如DQN。AI也信心满满的写代码，但是写出来的代码运行总有问题。这也说明RLLib复杂又多变，否则AI不会这么难。

prompt如下：

```
我想使用RLLib库创建EnvRunnerGroup，实现并发的与CartPole-v1环境交互，收集经验数据存储到deque里，而不创建LearnerGroup等其他组件。只收集经验

请帮我实现这样的一个函数
```

有问题的代码如下：

```python
from collections import deque
import numpy as np
from ray.rllib.env.env_runner_group import EnvRunnerGroup
from ray.rllib.utils.filter_manager import FilterManager
from ray.rllib.policy.sample_batch import SampleBatch
from ray.rllib.algorithms.algorithm_config import AlgorithmConfig

def collect_experience(num_env_runners=4, num_steps=1000, max_deque_size=10000):
    """
    使用RLlib的EnvRunnerGroup收集CartPole-v1环境的经验数据

    参数:
        num_env_runners: 并发的环境运行器数量
        num_steps: 总共要收集的步骤数
        max_deque_size: 经验回放队列的最大大小
    """
    # 初始化经验回放队列
    experience_deque = deque(maxlen=max_deque_size)

    # 创建算法配置 - 使用新API并指定PyTorch框架
    config = (
        AlgorithmConfig()
        .environment("CartPole-v1")
        .framework("torch")  # 使用PyTorch
        .env_runners(num_env_runners=num_env_runners)  # 使用新API
    )

    # 初始化EnvRunnerGroup
    env_runner_group = EnvRunnerGroup(
        env_creator=lambda env_config: config.env_class(config.env_config),
        default_policy_class=None,  # 使用默认随机策略
        config=config,
        num_env_runners=num_env_runners,
        logdir=None,
    )

    # 初始化过滤器管理器
    filter_manager = FilterManager()

    # 重置所有环境运行器
    env_runner_group.foreach_env_runner(
        lambda w: w.try_reset("default_policy")
    )

    # 开始收集经验
    collected_steps = 0
    while collected_steps < num_steps:
        # 获取所有环境运行器的样本
        sample_batches = env_runner_group.foreach_env_runner(
            lambda w: w.sample()
        )

        # 处理收集到的样本
        for batch in sample_batches:
            # 将SampleBatch转换为字典形式以便处理
            batch_dict = {
                SampleBatch.OBS: batch[SampleBatch.OBS],
                SampleBatch.ACTIONS: batch[SampleBatch.ACTIONS],
                SampleBatch.REWARDS: batch[SampleBatch.REWARDS],
                SampleBatch.NEXT_OBS: batch[SampleBatch.NEXT_OBS],
                SampleBatch.DONES: batch[SampleBatch.DONES],
            }

            # 将经验添加到队列中
            for i in range(len(batch[SampleBatch.OBS])):
                experience = {
                    "obs": batch_dict[SampleBatch.OBS][i],
                    "action": batch_dict[SampleBatch.ACTIONS][i],
                    "reward": batch_dict[SampleBatch.REWARDS][i],
                    "next_obs": batch_dict[SampleBatch.NEXT_OBS][i],
                    "done": batch_dict[SampleBatch.DONES][i],
                }
                experience_deque.append(experience)
                collected_steps += 1

                if collected_steps >= num_steps:
                    break
            if collected_steps >= num_steps:
                break

    # 关闭环境运行器
    env_runner_group.shutdown()

    return experience_deque

# 使用示例
if __name__ == "__main__":
    experience_data = collect_experience(num_env_runners=4, num_steps=1000)
    print(f"收集到的经验数量: {len(experience_data)}")
    print("第一条经验示例:")
    print(experience_data[0])

```

### 4 有挑战的任务场景

#### 4.1 奖励稀疏的任务（Pong）

#### 4.2 奖励稀疏+多次动作（BasicMath）

### 5 自定义环境

#### 5.1 Gym的自定义环境

#### 5.2 SB3的自定义环境