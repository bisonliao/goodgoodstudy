## 强化学习入门（2）

由于文件太大会导致加载缓慢、编辑的时候卡顿，所以另外开一个文件，不在原来的强化学习入门.md文件上继续追加内容。

### 1、 高维像素输入

#### 1.1 要考虑的问题

**问题1：延迟奖励带来的虚假奖励问题**

以gym里的breakout（小球敲砖块）的小游戏来理解：

先不考虑跳帧，如下场景： 

1. agent逐帧对环境进行观察，做出动作选择，每一帧都对环境施加了动作，也就是每一帧都会调用env.step(action)。虽然很多时候的action移动托盘，但不会接触到球，没有真正影响到环境，当然这样的移动对于下一次托举球做准备是有意义的。 
2. 在t-5帧的时候，托盘很好的托住了球，球反弹飞向砖块，而达到砖块还需要时间 
3. 此后的8帧时间内，agent继续保持对环境的每一帧都施加一个动作，也就是调用step(action)，这段时间内托盘都与球不发生接触，还在飞行，step返回0作为reward。reward和action会保存在同一个元组tuple里 
4. 在t时刻也就是t帧的时候，agent又对环境执行了step(action)，同时环境里发生了球击中砖块的事件，所以这一次的step（）会返回1 作为reward。那么这个等于1的reward会记录在与这次action相同的tuple元组里，表示这个时候对环境施加了action同时发生了球击中砖块事件，虽然这个事情因果关系并不强烈（因为t-5时刻以后的action对于这次击中砖块是没有帮助的）
5. DQN等算法在利用上述过程中收集的样本，会回溯，以折扣回报的方式奖励 t-5时刻的动作。**但算法也会错误的奖励 t-3, t-4等时刻的action。**
6. replay buffer里随机打散和随机抽样样本的方式进行训练，会不会导致 t 时刻的值为1的奖励不能很好的回馈 t-5时刻的托举动作？ 不会，因为通过TD误差的传播，在某个批次，reward 1 会影响Q(t)值，在某个批次，Q(t)值会影响Q(t-1)，依次类推， reward 1会影响到 Q(t-5, 托举)
7. 但不得不说，**这样的错误归因和奖励，会导致训练的agent收敛慢很多，甚至不能收敛。**

我觉得解决方案可能有：

1. 需要修改奖励机制，例如托举到球，要奖励；击中砖块，要回溯奖励最近一次托举和上一次托举之间的动作
2. 层级式强化学习？（HRL）



**问题2：逐帧处理有什么问题？跳帧处理又有什么问题？**

逐帧处理的问题：

1. 逐帧处理的话，要求Agent的处理速度必须非常高，以60帧每秒的游戏刷新率为例，那就要求agent的策略网络/价值网络的计算和动作施加都要控制在1/60秒内
2. 很多动作是冗余的微小的。
3. 从模拟人玩游戏的角度来看，逐帧处理没有太多必要，决策不需要这么高频的去观察和采取动作
4. 相同的堆叠数下，不容易发现运动趋势，因为帧与帧之间的差异不够大

跳帧的设计，我理解有下面的问题： 

1. 可能错过关键时刻，例如小球飞过去了没有能够托住
2.  动作重复的方式，如果动作是增量的相对的，那么动作可能过量 
3. 可能错失奖励，例如某一帧小球击中了砖块，但是这一帧被跳过了



CleanRL开源项目的代码中breakout游戏的处理，是粗暴的跳过4帧、动作重复执行4次，返回最后两次的观测的最大值和4次reward的累加。

```python
    def step(self, action: int) -> AtariStepReturn:
        """
        Step the environment with the given action
        Repeat action, sum reward, and max over last observations.

        :param action: the action
        :return: observation, reward, terminated, truncated, information
        """
        total_reward = 0.0
        terminated = truncated = False
        for i in range(self._skip):
            obs, reward, terminated, truncated, info = self.env.step(action)
            done = terminated or truncated
            if i == self._skip - 2:
                self._obs_buffer[0] = obs
            if i == self._skip - 1:
                self._obs_buffer[1] = obs
            total_reward += float(reward)
            if done:
                break
        # Note that the observation on the done=True frame
        # doesn't matter
        max_frame = self._obs_buffer.max(axis=0)

        return max_frame, total_reward, terminated, truncated, info
```

![image-20250430151447951](img/RL/image-20250430151447951.png)

CleanRL中对环境的处理有很多有用的组件：

![image-20250430151902087](img/RL/image-20250430151902087.png)