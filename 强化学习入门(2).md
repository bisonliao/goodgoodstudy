## 强化学习入门（2）

由于文件太大会导致加载缓慢、编辑的时候卡顿，所以另外开一个文件，不在原来的强化学习入门.md文件上继续追加内容。

### 1、 高维像素输入

#### 1.1 要考虑的问题

**问题1：延迟奖励带来的虚假奖励问题**

以gym里的breakout（小球敲砖块）的小游戏来理解：

先不考虑跳帧，如下场景： 

1. agent逐帧对环境进行观察，做出动作选择，每一帧都对环境施加了动作，也就是每一帧都会调用env.step(action)。虽然很多时候的action移动托盘，但不会接触到球，没有真正影响到环境，当然这样的移动对于下一次托举球做准备是有意义的。 
2. 在t-5帧的时候，托盘很好的托住了球，球反弹飞向砖块，而达到砖块还需要时间 
3. 此后的8帧时间内，agent继续保持对环境的每一帧都施加一个动作，也就是调用step(action)，这段时间内托盘都与球不发生接触，还在飞行，step返回0作为reward。reward和action会保存在同一个元组tuple里 
4. 在t时刻也就是t帧的时候，agent又对环境执行了step(action)，同时环境里发生了球击中砖块的事件，所以这一次的step（）会返回1 作为reward。那么这个等于1的reward会记录在与这次action相同的tuple元组里，表示这个时候对环境施加了action同时发生了球击中砖块事件，虽然这个事情因果关系并不强烈（因为t-5时刻以后的action对于这次击中砖块是没有帮助的）
5. DQN等算法在利用上述过程中收集的样本，会回溯，以折扣回报的方式奖励 t-5时刻的动作。**但算法也会错误的奖励 t-3, t-4等时刻的action。**
6. replay buffer里随机打散和随机抽样样本的方式进行训练，会不会导致 t 时刻的值为1的奖励不能很好的回馈 t-5时刻的托举动作？ 不会，因为通过TD误差的传播，在某个批次，reward 1 会影响Q(t)值，在某个批次，Q(t)值会影响Q(t-1)，依次类推， reward 1会影响到 Q(t-5, 托举)
7. 但不得不说，**这样的错误归因和奖励，会导致训练的agent收敛慢很多，甚至不能收敛。**

我觉得解决方案可能有：

1. 需要修改奖励机制，例如托举到球，要奖励；击中砖块，要回溯奖励最近一次托举和上一次托举之间的动作
2. 层级式强化学习？（HRL）



**问题2：逐帧处理有什么问题？跳帧处理又有什么问题？**

逐帧处理的问题：

1. 逐帧处理的话，要求Agent的处理速度必须非常高，以60帧每秒的游戏刷新率为例，那就要求agent的策略网络/价值网络的计算和动作施加都要控制在1/60秒内
2. 很多动作是冗余的微小的，紧接着的帧冗余，没有意义，加大了运算负担。
3. 从模拟人玩游戏的角度来看，逐帧处理没有太多必要，决策不需要这么高频的去观察和采取动作
4. 相同的堆叠数下，不容易发现运动趋势，因为帧与帧之间的差异不够大

跳帧的设计，我理解有下面的问题： 

1. 可能错过关键时刻，例如小球飞过去了没有能够托住
2.  动作重复的方式，如果动作是增量的相对的，那么动作可能过量 
3. 可能错失奖励，例如某一帧小球击中了砖块，但是这一帧被跳过了



CleanRL开源项目的代码中breakout游戏的处理，是粗暴的跳过4帧、动作重复执行4次，返回最后两次的观测的最大值和4次reward的累加。

```python
    def step(self, action: int) -> AtariStepReturn:
        """
        Step the environment with the given action
        Repeat action, sum reward, and max over last observations.

        :param action: the action
        :return: observation, reward, terminated, truncated, information
        """
        total_reward = 0.0
        terminated = truncated = False
        for i in range(self._skip):
            obs, reward, terminated, truncated, info = self.env.step(action)
            done = terminated or truncated
            if i == self._skip - 2:
                self._obs_buffer[0] = obs
            if i == self._skip - 1:
                self._obs_buffer[1] = obs
            total_reward += float(reward)
            if done:
                break
        # Note that the observation on the done=True frame
        # doesn't matter
        max_frame = self._obs_buffer.max(axis=0)

        return max_frame, total_reward, terminated, truncated, info
```

![image-20250430151447951](img/RL/image-20250430151447951.png)

CleanRL中对环境的处理有很多有用的组件，用于包装环境：

![image-20250430151902087](img/RL/image-20250430151902087.png)

#### 1.2 实操1：马里奥跳过毒乌龟 / 马里奥跳吃金币

先从简单的场景入手，有一只毒乌龟左右移动，头顶悬着一些金币。马里奥可以跳起来得到金币，遇到乌龟要跳起来躲避他，否则就会受伤。

#### 1.3 实操2：breakout游戏

训练效果很好，经过5个小时1x3万个回合后，能够很稳定的弹球，见评估的[效果视频](img/RL/my_breakout.mp4)。

![image-20250520074803950](img/RL/image-20250520074803950.png)

代码如下：

```python
import os
import random

import numpy
import numpy as np
from collections import deque, namedtuple
from typing import List, Tuple, Optional


import ale_py
import os

from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack


import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime


# 设备配置 (优先使用CUDA)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class CNN_DQN(nn.Module):
    """处理像素输入的深度Q网络"""

    def __init__(self, stack_size: int, action_dim: int):
        super(CNN_DQN, self).__init__()
        self.conv1 = nn.Conv2d(stack_size, 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
        self.fc1 = nn.Linear(64 * 7 * 7, 512)
        self.fc2 = nn.Linear(512, action_dim)

        # 初始化权重
        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    m.bias.data.zero_()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """前向传播"""
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.contiguous().view(x.size(0), -1)  # 展平
        x = F.relu(self.fc1(x))
        return self.fc2(x)


class DQNAgent:
    """DQN智能体，处理像素输入"""

    def __init__(self, env, stack_size: int = 4, buffer_capacity: int = 10_000,
                 batch_size: int = 32, gamma: float = 0.99, lr: float = 1e-4,
                 tau: float = 1.0, initial_epsilon: float = 1.0,
                 final_epsilon: float = 0.01, num_episodes=100_000, n_envs=4):
        self.env = env
        self.n_envs = n_envs
        self.stack_size = stack_size
        self.action_dim = env.action_space.n
        print(f"action_dim:{self.action_dim}")
        self.batch_size = batch_size
        self.gamma = gamma
        self.tau = tau
        self.epsilon = initial_epsilon
        self.final_epsilon = final_epsilon
        self.num_episodes = num_episodes

        # 初始化网络
        self.policy_net = CNN_DQN(stack_size, self.action_dim).to(device)
        self.target_net = CNN_DQN(stack_size, self.action_dim).to(device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()  # 目标网络不训练

        # 优化器
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)

        # 经验回放
        self.memory = deque(maxlen=buffer_capacity)  # 10万个step就是11个GB

        # TensorBoard
        dt = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.writer = SummaryWriter(log_dir=f"logs/raw_dqn_{dt}")
        self.steps_done = 1

    def select_action(self, state: np.ndarray, evaluate: bool = False) -> int:

        """根据ε-greedy策略选择动作"""
        if not evaluate and random.random() < self.epsilon:
            result = numpy.ndarray((self.n_envs,), dtype=numpy.int32)
            for i in range(self.n_envs):
                result[i] = random.randint(0, self.action_dim - 1)
            return result

        # 转换为torch张量并添加batch维度
        state_tensor = torch.FloatTensor(state).to(device)
        assert state_tensor.shape.__len__() == 4

        with torch.no_grad():
            q_values = self.policy_net(state_tensor)

        return q_values.argmax(dim=1).numpy()

    def update_model(self) -> float:
        """更新策略网络"""
        if len(self.memory) < self.batch_size*100:
            return 0.0  # 如果经验不足，不更新

        # 从回放缓冲区采样


        batch = random.sample(self.memory, k=self.batch_size)
        s, a, s_, r, d = zip(*batch)

        # 转换为张量
        state_batch = torch.FloatTensor(np.array(s)).to(device)
        action_batch = torch.LongTensor(a).unsqueeze(1).to(device)
        next_state_batch = torch.FloatTensor(np.array(s_)).to(device)
        reward_batch = torch.FloatTensor(r).to(device)
        done_batch = torch.FloatTensor(d).to(device)

        # 计算当前Q值
        current_q = self.policy_net(state_batch).gather(1, action_batch)
        self.writer.add_scalar(f"steps/current_Q", current_q.mean().item(), self.steps_done)
        # 计算目标Q值
        with torch.no_grad():
            next_q = self.target_net(next_state_batch).max(1)[0].detach()
            target_q = reward_batch + (1 - done_batch) * self.gamma * next_q
        self.writer.add_scalar(f"steps/target_Q", target_q.mean().item(), self.steps_done)

        loss = F.mse_loss(current_q.squeeze(), target_q)

        # 反向传播
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)  # 新增梯度裁剪
        '''if self.steps_done % 10000 == 0:
            # 检查梯度是否存在
            for name, param in self.policy_net.named_parameters():
                if param.grad is None:
                    self.writer.add_scalar(f"grad/{name}", 0, self.steps_done)
                else:
                    self.writer.add_scalar(f"grad/{name}", param.grad.mean().item(), self.steps_done)'''
        self.optimizer.step()

        # 更新目标网络 (软更新)
        if self.steps_done % 1000 == 0:
            for target_param, policy_param in zip(self.target_net.parameters(),
                                                  self.policy_net.parameters()):
                target_param.data.copy_(
                    self.tau * policy_param.data + (1 - self.tau) * target_param.data
                )

        return loss.item()

    def decay_epsilon(self, episode):
        """衰减探索率"""
        stop_pointer = 5000
        decay_rate = (1.0 - self.final_epsilon) / stop_pointer  # 在1-stop pointer，epsilon就从1线性下降到final_epsilon
        if episode  < stop_pointer:
            self.epsilon = 1 - episode * decay_rate
        else:
            self.epsilon = self.final_epsilon

    def save_checkpoint(self, episode: int, total_reward, checkpoint_dir: str = "checkpoints"):
        """保存模型检查点"""
        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)

        checkpoint_path = os.path.join(checkpoint_dir, f"dqn_episode_{episode}_{total_reward}.pth")
        torch.save({
            'episode': episode,
            'policy_state_dict': self.policy_net.state_dict(),
            'target_state_dict': self.target_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'steps_done': self.steps_done
        }, checkpoint_path)

        # print(f"Checkpoint saved to {checkpoint_path}")

    def load_checkpoint(self, checkpoint_path: str):
        """加载模型检查点"""
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"Checkpoint file {checkpoint_path} not found")

        checkpoint = torch.load(checkpoint_path)
        self.policy_net.load_state_dict(checkpoint['policy_state_dict'])
        self.target_net.load_state_dict(checkpoint['target_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint['epsilon']
        self.steps_done = checkpoint['steps_done']

        print(f"Checkpoint loaded from {checkpoint_path}, "
              f"resuming from episode {checkpoint['episode']}")

        return checkpoint['episode']



    def train(self, checkpoint_interval: int = 1000,
              checkpoint_dir: str = "checkpoints", resume_checkpoint: Optional[str] = None):
        """训练循环"""
        start_episode = 0

        # 如果提供了检查点，从中恢复
        if resume_checkpoint is not None:
            start_episode = self.load_checkpoint(resume_checkpoint) + 1


        episode_cnt = 0
        max_episode_len = -1
        while episode_cnt < 10_000:
            states = self.env.reset()
            states = states.transpose(0, 3, 1, 2)
            total_reward = 0
            episode_len = 1e-4
            for _ in range(300):
                actions = self.select_action(states)
                next_states, rewards, dones, infos = self.env.step(actions)
                next_states = next_states.transpose(0, 3, 1, 2)
                self.steps_done += 1
                if dones[0]:
                    # 处理episode_len数据
                    episode_cnt += 1
                    self.writer.add_scalar("episode/episode_len", episode_len, episode_cnt)
                    self.writer.add_scalar('episode/total_reward', total_reward, episode_cnt)
                    if episode_len >= max_episode_len:  # 等于也保存，觉得后面训练时间更长，可能效果更好
                        max_episode_len = episode_len
                        self.save_checkpoint(episode_cnt, episode_len, "./checkpoints")
                    episode_len = 1e-4
                    total_reward = 0
                else:
                    episode_len += 1
                    total_reward += rewards[0]

                # 经验回放缓存
                for env_idx in range(self.n_envs):
                    if dones[env_idx]:
                        if infos[env_idx]["TimeLimit.truncated"]:  # 被截断了，不是真的结束，那么这个时间步的done flag应该为false
                            self.memory.append(
                                (states[env_idx], actions[env_idx], next_states[env_idx], rewards[env_idx], False))
                        else:  # 是真的一个回合结束了，由于环境自动reset，所以next_states并不是真的随后一个状态，而是reset后下一回合的新状态
                            next_stt = infos[env_idx]["terminal_observation"]
                            next_stt = next_stt.transpose(2, 0, 1)
                            self.memory.append( (states[env_idx], actions[env_idx],  next_stt, rewards[env_idx],  dones[env_idx]) )
                    else:
                        self.memory.append(
                            (states[env_idx], actions[env_idx], next_states[env_idx], rewards[env_idx], dones[env_idx]))

                states = next_states

                # 更新模型
                if  self.steps_done % 4 == 0:
                    loss = self.update_model()
                    self.writer.add_scalar('episode/loss', loss, self.steps_done)


            # 衰减探索率
            self.decay_epsilon(episode_cnt)

            # 记录到TensorBoard


            self.writer.add_scalar('episode/epsilon', self.epsilon, episode_cnt)


            # 定期保存检查点
            if (episode_cnt + 1) % checkpoint_interval == 0:
                self.save_checkpoint(episode_cnt, 0, checkpoint_dir)

        self.writer.close()

    def evaluate(self, vec_env, num_episodes: int = 10, checkpoint_path: Optional[str] = None):
        """评估模型性能"""
        # 使用 render_mode="human" ，再单独调用 env.render() 不能获取 RGB 数组

        if checkpoint_path is not None:
            self.load_checkpoint(checkpoint_path)


        total_rewards = []

        for episode in range(num_episodes):
            states = vec_env.reset()
            states = states.transpose(0, 3, 1, 2)
            total_reward = 0
            done = False

            while not done:
                # 选择动作 (评估时不使用探索)
                actions = self.select_action(states, evaluate=True)

                states, rewards, dones, _ = vec_env.step(actions)
                done = dones[0]
                states = states.transpose(0, 3, 1, 2)
                total_reward += rewards[0]
                vec_env.render("human")

            total_rewards.append(total_reward)
            print(f"Evaluation Episode {episode + 1}, Reward: {total_reward:.1f}")

        avg_reward = np.mean(total_rewards)
        print(f"Average reward over {num_episodes} episodes: {avg_reward:.1f}")
        return avg_reward


if __name__ == "__main__":

    # 创建环境 (训练时不渲染)
    n_envs = 3
    vec_env = make_atari_env('BreakoutNoFrameskip-v4', n_envs=n_envs, seed=0) #这里会进行灰度化和裁剪为 84x84
    # Frame-stacking with 4 frames
    vec_env = VecFrameStack(vec_env, n_stack=4)

    # 初始化智能体
    agent = DQNAgent(vec_env, n_envs=n_envs)

    agent.train(checkpoint_interval=500, checkpoint_dir="./checkpoints")
    agent.evaluate(vec_env, 10,"./checkpoints/dqn_episode_9226_299.0001.pth")

```



### 2 机器人手臂的仿真模拟环境

![image-20250501200451197](img/RL/image-20250501200451197.png)

### 3 高并发环境的并行仿真

并行化环境交互是强化学习**提速**、**稳健**与**可扩展**的基础手段：

1. 提升样本吞吐，缩短调参/收敛时间。尤其对于交互时延很大/计算量很大的环境
2. 打散数据相关性，稳定梯度估计
3. 更好地利用 CPU/GPU 资源，例如可以异步的同时进行样本收集（侧重CPU+网络）和网络更新（侧重GPU）
4. 支撑异步、大规模分布式算法。例如当下很火的LLM，每次交互时延/计算量都很大，为了高效收集样本数据，必须进行分布式部署

快速研究了一下三个并发库，结论是SB3很好，其他两个有或多或少的问题，不好用。

```
# stable_baseline3
https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html

# ray rllib
https://docs.ray.io/en/latest/cluster/getting-started.html

# envpool
https://envpool.readthedocs.io/en/latest/
```

#### 3.1 开发库 Stable-Baselines3

[SB3](https://stable-baselines3.readthedocs.io/en/master/index.html)是一个基于 PyTorch 的强化学习（RL）库，提供可靠且易于使用的 RL 算法实现，特性有：

1. 实现了多种常用的强化学习算法，所有算法遵循统一的接口和代码结构，便于快速上手和切换不同算法
2. 通过向量化环境（VecEnv）支持并行训练，加速实验过程
3. 提供详细的用户指南、API 参考和教程；充分测试（95%的覆盖率）
4. RL Baselines3 Zoo提供预训练的代理、超参数调优脚本、评估工具和结果可视化
5. 支持用户定义自己的特征提取器和网络结构，满足特定需求；提供灵活的回调机制，便于在训练过程中插入自定义逻辑，如早停、模型保存等
6. atari环境的wrappers，例如ClipRewardEnv / EpisodicLifeEnv / FireResetEnv等， 还有各种工具类，让开发变得简单方便。



思考：

stable-baseline3提供了对很多RL算法的封装，对于典型的gym 游戏的训练只需要3-5行代码，极其方便：

1. 从知其所以然的角度来看，封装的太好不利于我这样的初学者洞悉背后的细节，很多时候我希望手搓代码
2. 但是从有一个好的开始而提振信心（尤其是对于高维像素输入方式下的游戏agent训练）来说，有一个先成功跑起来的代码是有较大价值的。下面的3.4就让人很激动
3. 另外，研究中用作实验对比的基准，需要一个可靠的RL算法x环境的实现。毕竟RL很玄学，有时候即使训练出来一个可以work的agent，也拿不准是不是充分收敛的agent



**具体到SB3中提供的VecEnv来实现并行多环境交互：**

VecEnv有两个具体实现：DummyVecEnv和SubprocVecEnv，前者实际上是串行的在多个env上执行交互，后者才是真正的多进程并发。

使用make_vec_env()函数可以创建VecEnv并明确指定具体的实现，也可以直接调用他们的构造函数。

```python
vec_env = make_vec_env(env_id='CartPole-v1', n_envs=4, vec_env_cls=SubprocVecEnv)
```

VecEnv的使用注意事项：

![image-20250516123835826](img/RL/image-20250516123835826.png)

根据上面的文档，**回合终止的处理需要额外注意**：

1. 当一个 episode 正常终止（terminated），我们就知道它的最终结果，比如 agent 掉落悬崖，得了 -100 分 —— 这是明确的，我们就不需要估计接下来的价值（因为 episode 已经结束，没有下一步）。但如果 episode 是因为 **时间限制（timeout）被截断（truncated）**，那其实 agent 可能还没死、还可以继续获得 reward，只是我们强制停止了。在这种情况下：
   1. 我们**不能简单地认为 episode 到此就结束了**；
   2. 所以我们**应当用当前状态的值函数来估算未来**，这就是 bootstrap。
2. 另外，当一个回合终止后，由于并行环境会自动reset，那么step函数返回的state其实是reset后新的回合的开始，并非上一个回合真的最后一个state，所以要通过infos字段来获得真正的最后一个state，放入到replaybuffer里。



#### 3.2 实操一：用VecEnv+手搓DQN实现并行交互

手搓代码总是让人更好的理解背后的机制，下面的代码能够取得不错的训练效果：

![image-20250516121322765](img/RL/image-20250516121322765.png)

```
Inference finished. Total reward: 369.0
```

代码如下：

```python
import random
import gym
import numpy
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from collections import deque
import argparse
import os
from stable_baselines3.common.vec_env import SubprocVecEnv
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime as dt

# 设备选择
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
writer = SummaryWriter(f"logs/parrallel_{dt.now().strftime('%y%m%d_%H%M%S')}")

# 超参数
gamma = 0.99  # 折扣因子
epsilon = 1.0  # 初始探索率
epsilon_min = 0.01  # 最低探索率
epsilon_decay = 0.95  # 探索率衰减
learning_rate = 1e-3  # 学习率
batch_size = 64  # 经验回放的批量大小
memory_size = 100_000  # 经验池大小
target_update_freq = 10  # 目标网络更新频率
n_envs = 4

# 创建一个返回 CartPole 环境的函数（每个子环境都需要一个独立实例）
def make_env(seed, index):
    def _init():
        env = gym.make("CartPole-v1", max_episode_steps=None) #max_episode_steps这样修改没有用，内部还是500步最大，需要继续研究
        return env
    return _init

single_env = gym.make("CartPole-v1", render_mode="human")
n_state = single_env.observation_space.shape[0]  # 状态维度
n_action = single_env.action_space.n  # 动作数量

# DQN 网络定义
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)


# 初始化网络
model = DQN(n_state, n_action).to(device)
target_model = DQN(n_state, n_action).to(device)
target_model.load_state_dict(model.state_dict())
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
memory = deque(maxlen=memory_size)


def select_action(states:numpy.ndarray, epsilon):

    """基于 ε-greedy 选择动作"""
    if random.random() < epsilon:
        actions = torch.zeros(states.shape[0], dtype=torch.long).to(device)
        for i in range(states.shape[0]):
            actions[i] = random.randint(0, n_action - 1)
        return   actions.cpu().numpy()
    else:
        states = torch.tensor(states).to(device)
        if states.shape.__len__() < 2:
            states = states.unsqueeze(0)
        return model(states).argmax(1).cpu().numpy()  # 选取 Q 值最大的动作


def train():
    if len(memory) < batch_size*10:
        return 99.0  # 经验池数据不足时不训练

    batch = random.sample(memory, batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)

    states = torch.FloatTensor(states).to(device)  # (batch_size, 4)
    actions = torch.LongTensor(actions).unsqueeze(1).to(device)  # (batch_size,) -> (batch_size, 1)
    rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)  # (batch_size,) -> (batch_size, 1)
    next_states = torch.FloatTensor(next_states).to(device)  # (batch_size, 4)
    dones = torch.FloatTensor(dones).unsqueeze(1).to(device)  # (batch_size,) -> (batch_size, 1)

    # 计算当前 Q 值
    q_values = model(states).gather(1, actions)  # 从 Q(s, a) 选取执行的动作 Q 值

    # 计算目标 Q 值
    next_q_values = target_model(next_states).max(1, keepdim=True)[0]  # 选取 Q(s', a') 的最大值
    target_q_values = rewards + gamma * next_q_values * (1 - dones)  # TD 目标

    # 计算损失
    loss = F.mse_loss(q_values, target_q_values.detach())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()


def save_checkpoint(id):
    path=f"checkpoints/dqn_checkpoint_{id}.pth"
    torch.save(model.state_dict(), path)
    print(f"Checkpoint saved to {path}")


def load_checkpoint(path):
    if os.path.exists(path):
        model.load_state_dict(torch.load(path, map_location=device))
        print(f"Checkpoint loaded from {path}")
    else:
        print("No checkpoint found, starting from scratch.")


def main(mode, env):
    global epsilon

    if mode == "train":
        epoches = 500
        states = env.reset() 
        episode_len = 0 #跟踪0号环境的回合长度
        episode_cnt = 0 # 跟踪0号环境的回合个数
        step_cnt = 0 #总的交互次数，没有乘以环境并发数
        max_episode_len = -1 #跟踪0号环境最大的回合长度
        for epoch in range(epoches):
            # 本来是可以一直执行下去，因为并行环境会自动reset，但这里做一个控制：
            # 每300步要跳出来做一点其他事情，例如更新epsilon。
            # 300这个参数很关键的，直接影响epsilon的节奏
            for _ in range(300):
                actions = select_action(states, epsilon)
                next_states, rewards, dones, infos = env.step(actions) 
                step_cnt += 1
                if dones[0]:
                    # 处理episode_len数据
                    episode_cnt += 1
                    writer.add_scalar("a_train/episode_len", episode_len, episode_cnt) #加上a_让这个排在前面
                    if episode_len >= max_episode_len: #等于也保存，觉得后面训练时间更长，可能效果更好
                        max_episode_len = episode_len
                        save_checkpoint(max_episode_len)
                        if  episode_len == 499:
                            print("reach 499")
                    episode_len = 0
                else:
                    episode_len += 1

                # 经验回放缓存
                for env_idx in range(n_envs):
                    if not dones[env_idx]:
                        memory.append((states[env_idx], actions[env_idx], rewards[env_idx], next_states[env_idx], dones[env_idx]))
                        continue
                    if infos[env_idx]["TimeLimit.truncated"]: #被截断了，不是真的结束，那么这个时间步的done flag应该为false
                        memory.append( (states[env_idx], actions[env_idx], rewards[env_idx], next_states[env_idx], False))
                    else: # 是真的一个回合结束了，由于环境自动reset，所以next_states并不是真的随后一个状态，而是reset后下一回合的新状态
                        next_stt = infos[env_idx]["terminal_observation"] 
                        memory.append( (states[env_idx], actions[env_idx], rewards[env_idx], next_stt, dones[env_idx]))

                states = next_states


                # 训练 DQN
                loss = train()
                if step_cnt % 10 == 0:
                    writer.add_scalar("loss/loss", loss, step_cnt)

            # 逐步降低 epsilon，减少随机探索，提高利用率
            epsilon = max(epsilon_min, epsilon * epsilon_decay)

            # 定期更新目标网络，提高稳定性
            if epoch % target_update_freq == 0:
                target_model.load_state_dict(model.state_dict())



            writer.add_scalar("epoch/epsilon", epsilon, epoch)
            print(f"Finished epoch#{epoch}")

    elif mode == "infer":
        load_checkpoint("./checkpoints/dqn_checkpoint_499.pth")
        state = single_env.reset()
        state = state[0]
        total_reward = 0

        while True:
            single_env.render()
            action = select_action(state, 0)  # 纯利用，epsilon=0
            state, reward, done, _, _ = single_env.step(action[0])
            total_reward += reward

            if done:
                break

        print(f"Inference finished. Total reward: {total_reward}")


if __name__ == "__main__":
    # 使用 SubprocVecEnv 实现多进程并行环境（注意：在 Windows 中建议使用 if __name__ == "__main__"）
    env_fns = [make_env(seed=i, index=i) for i in range(n_envs)]
    envs = SubprocVecEnv(env_fns)
    main("train", envs)
```

上面的SubprocVecEnv的创建方式默认有限制一个回合的最大步数为500，也可以通过下面的代码去掉：

```python
import gym
from gym.wrappers import TimeLimit
from stable_baselines3.common.env_util import make_vec_env

def make_unlimited_cartpole():
    env = gym.make("CartPole-v1")
    # 如果环境被 TimeLimit 包装，移除它
    if isinstance(env, TimeLimit):
        env = env.env  # unwrap TimeLimit
    return env

# 使用 make_vec_env 构造并行环境
envs = make_vec_env(make_unlimited_cartpole, n_envs=4)

```

这样果然会出现回合长度超过10000的回合。

#### 3.3 实操二：直接用SB3训练DQN（CartPole）

感觉上面3.2的自己手搓的代码还不够好，因为训练的回合长度没有一路单调增长，中间波动很大，但又不知道问题出在哪里。

所以直接用SB3这样的专家代码来训练DQN，发现回合平均长度也波动很大。

下面代码直接使用SB3库解决CartPole任务，可以看到回合平均长度波动很大且后面都变得很糟糕。但通过及时的保存best model，得到的模型在评估中表现很好。

![image-20250517154356656](img/RL/image-20250517154356656.png)

评估时候输出的回合长度：

```
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
episode len:500
```

代码：

```python
from stable_baselines3 import DQN
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback


eval_env = make_vec_env("CartPole-v1", n_envs=1)
# 创建评估回调
eval_callback = EvalCallback(
    eval_env,
    best_model_save_path="./best_model/",
    log_path="./logs/eval/",
    eval_freq=50_000,                  # 每隔多少步评估一次
    n_eval_episodes=5,                 # 每次评估使用多少个 episode
    deterministic=True,
    render=False,
    verbose=1                          # 日志显示
)

if __name__ == "__main__":
    #下面函数默认是创建DummyVecEnv，不是真的并行，除非明确指定vec_env_cls=SubprocVecEnv，但对于cartpole这样简单的
    # 环境，后者反而更慢，因为增加了额外的进程间通信开销
    vec_env = make_vec_env(env_id='CartPole-v1', n_envs=4,vec_env_cls=SubprocVecEnv)
    model = DQN("MlpPolicy", vec_env,
                    verbose=1,
                    tensorboard_log="logs") #这里可以写很多参数，包括replaybuffer大小、衰减率、学习率等等
    model.learn(total_timesteps=5_000_000, callback=eval_callback)
    del model

    model = DQN.load("./best_model/best_model.zip")

    obs = vec_env.reset()
    for i in range(10):
        done = False
        episode_len = 0
        while not done:
            action, _states = model.predict(obs, deterministic=False)
            obs, rewards, dones, info = vec_env.step(action)
            done = dones[0]
            episode_len += 1
            vec_env.render("human")
        print(f"episode len:{episode_len}")

```



#### 3.4 实操三：用SB3库搞定像素输入（Breakout）

效果很好，可以说是惊艳，可以上分100多，见[视频](img/RL/SB3_breakout.mp4)：

```
一开始：
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 703      |
|    ep_rew_mean      | 1.13     |
|    exploration_rate | 0.923    |
| time/               |          |
|    episodes         | 1000     |
|    fps              | 475      |
|    time_elapsed     | 68       |
|    total_timesteps  | 32512    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00183  |
|    n_updates        | 1406     |
----------------------------------
经过4个小时后如下:
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 5.22e+03 |
|    ep_rew_mean      | 69.9     |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 27000    |
|    fps              | 295      |
|    time_elapsed     | 13172    |
|    total_timesteps  | 3898952  |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00214  |
|    n_updates        | 243059   |
----------------------------------
```



代码如下（来自SB3文档）：

```python
import gymnasium as gym
from stable_baselines3 import DQN
from stable_baselines3.common.atari_wrappers import AtariWrapper
from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv
import ale_py

# There already exists an environment generator
# that will make and wrap atari environments correctly.
# Here we are also multi-worker training (n_envs=4 => 4 environments)
vec_env = make_atari_env("BreakoutNoFrameskip-v4", n_envs=4, seed=0)
# Frame-stacking with 4 frames
vec_env = VecFrameStack(vec_env, n_stack=4)

# 使用 CNN 网络进行训练
model = DQN("CnnPolicy", vec_env, verbose=1, buffer_size=10_000, learning_rate=1e-4, batch_size=32,
            learning_starts=10_000, target_update_interval=1_000, train_freq=4)

# 训练 100k steps（可以增加）
model.learn(total_timesteps=4_000_000, log_interval=1_000)

# 保存模型
model.save("dqn_breakout")
del model

model = DQN.load("dqn_breakout")

obs = vec_env.reset()
while True:
    action, _states = model.predict(obs, deterministic=False)
    obs, rewards, dones, info = vec_env.step(action)
    vec_env.render("human")

```

#### 3.5 开发库 EnvPool（不好用有bug）

Here are EnvPool’s several highlights:

- Compatible with OpenAI `gym` APIs and DeepMind `dm_env` APIs;
- Manage a pool of envs, interact with the envs in batched APIs by default;
- Support both synchronous execution and asynchronous execution;
- Support both single player and multi-player environment;
- Easy C++ developer API to add new envs: [Customized C++ environment integration](https://envpool.readthedocs.io/en/latest/content/new_env.html);
- High performance
  - Free **~2x** speedup with only single environment;
  - **1 Million** Atari frames / **3 Million** Mujoco steps per second simulation with 256 CPU cores, **~20x** throughput of Python subprocess-based vector env;
  - **~3x** throughput of Python subprocess-based vector env on low resource setup like 12 CPU cores;
- Compatible and general
  - XLA support with JAX jit function;
  - Comparing with the existing GPU-based solution ([Brax](https://github.com/google/brax) / [Isaac-gym](https://developer.nvidia.com/isaac-gym)), EnvPool is a **general** solution for various kinds of speeding-up RL environment parallelization;
  - Compatible with some existing RL libraries, e.g., [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3), [Tianshou](https://github.com/thu-ml/tianshou), [ACME](https://github.com/deepmind/acme), [CleanRL](https://github.com/vwxyzjn/cleanrl) ([Solving Pong in 5 mins](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#solving-pong-in-5-minutes-with-ppo--envpool)), [rl_games](https://github.com/Denys88/rl_games) (2 mins [Pong](https://colab.research.google.com/drive/1iWFv0g67mWqJONoFKNWUmu3hdxn_qUf8?usp=sharing), 15 mins [Breakout](https://colab.research.google.com/drive/1U_NxL6gSs0yRVhfl0cKl9ttRmcmMCiBS?usp=sharing), 5 mins [Ant](https://colab.research.google.com/drive/1C9yULxU_ahQ_i6NUHCvOLoeSwJovQjdz?usp=sharing) and [HalfCheetah](https://colab.research.google.com/drive/1bser52bpItzmlME00IA0bbmPdp1Xm0fy?usp=sharing)).

**envpool比较操蛋，果断弃用**：

1. 不支持windows，只支持类unix环境。
2. 文档几乎没有，找个想要的示例代码都没有
3. 大模型对于envpool的问题，几乎回答不了，说明envpool在网上的信息很少或者质量不行
4. 下面3.6的代码，发现envpool有bug，8个环境reset出来都是一样的，env_id都是0，step为每个环境输入的action不一样得到的next_obs也是一样的。换Pool-v5也这样



#### 3.6 实操四：用EnvPool+手搓DQN实现并行交互

下面的代码在额外购买的linux云主机上可以跑，但实际上只有一个环境，envpool有bug

官方代码库还有很多[没有关闭的Bug Issue](https://github.com/sail-sg/envpool/issues)：

代码如下：

```python
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime as dt
import os
import envpool  
import gym

# 设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
writer = SummaryWriter(f"logs/envpool_{dt.now().strftime('%y%m%d_%H%M%S')}")

# 超参数
gamma = 0.99
epsilon = 1.0
epsilon_min = 0.01
epsilon_decay = 0.95
learning_rate = 1e-3
batch_size = 64
memory_size = 100_000
target_update_freq = 10
n_envs = 8

# 单个环境用于推理
single_env = gym.make("CartPole-v1", render_mode="human")
n_state = single_env.observation_space.shape[0]
n_action = single_env.action_space.n

# 网络结构
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

# 初始化模型
model = DQN(n_state, n_action).to(device)
target_model = DQN(n_state, n_action).to(device)
target_model.load_state_dict(model.state_dict())
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
memory = deque(maxlen=memory_size)

# ε-greedy 动作选择
def select_action(states: np.ndarray, epsilon):
    if random.random() < epsilon:
        actions = np.random.randint(0, n_action, size=states.shape[0])
        return actions
    else:
        states = torch.tensor(states, dtype=torch.float32).to(device)
        if states.ndim == 1:
            states = states.unsqueeze(0)
        with torch.no_grad():
            return model(states).argmax(1).cpu().numpy()

# 训练过程
def train():
    if len(memory) < batch_size * 10:
        return 99.0

    batch = random.sample(memory, batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)

    states = torch.FloatTensor(states).to(device)
    actions = torch.LongTensor(actions).unsqueeze(1).to(device)
    rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)
    next_states = torch.FloatTensor(next_states).to(device)
    dones = torch.FloatTensor(dones).unsqueeze(1).to(device)

    q_values = model(states).gather(1, actions)
    next_q_values = target_model(next_states).max(1, keepdim=True)[0]
    target_q_values = rewards + gamma * next_q_values * (1 - dones)

    loss = F.mse_loss(q_values, target_q_values.detach())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()

# 保存模型
def save_checkpoint(id):
    os.makedirs("checkpoints", exist_ok=True)
    path = f"checkpoints/dqn_checkpoint_{id}.pth"
    torch.save(model.state_dict(), path)
    print(f"Checkpoint saved to {path}")

# 加载模型
def load_checkpoint(path):
    if os.path.exists(path):
        model.load_state_dict(torch.load(path, map_location=device))
        print(f"Checkpoint loaded from {path}")
    else:
        print("No checkpoint found, starting from scratch.")

# 主函数
def main(mode, env):
    global epsilon

    if mode == "train":
        epoches = 500
        obs = env.reset()
        print("reset returns:", obs)
        states = obs[0]
        episode_len = 0
        episode_cnt = 0
        step_cnt = 0
        max_episode_len = -1

        for epoch in range(epoches):
            for _ in range(300):
                actions = select_action(states, epsilon)
                if step_cnt == 0: print("actions:", actions)
                next_obs, rewards, dones, truncs, infos = env.step(actions)
                next_states = next_obs
                if step_cnt == 0: print('Next_states:', next_states)
                step_cnt += 1

                # 处理0号环境的episode长度
                if dones[0] or truncs[0]:
                    episode_cnt += 1
                    writer.add_scalar("a_train/episode_len", episode_len, episode_cnt)
                    if episode_len >= max_episode_len:
                        max_episode_len = episode_len
                        save_checkpoint(max_episode_len)
                        if episode_len == 499:
                            print("reach 499")
                    episode_len = 0
                else:
                    episode_len += 1

                # 存入经验池，按环境逐一处理
                for env_idx in range(n_envs):
                    s = states[env_idx]
                    a = actions[env_idx]
                    r = rewards[env_idx]

                    # 情况 1：未终止也未截断，正常步
                    if not dones[env_idx] and not truncs[env_idx]:
                        next_s = next_states[env_idx]
                        done_flag = False

                    # 情况 2：被截断（如超时终止），使用 terminal_observation
                    elif truncs[env_idx] and "terminal_observation" in infos[env_idx]:
                        next_s = infos[env_idx]["terminal_observation"]
                        print(">>>>>>>>>>", next_s.shape)
                        done_flag = False  # 截断并不意味着任务完成

                    # 情况 3：真正 episode 结束（自然结束或其他终止）
                    else:
                        next_s = next_states[env_idx]
                        done_flag = True

                    memory.append((s, a, r, next_s, done_flag))

                states = next_states

                # 训练网络
                loss = train()
                if step_cnt % 10 == 0:
                    writer.add_scalar("loss/loss", loss, step_cnt)

            epsilon = max(epsilon_min, epsilon * epsilon_decay)

            if epoch % target_update_freq == 0:
                target_model.load_state_dict(model.state_dict())

            writer.add_scalar("epoch/epsilon", epsilon, epoch)
            print(f"Finished epoch#{epoch}")

    elif mode == "infer":
        load_checkpoint("./checkpoints/dqn_checkpoint_499.pth")
        state = single_env.reset()[0]
        total_reward = 0

        while True:
            single_env.render()
            action = select_action(state, 0)
            state, reward, done, _, _ = single_env.step(action[0])
            total_reward += reward
            if done:
                break
        print(f"Inference finished. Total reward: {total_reward}")

if __name__ == "__main__":
    # 初始化并行环境
    print("envpool version:", envpool.__version__)
    envs = envpool.make("CartPole-v1", num_envs=n_envs, seed=43,  env_type="gymnasium", batch_size=n_envs)
    main("train", envs)

```

cleanRL项目里有使用envpool：

```
https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_atari_envpool.py
https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_rnd_envpool.py
```

#### 3.7 开发库 Ray RLlib

Ray is an open source unified framework for scaling AI and Python applications. It provides a simple, universal API for building distributed applications that can scale from a laptop to a cluster. It includes:

1. Ray Data: Scalable Dataset for ML。分布式数据集
2. Ray Train: Distributed Model Training。分布式的模型训练
3. Ray Tune: Hyperparameter Tuning 。超参数搜索
4. Ray Serve: Scalable Model Serving。模型分布式部署用于推理
5. Ray RLlib: Industry-Grade Reinforcement Learning。强化学习库

具体到强化学习库RLlib:

![image-20250519114310130](img/RL/image-20250519114310130.png)

如上图，主要的概念有：

1. 算法
2. （算法）配置
3. EnvRunner，即分布式的环境交互，完成经验收集
4. Learner， 即分布式的agent训练，利用收集的经验进行agent的训练

#### 3.8 实操五：用RLlib训练DQN（CartPole任务）

很简单的从官方抄下来的几行代码，在windows下异常：启动后报错，又停不下来，只能完全退出pycharm。

```
2025-05-19 14:17:22,351	ERROR services.py:1362 -- Failed to start the dashboard , return code 3221226505
2025-05-19 14:17:22,351	ERROR services.py:1387 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure' to find where the log file is.
2025-05-19 14:17:22,352	ERROR services.py:1397 -- Couldn't read dashboard.log file. Error: 'utf-8' codec can't decode byte 0xbe in position 22: invalid start byte. It means the dashboard is broken even before it initializes the logger (mostly dependency issues). Reading the dashboard.err file which contains stdout/stderr.
2025-05-19 14:17:22,352	ERROR services.py:1431 -- 
The last 20 lines of C:\Users\11228\AppData\Local\Temp\ray\session_2025-05-19_14-17-18_283568_7704\logs\dashboard.err (it contains the error message from the dashboard): 
2025-05-19 14:17:22,511	INFO worker.py:1888 -- Started a local Ray instance.
[2025-05-19 14:17:24,403 E 7704 5736] core_worker.cc:513: Failed to register worker to Raylet: IOError: [RayletClient] Unable to register worker with raylet. Unknown error worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff

```

在linux下可以正常运行：

```
Episode complete:
  Steps taken: 72
  Total reward: 72.00
```

代码如下：

```python
import gymnasium as gym
import numpy as np
import torch
from typing import Dict, Tuple, Any, Optional
from ray.rllib.algorithms.dqn import DQNConfig

print("Setting up the DQN configuration...")
config = (
    DQNConfig().environment("CartPole-v1")
    .training( lr=1e-4 )
    # Parallelize environment rollouts for faster training.
    .env_runners(num_env_runners=3)
)

# Construct the actual PPO algorithm object from the config.
algo = config.build_algo()
rl_module = algo.get_module()


print("\nStarting training loop...")
for i in range(8):
    results = algo.train()

    # Log the metrics from training results
    print(f"Iteration {i+1}")
    print(f"  Training metrics: {results['env_runners']}")

# Save the trained algorithm (optional)
checkpoint_dir = algo.save()
print(f"\nSaved model checkpoint to: {checkpoint_dir}")

print("\nRunning inference with the trained policy...")
env = gym.make("CartPole-v1", render_mode="human")
# Get the initial observation (should be: [0.0] for the starting position).
obs, info = env.reset()
terminated = truncated = False
total_reward = 0.0
step_count = 0

# Play one episode and track the agent's trajectory
print("\nAgent trajectory:")

while not terminated and not truncated:
    # Compute an action given the current observation
    actions = rl_module.forward_inference(
        {"obs": torch.from_numpy(obs).unsqueeze(0)}
    )
    action = actions['actions'].item()

    # Log the agent's decision
    action_name = "LEFT" if action == 0 else "RIGHT"
    print(f"  Step {step_count}: Action: {action_name}")

    # Apply the computed action in the environment
    obs, reward, terminated, truncated, info = env.step(action)

    # Sum up rewards
    total_reward += reward
    step_count += 1

# Report final results
print(f"\nEpisode complete:")
print(f"  Steps taken: {step_count}")
print(f"  Total reward: {total_reward:.2f}")

```

#### 3.9 实操：只使用RLLib的EnvRunnerGroup组件

尝试只使用EnvRunnerGroup进行并发的与CartPole-v1环境进行交互、收集经验数据，用于off policy训练，例如DQN。AI也信心满满的写代码，但是写出来的代码运行总有问题。这也说明RLLib复杂又多变，否则AI不会这么难。

prompt如下：

```
我想使用RLLib库创建EnvRunnerGroup，实现并发的与CartPole-v1环境交互，收集经验数据存储到deque里，而不创建LearnerGroup等其他组件。只收集经验

请帮我实现这样的一个函数
```

有问题的代码如下：

```python
from collections import deque
import numpy as np
from ray.rllib.env.env_runner_group import EnvRunnerGroup
from ray.rllib.utils.filter_manager import FilterManager
from ray.rllib.policy.sample_batch import SampleBatch
from ray.rllib.algorithms.algorithm_config import AlgorithmConfig

def collect_experience(num_env_runners=4, num_steps=1000, max_deque_size=10000):
    """
    使用RLlib的EnvRunnerGroup收集CartPole-v1环境的经验数据

    参数:
        num_env_runners: 并发的环境运行器数量
        num_steps: 总共要收集的步骤数
        max_deque_size: 经验回放队列的最大大小
    """
    # 初始化经验回放队列
    experience_deque = deque(maxlen=max_deque_size)

    # 创建算法配置 - 使用新API并指定PyTorch框架
    config = (
        AlgorithmConfig()
        .environment("CartPole-v1")
        .framework("torch")  # 使用PyTorch
        .env_runners(num_env_runners=num_env_runners)  # 使用新API
    )

    # 初始化EnvRunnerGroup
    env_runner_group = EnvRunnerGroup(
        env_creator=lambda env_config: config.env_class(config.env_config),
        default_policy_class=None,  # 使用默认随机策略
        config=config,
        num_env_runners=num_env_runners,
        logdir=None,
    )

    # 初始化过滤器管理器
    filter_manager = FilterManager()

    # 重置所有环境运行器
    env_runner_group.foreach_env_runner(
        lambda w: w.try_reset("default_policy")
    )

    # 开始收集经验
    collected_steps = 0
    while collected_steps < num_steps:
        # 获取所有环境运行器的样本
        sample_batches = env_runner_group.foreach_env_runner(
            lambda w: w.sample()
        )

        # 处理收集到的样本
        for batch in sample_batches:
            # 将SampleBatch转换为字典形式以便处理
            batch_dict = {
                SampleBatch.OBS: batch[SampleBatch.OBS],
                SampleBatch.ACTIONS: batch[SampleBatch.ACTIONS],
                SampleBatch.REWARDS: batch[SampleBatch.REWARDS],
                SampleBatch.NEXT_OBS: batch[SampleBatch.NEXT_OBS],
                SampleBatch.DONES: batch[SampleBatch.DONES],
            }

            # 将经验添加到队列中
            for i in range(len(batch[SampleBatch.OBS])):
                experience = {
                    "obs": batch_dict[SampleBatch.OBS][i],
                    "action": batch_dict[SampleBatch.ACTIONS][i],
                    "reward": batch_dict[SampleBatch.REWARDS][i],
                    "next_obs": batch_dict[SampleBatch.NEXT_OBS][i],
                    "done": batch_dict[SampleBatch.DONES][i],
                }
                experience_deque.append(experience)
                collected_steps += 1

                if collected_steps >= num_steps:
                    break
            if collected_steps >= num_steps:
                break

    # 关闭环境运行器
    env_runner_group.shutdown()

    return experience_deque

# 使用示例
if __name__ == "__main__":
    experience_data = collect_experience(num_env_runners=4, num_steps=1000)
    print(f"收集到的经验数量: {len(experience_data)}")
    print("第一条经验示例:")
    print(experience_data[0])

```

