
# RPC和路由管理 #
RPC的概念其实出现已经很久了，记得笔者读大学的时候，接触到RPC的概念，总觉得不重要，多此一举：

1、 我掌握好socket通信这个利器和tcp/ip协议族原理，什么功能不能实现？

2、 RPC就跟本地函数调用一样写代码，确实开发效率比较高；我自己把socket相关函数好好封装一下，让代码复用起来，开发效率也很高。

3、 不懂或者不关注网络通信底层原理，光会函数调来调去，这样的程序员太没有出息了！

后来，笔者开始带团队，亲身经历了一些团队协作和IT服务运营过程中的故事，才发现RPC非常关键。这里分享我经历过的很早以前的两个故事。

故事一：有一个基础模块A，被非常多的其他模块远程调用，模块A的门户提供协议文档、API、调用示例代码，每当有人来申请使用，模块A负责人就会给调用方一组接口机的IP，调用方可以给这些IP发网络请求。

重视可用性的有追求的调用方，通常在拿到IP后，会把IP写在配置文件里，并且自己在代码里实现一定的容错逻辑：如果某个IP请求连续失败多少次，就一段时间内不要给它发请求了。这个容错逻辑做好可不简单，涉及到很多细节。

大多数的调用方，是把IP写死在代码里，简单的轮询请求这些IP。

如果模块A的某台接口机死机了，或者网络局部故障导致某些接口机不可达，很多调用方就会跳起来：你们怎么回事？你们的服务水平怎么这么差！

如果机房裁撤，一些机器IP要下架，模块A负责任会非常头疼：

1、  首先不知道有哪些人在请求这个IP。读者说：傻啊，抓包看一下不就知道有哪些调用方了？但是要知道有的请求不是持续的，是不定期的访问一下模块A。

2、  模块A的负责人要大范围的邮件通知调用方改路由（通常要改代码编译发布），过一段时间后，抓包看还有哪些调用方没有改，再挨个敦促修改路由

3、  有时候某个IP下架了，过了几天，突然有个调用方跳起来：我们还在用呢！我们是写死IP的，代码找不到了，只能拿二进制可执行文件“硬”改

故事二： 一个团队里，通常有很多技术能力、服务意识和责任心都非常强的同事，他们的工作产出质量非常高，每个远程调用都有次数和成功率的上报（简单的说就是上报到一个监控系统，可以通过监控系统web界面查看曲线图），请求报文中的一些重要但不强校验的字段也都认真填写（例如染色标记），所以他们负责的模块，如果出现异常，很容易通过监控系统和日志监控到，并能快速定位到问题。

但是，也有一些同事责任心和能力不那么突出，重要的监控上报缺失、请求包里一些重要的字段没有填写。有时候服务的故障有异常了很久，被用户投诉才发现，事故报告里总是会出现这样的改进措施：增加对xxx的监控上报，增强服务运营意识。 

类似的事故通常会反复出现，管理干部就会拉起一次运动式的梳理和整顿，但过一段时间，肯还会出现。

通过这两个事故可见：如果没有很好的实现RPC和路由管理，IT系统服务质量会过度的依赖人的意识，而这个通常成本非常高、效果也不好。


#服务标准化#
一套互联网后台服务的开发和运营涉及到非常多的细节：

1、 访问其他服务模块，服务端IP如何管理？网络报文格式是怎样的？

2、 有哪些配置文件？ 用到哪些第三方的库？

3、 业务逻辑和基础框架是如何分离的？

4、 对外提供怎样的网络接口？怎么对外提供接口API和文档？

5、 运营机器上的安装目录准备怎么安排？  有哪些运维脚本和工具？

6、 应该监控哪些指标？应该记录哪些日志？

7、 还有很多…

上面种种细节，每个程序员实现起来都有不同的做法。经验证明，如果后台各个模块没有标准化和规范化，可能导致：

1、 同一个团队开发的服务，千差万别千奇百怪，负责运维的同事面对的多个模块“长”的都不一样，程序框架完全不一样，安装目录乱七八糟，无法规模化的高效运维

2、 服务的质量完全依赖团队成员的技能和意识，有的成员可能会做得比较好，配置文件命名易懂、文档及时更新与代码保持一致、有对服务做细致的监控上报和日志记录，提供了运维脚本，但是也有的成员的工作让人抓狂

3、 每当有团队成员离职和工作交接，交接本身就是工作量很大，交接时间长，交接质量不好，文档缺失，很多信息在交接过程中丢失，运营事故往往频发

4、 经验难以得到传承，一块石头反复绊倒各个成员和业务模块，运营事故雷同、频出，团队挫折感倍增、服务可用性低下

5、 也曾经有过做事比较规范的时候，但是这些规范通常靠耳提面命、人口相传，靠管理者运动式的整顿，有时候管理焦点没有持续跟进，或者随着人员更替，团队又把这些宝贵的经验丢弃了，变得无序

所以服务标准化是后台技术团队组建开始的第一要务。

## 2、几个误区 ##

误区一：找几个开源的组件用起来就好了呗

通常的开源的组件，只是在某一方面上规范了服务，有的是规范了网络调用，有的是规范了如何监控，有的是规范了如何记录远程记录，其实这还远远不够，例如配置文件、接口定义、使用到的外部库、安装目录的结构等非常多的细节，必须统一管理、有唯一出处。

误区二：你说的我都懂，我们团队刚起步，业务需求多，时间紧，先野蛮生长，打破条条框框，后面再规范再改

一开始没有标准化，后面当代码和模块都多起来了，且都处于运营状态，再改再标准化，难度非常大，成本非常大，风险非常大；另外工欲善其事必先利其器，一开始就标准化好，其实可以让业务跑的更快

# 如何应对crash #

## 1 crash的危害 ##
crash是c/c++开发者面临的比较头疼的问题，从纯粹开发的角度来看，crash通常比较难以规避，且定位困难；从后台服务的角度，crash甚至可能导致整个服务不可用。

笔者曾经遇到过至少三次比较大的事故。

事故一：A模块调用B模块，B模块进行了代码升级，给到A模块的回包某些情况下某个字段不合法，且A模块没有做好防御性编程，A模块几百台机器上的进程全部crash了，当时好在两个模块的负责人是在同一个部门且有正确的事故流程指导，及时回滚了B的发布，影响时间还不是很长。

事故二：A模块调用B模块，A模块进行了代码升级，发送给B模块的包某个字段不合法，B模块没有做好防御性编程，导致B模块几十台机器上的进程全部crash。情况很特殊：B模块是全公司范围使用的公共服务，前端有成千上万的调用方，A模块只是其中一个，且A模块在另外一个部门，他们的发布B模块根本不知情啊。 几乎所有的调用方都收到了影响，定位和修复搞了好久，压力巨大。

其实我没有说的更狗血的信息是：A模块当时其实还没有正式发布，只是一个勤奋的小伙半夜在测试环境调试A模块的新代码而已….

事故三：A模块调用B模块，A模块没有发布，B模块也没有发布，某天突然B模块crash了，原因就是A模块的某个分支这天终于走到了，偶尔发几个非法包给到B

有同学可能很快想到几个改进措施啦：

1、  加强防御性编程，并且使用protocolbuffer等比较成熟的IDL工具

2、  代码要充分测试，且不允许用公共服务的正式环境进行测试

3、  进程监控和快速拉起

4、  加强部门间信息的同步

上述改进措施笔者是赞同的，但是想说的是，大家也知道：加强总不够强，优化总不够优，很难落实的。

反正几年下来，我对crash的感触是：防不胜防

## 2 如何对付crash ##

### 2.1 选择自由度低更安全的语言 ###

如果其他因素允许的前提下，选择程序员自由度更低更安全的语言，例如java

### 2.2提升研发质量 ###
加强防御性编程，使用protocolbuffer等IDL工具尽量减少人工编码，充分测试，不允许在正式环境上进行测试。这些当我没说。

可能有人会误以为使用pb这样的工具生成接口代码，应该就可以比较彻底的防御非法字段带来的crash问题。其实不然。想象一下int型的年龄字段，pb确实能够保证打包拆包的正确性，但是模块在拆包后获得年龄后，在业务逻辑处理过程中，假设假定了范围是0-100，又没有做边界检查，可能就会导致异常和crash。也就是说拆解包没有问题，但后续的业务逻辑处理过程crash了。

### 2.3进程监控\秒起和共享内存 ###

对进程进行监控，一旦发现不在了，立即拉起。其实这里也好多坑，不该拉起的时候拉起。

还有就是在代码中使用秒起组件，crash的时候会捕获信号，重新拉起进程本身。

将数据保存在共享内存里，进程crash重新拉起后，去除重新load数据的时间，可以更快的重新进入正常服务状态。

### 2.4轻重分离，隔离故障 ###
         
对于公共服务模块，前端有大量的调用者，每个调用者都频繁的发布和并更，且代码质量参差不齐。那这个公共服务模块就有必要多部署几个set，每个set对应一组不同的调用者，重要的调用者专用某些公共服务的set。



# 理想的互联网服务后台框架的九个要点 #
对于互联网服务后台团队，开发框架的选择是非常关键的一个问题，多年的海量服务经验和教训使得我们团队深刻的认识到：

1. 要尽早规范团队的开发服务框架，避免到了后期，各种开发语言混杂、各类存储组件充斥、重复编码、每个模块形态不统一、文档缺失、监控瘫痪、人员离职造成大量信息丢失，最后积重难返、痛苦不堪。
2. 没有框架来规范，团队的随意性就太大，合作效率就大打折扣，甚至于内耗、反复的挖坑填坑，系统的成败过于依靠人的意识和水平。
3. 规范，不能靠文档、不能靠劳动纪律、不能靠苦口婆心、不能靠人员意识、不能靠运动式的整顿，要靠技术框架上切实的限制与贴心保护。


如果有机会从0开始定义一个理想的开发框架，需要考虑哪些点？我们觉得主要有如下9个方面：

【同步编码异步执行】兼顾运行效率和编码效率，希望代码写起来是同步和顺序的，而执行的时候是异步的

【IDL/RPC】支持IDL（接口描述语言）和RPC，减少网络协议相关的重复工作，协议有比较好的扩展性；远程调用友好且高效，做到覆盖主要的开发语言

【LB】对服务间的调用选路进行统一的管理，对单机故障和网络波动等常见情况有自动容错，我们简称load balance(LB)

【  存储服务化】这个其实和开发框架关系不太紧密，这里提一下，强调存储应该有统一的组件且由专业的团队运维，就像共有云一样

【过载保护】框架必须有成熟自带的过载保护机制，不需要业务开发人员关注或者关注很少

【基础的监控和告警】RPC调用、机器的cpu/网络活动、任务并发度、时延、进程监控和秒起等基础信息，要有上报、统计和告警，不需要业务开发人员关注。

【完整的业务流转呈现】统一日志，在一个地方能够清晰的呈现某次业务处理过程的流转详细情况：经过了哪些模块间调用，调用参数是怎样的，每个模块处理的重要分支和结果是怎样的，最好图形化呈现。支持染色和不同的日志详细级别

【中央总控】整个系统的配置和文档等重要信息，例如每个模块有哪些机器，分布在哪些机房、容量冗余情况、模块间调用关系、访问控制的配置动态管理甚至电子流，都希望能统一在一个地方web化的管理起来，并且与运营的系统是直接联系直接生效的

【云调度】容量的自动调度。例如要进行某个运营活动需要大量的扩容，只需要把设备放进去，就能自动的扩缩容。当某个城市机房故障，能够自动调度容量到其他城市