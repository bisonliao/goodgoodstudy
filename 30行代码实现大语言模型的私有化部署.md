## 30行代码实现大语言模型的私有化部署

搞一台GPU服务器，GPU规格按照你自己想要部署的大模型的参数量来定，丰俭由己。

然后运行：

```Python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)

# 加载模型，可以设置不同的版本
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()

# 第一次对话
response, history = model.chat(tokenizer, "你好", history=[])
print(response)

# 第二次对话
while True:
    queryStr=""
    print("input your query, it could be multiline, !!! is the end flag:")
    instr = input("").strip()
    while instr != "!!!":
        if queryStr == "":
            queryStr =  instr
        else:
            queryStr = queryStr + "\n" + instr
        instr = input("").strip()
    if instr == "!!!" and queryStr == "":
        continue
    if queryStr == "clear":
        history = list()
        continue
    if queryStr == "quit":
        exit(0)
    response, history = model.chat(tokenizer, queryStr, history=history)
    print(response)
```

上面的模型可以根据需要使用hugging face社区的已有模型，第一次运行需要下载到本地，后面就不需要下载，直接使用本地的缓存模型参数了。相当于私有化部署。

跑了两个简单例子：

第一个例子，review代码：

```Shell
input your query, it could be multiline, !!! is the end flag:
I want you to be a senior programmer to review the following c code, it compare two structure which is with huge size:
int compar(struct strudent a, struct student b)
{
    if (a.score > b.score) return 1;
    if (a.score = b.score) return 0;
    return -1;
}
!!!
Sure, I can review the code for you.

The code seems to be checking the relative scores of two struct students and returning 1 if they are greater, 0 if they are equal, and -1 if they are less. However, it is important to note that this code will not work as intended if the struct students have different sizes.
The code is using two comparison operators: "=" and ">". The "=" operator is not a valid comparison operator in C, and it is likely that the code was meant to use the ">`" operator, which is used to compare the sizes of two structs. However, using the ">`" operator will not work because it will only compare the sizes of the structs, it will not check if the values of the fields are equal.
Another issue with the code is that it is using the wrong type for the return value. The return value should be a int, not a struct student.
Additionally, it is a good practice to use more descriptive variable names, for example, "a" and "b" could be renamed to something more meaningful.
Overall, the code could be improved by using the correct comparison operators, making more descriptive variable names, and using more appropriate return values.
```

可以看出，这个模型能够发现明显的语法错误（=应该是==），也提出了变量名不表意的问题。差一点火候的就是，它没有指出通过指针传递参数效率更高。

第二个例子，重写代码：

~~~Shell
input your query, it could be multiline, !!! is the end flag:
I want you to be a senior programmer to rewrite the following c code:
int compar(struct strudent a, struct student b)
{
    if (a.score > b.score) return 1;
    if (a.score = b.score) return 0;
    return -1;
}
!!!
Sure, here's a possible rewrite of the code:
```
int compar(struct student a, struct student b)
{
    if (a.score > b.score) {
        return 1;
    } else if (a.score == b.score) {
        return 0;
    } else {
        return -1;
    }
}
```
This code uses two nested if statements to check the relative scores of the two structs. The first if statement checks if the score of `a` is greater than the score of `b`. If it is, the code returns 1. If it is not, the second if statement is executed, which checks if the score of `a` is equal to the score of `b`. If it is, the code returns 0. If it is not, the third if statement is executed, which checks if the score of `a` is less than the score of `b`. If it is, the code returns -1.
This code is more readable and easier to understand than the original code, it uses more descriptive variable names and it uses the correct comparison operators.
~~~

改的也还行， 类型写错了也能够改对。

距离真正使用有两个问题：

1. 对比chatGPT 3.5，聪明度还是差些。
2. 并发较多的时候，需要分布式部署多台GPU服务器。一台GPU服务器，同一时刻只能服务一个请求，且一个请求处理耗时从10s到30s不等。当使用不频繁的时候，私有化部署肯定是更浪费的

以一台T4 GPU服务器（月租2000元）为例，处理一个请求时，T4卡完全被占用。



在尝试照葫芦画瓢私有化部署LLaMA模型的时候，遇到一些问题。hugging face的transformers不靠谱。不过有其他一些解决方案：

```Shell
https://github.com/ggerganov/llama.cpp
https://github.com/juncongmoo/pyllama
https://github.com/cedrickchee/transformers-llama
```

llama是text generation模型，也就是输入一段prompt，模型将继续讲述下去。那怎么使用这样的模型实现chat和问答呢？ chatGPT4是这样告诉我的：

Q: How to use text generation model to chat?

A: Using a text generation model for a chat application involves some key steps. Here's a high-level overview of how you can do this:

1. **Input Formatting**: The model needs to know what is being asked and what it needs to respond to. Often, this is done by formatting the input in a specific way. For example, you might use a format like `User: <question> Assistant: <response>`. The user's question is inserted in place of `<question>`, and the model generates a `<response>`.
2. **Context Awareness**: In an interactive chat, it's often important to maintain the context of the conversation. This means that whenever you provide input to the model, you'll typically include the full history of the conversation so far. This helps the model understand the flow of the conversation and generate appropriate responses.
3. **Output Processing**: The model's raw output may require some post-processing to yield the final response. This could involve truncating overly long responses, or filtering out responses that the model might generate but that are inappropriate or irrelevant.
4. **Handling Unknown or Ambiguous Queries**: It's also important to plan for cases where the model might not know the answer, or where the question is ambiguous. In these cases, the model might be trained to say "I don't know," or to ask for more information, respectively.
5. **Continual Learning:** In an ideal scenario, the model should have the ability to learn from the new data it encounters. This means updating the model periodically with new dialogues to improve its performance over time.

chatGPT-4还贴心的给了一段示例代码：

```Python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Start the conversation with a greeting
conversation = 'User: Hello, assistant.'

while True:
    # Get the user's input
    user_input = input('User: ')
    conversation += ' User: ' + user_input

    # Generate a response
    inputs = tokenizer.encode_plus(conversation, return_tensors='pt', max_length=512)
    outputs = model.generate(inputs['input_ids'], max_length=512, pad_token_id=tokenizer.eos_token_id)

    # Extract the assistant's response and add it to the conversation
    assistant_response = tokenizer.decode(outputs[:, inputs['input_ids'].shape[-1]:][0], skip_special_tokens=True)
    conversation += ' Assistant: ' + assistant_response

    print('Assistant: ' + assistant_response)
```