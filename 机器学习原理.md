## 一、梯度下降法 ##

以一个线性回归问题为例:

N个实例组成的训练集(x1, x2， y)[N], 训练出一组参数a,b,c，使得输入x1,x2,  计算y'=a*x1 + b*x2 +c， y'与y误差尽量小。

定义损失函数为 J = 1/N * ∑(y-y')^2 ， 求使得损失函数最小的参数a,b,c， 可以使用梯度下降法：

1. 随机选取向量 **w(0)** = (w1, w2, w3)作为初始值
2. 根据梯度公式 **▽J(w)** = ( ∂J/∂w1, ∂J/∂w2, ∂J/∂w2) ，代入当前的(w1, w2, w3)值，即获得当前位置的梯度
3. 沿着梯度下降的方向前进一小步，计算得到新的**w**。即 **w(n+1)** = **w(n)** - step * **▽J(w)**
4. 如此反复，直到收敛。

下面用mathematica代码演示：

先构建一组训练集：

	ClearAll["Global`*"];
	a = 1;
	b = 2;
	c = 3;
	NUM = 20;
	y[x1_, x2_] = a*x1 + b*x2 + c;
	x1arr = Table[RandomReal[{-5, 5}], {i, 1, NUM}];
	x2arr = Table[RandomReal[{-10, 10}], {i, 1, NUM}];
	trainset = Table[0, {i, 1, NUM}];
	labels = Table[0, {i, 1, NUM}];
	For[i = 1, i <= NUM, i++,
	  labels[[i]] = y[x1arr[[i]], x2arr[[i]]] + RandomReal[{-0.1, 0.1}];
	  trainset[[i]] = {x1arr[[i]], x2arr[[i]], labels[[i]] }
	  
	  ];

将参数都初始化为1，然后定义损失函数，并求梯度公式、代入参数计算当前的梯度值

	w1 = 1;
	w2 = 1;
	w3 = 1;
	stepsz = 0.001;
	loss = Sum[(s1*x1arr[[i]] + s2*x2arr[[i]] + s3 - labels[[i]])^2 / 
	    NUM, {i, 1, NUM}];
	Do[
	  g1 = D[loss, s1] /. {s1 -> w1, s2 -> w2, s3 -> w3};
	  g2 = D[loss, s2] /. {s1 -> w1, s2 -> w2, s3 -> w3};
	  g3 = D[loss, s3] /. {s1 -> w1, s2 -> w2, s3 -> w3};
	  
	  w1 = w1 - g1*stepsz;
	  w2 = w2 - g2*stepsz;
	  w3 = w3 - g3*stepsz
	  ,
	  {k, 1, 2000}];
	Print[{w1, w2, w3}];

得到正确的输出为：

	{1.01699,2.00054,2.90552}



