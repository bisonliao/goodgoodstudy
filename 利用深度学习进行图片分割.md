# 利用深度学习进行图片语义分割

## 1、问题提出

图片语义分割，就是自动分割并识别出图片中的各部分内容。类似下面图中识别出人体、动物、球拍等。

![这里有张图片](img/FCN/fcn.png)

用到的技术之一是全卷积网络（FCN），顾名思义，就是这个网络是全部由卷积层组成的。

FCN是CNN演变而来的，CNN最典型的一个用法是用于图片分类，最后的几层通常是全连接层和SoftMax层，将图片进过卷积层提取到的特征经过计算后，转化为N X 1的向量，向量的每个元素对应一个分类的概率。类似下面图中的上面部分最后对1000个分类进行概率判断：

![这里有张图片](img/FCN/fcn2.jpg)

FCN通过deconvolution层上采样，相当于对输入图片的每个像素都进行分类，也就是输入一张 Ch X W X H的图片，输出一个 N X W X H的结果， N是分类总个数，像素位置一一对应。

deconvolution可以理解为反向的卷积层，下游输出BLOB的尺寸的推导，与卷积层刚好是反过来的。例如下图：

![这里有张图片](img/FCN/fcn3.jpg)

来自上层的输入尺寸 7 X 7， 卷积核大小是4，步进大小是2，那么输出尺寸是多少呢？  反过来推导：输入尺寸是多少，经过一个核大小为4步进大小是2的卷积核，才能输出7 X 7 呢？ 很容易计算得到是16 X 16。

FCN的

[详细的论文](https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html)

[官方github代码](https://github.com/shelhamer/fcn.berkeleyvision.org)

## 2、试验

### 2.1 试验数据的获取和整理

我使用的是coco数据集中的train2014数据包和对应的标注，一开始简单点：只做二分类 ：区分前景和背景，把coco数据集中的segmentation标注的多边形区域都认为是前景，其他区域认为是背景。生成的训练集的每个像素的label为0/1二分类。

[coco数据集](http://cocodataset.org/)

先用python脚本把标注信息整理成一个文本文件，每行的格式为

```
文件名 分割信息的数组
```

然后用下面的代码处理成HDF5文件或者是单独的图片，该程序主要用到了jsoncpp  opencv hdf5等库。

[生成训练集的代码](code/fcn/ConvertRegionToClass.cpp)

摘要一些片段如下：

```c
int segm_num = root.size();// how many segm in this image
for (int i = 0;  i < segm_num; ++i)
{
	int partnum = root[i].size(); // how many part in this segm

	for (int j = 0; j < partnum; ++j)
	{
		int coornum = root[i][j].size();//how many coordinary in this part
		static Point points[1][1024];
		int k;
		for (k = 0; k < coornum && k < 2048; k+=2)
		{
			int x = root[i][j][k].asInt();
			int y = root[i][j][k+1].asInt();
			points[0][k / 2] = Point(x, y);
		}
		
		const Point* pts[] = { points[0] };
		int npts[1];
		npts[0] = k / 2;
		//polylines(img, pts, npts, 1, true, Scalar(255), 3);
		fillPoly(dest, pts, npts, 1, Scalar(255));
		//polylines(dest, pts, npts, 1, true, Scalar(255), 3);
	}

}
```
下面是其中一个样本，左边是输入的图片，右边是标注的分类，实际输入到网络里的分类值是0和1，不是0和255。

![这里有张图片](img/FCN/input.jpg)

### 2.2 使用官方的pycaffe接口的方式进行训练

fcn官方github上代码是使用caffe的python接口写代码来训练的方式，而不是我常见的caffe.exe train命令行的方式。

官方的代码和文档几乎没有什么整理，以至于csdn上大量的博客写怎么怎么搞定这件事情的，这些作者通常会悲催的弄上两三个星期。

[这篇博客记录得很详细](https://blog.csdn.net/jiongnima/article/details/78549326)

下载了github上的zip包，我照着博客指引跑通了，细节不赘述了，记录几个要点：

1. 用了python类型的数据层来输入数据和标注，voc_layers.py、siftflow_layers.py、pascalcontext_layers.py分别应对不同的开源数据集。这些python层通常有两个类，分别处理输入数据和输入标签
2. voc_xxx、siftflow_xxx、pascalcontent_xxx等子目录，是根据开源数据集和上采样的倍数区分的，大同小异，都是做语义分割，进入其中一个子目录开始折腾就好。有耐心的解决solver.py执行中遇到的问题即可
3. fcn训练是很耗资源的，我的小机器从0开始训练基本上没有什么进展，所以要基于一定的基础做finetune，子目录下的caffemodel-url文件给出了一个现成的用来finetune的模型，比较大，要有耐心下载
4. 把infer.py  score.py等这些被引用的python代码都拷贝到你想要折腾的那个子目录，把训练集目录对应的路径相关的代码有耐心的改改
5. 需要反复尝试不同的base_lr值，才能找到一个合适的准确率逐步提升的学习率。

我用了三万张图片来训练，跑了10来个小时后，loss从4下降到1左右不收敛了，测试的准确率为80%。 对于一个二分类问题来说，这个准确率太低，比瞎蒙好不了多少。mean accuracy是什么鬼？

![这里有张图片](img/FCN/fcn4.jpg)

### 2.3 使用我熟悉的命令行方式训练

其实我是先用命令行方式来训练的，只是后来不收敛了，就开始怀疑人生，然后找到了上面提到的csdn上的那篇博客，又尝试用python方式。

博客中提到的什么三次插值初始化是收敛的关键点，我有点怀疑。因为其实大家都是在同一个现有模型的基础上finetune，那某一层的额外的初始化还重要吗？

我用的是HDF5文件作为数据输入类型，其他预置的数据输入类型应该不支持该场景的输入与标签表示需要。

用到的prototxt文件在下面目录

[存放网络定义文件的目录](code/myfcn)

[finetune的基石模型](http://dl.caffe.berkeleyvision.org/fcn8s-heavy-pascal.caffemodel)

跑了5个小时，准确率从68%提升到77%，中间准确率到过79%：

![这里有张图片](img/FCN/fcn5.jpg)

这里有个曲折的过程，我一开始习惯性的输入灰度图片，所以对于第一层卷积核的尺寸是1X3X3，而finetune的caffemodel文件里的第一层卷积层的核的尺寸是三通道的，即3 X 3 X 3，尺寸与我的train.prototxt里定义的卷积层的核尺寸不符，加载会报错，所以我改了该卷积层的名字，没有加载finetune的caffemodel文件里的参数作为初始值，且在我的train.prototxt文件里一开始没有使用weight_filler / bias_fillter明确指定初始化方法，caffe对卷积层的参数默认初始化为0，自然这一层参数就无法梯度优化，一直是0。

我尝试改进：

1. 先对第一层卷积层使用xavier方式的参数初始化，尝试训练，准确率提升很有限
2. 后来我就改为输入三通道彩色图片，准确率从68%提升到77%

另外，base_lr的设置也影响很大，需要反复尝试，设置的比较大（1e-6）的时候，loss值下降很明显，但测试到的准确率不怎么变化，应该是过拟合了；设置的稍微小一点，虽然loss值下降不快，但测试到的准确率有比较稳步的上升。

在网上找了些python代码改改，做成一个查看caffemodel文件的工具，发现了上述问题。这里也一并把这个工具给出：

[查看caffemodel的脚本工具](code/myfcn/showCaffeModel.py)

```python
import caffe
import struct
import numpy as np
import sys
import os

np.set_printoptions(threshold='nan')

MODEL_FILE = sys.argv[2]
PRETRAIN_FILE = sys.argv[1]

params_txt = 'params.txt'
pf_txt = open(params_txt, 'w')

net = caffe.Net(MODEL_FILE, PRETRAIN_FILE, caffe.TEST)

for param_name in net.params.keys():
    weight = net.params[param_name][0].data
                     
    pf_dat = open('./weight/'+param_name+'.txt', 'w')
    len_w=len(weight.shape)
    if (len_w==4):##conv layer
        byte1=struct.pack('i',weight.shape[3])
        byte3=struct.pack('i',weight.shape[1])
        byte4=struct.pack('i',weight.shape[0])
    elif(len_w==2):##fc layer
        byte1=struct.pack('i',weight.shape[1])
        byte2=struct.pack('i',weight.shape[0])
         
    pf_txt.write(param_name)
    pf_txt.write('\n')
    pf_txt.write('shape:(')
    pf_dat.write('shape:(')
    for dim in weight.shape[:]:
        pf_txt.write(str(dim))
        pf_dat.write(str(dim))
        pf_txt.write(" ")
        pf_dat.write(" ")
    pf_txt.write(')\n')
    pf_dat.write(')\n')
     
    pf_dat.write('\n' + param_name + '_weight:\n\n')
    
    weight.shape = (-1, 1)
        
    for w in weight:
        pf_dat.write('%f, ' % w)
     
    if len(net.params[param_name]) < 2:
        pf_dat.write("\n\nlayer %s has NO bias!!"%param_name)
        pf_dat.close
        continue
    
    bias =  net.params[param_name][1].data
    pf_dat.write('\n\n' + param_name + '_bias:\n\n')
    bias.shape = (-1, 1)
    for b in bias:
        pf_dat.write('%f, ' % b)
    pf_dat.close

pf_txt.close
```

### 2.4 模型使用

准确率虽然不高，把训练好的模型还是拿出来测试几个例子看看吧。

关键的代码如下：

```c++
void classify(boost::shared_ptr<Net<float> > net, const Mat & img)
{
	float data_input[3][input_size][input_size];

	if (img.channels() != 3)
	{
		return;
	}
	Mat inputMat;
	resize(img, inputMat, Size(input_size, input_size), 0, 0, INTER_CUBIC);
	int width, height, chn;
	for (height = 0; height < input_size; ++height)
	{
		for (width = 0; width < input_size; ++width)
		{
			cv::Point3_<uchar>* p = inputMat.ptr<cv::Point3_<uchar> >(height, width);
			data_input[0][height][width] = p->x;//B
			data_input[1][height][width] = p->y;//G
			data_input[2][height][width] = p->z;//R
		}
	}

	Blob<float>* input_blobs = net->input_blobs()[0];
	printf("inpupt blob x count:%d\n", input_blobs->count());
	switch (Caffe::mode())
	{
	case Caffe::CPU:
		memcpy(input_blobs->mutable_cpu_data(), data_input,
			sizeof(float) * input_blobs->count());
		break;
	case Caffe::GPU:

		cudaMemcpy(input_blobs->mutable_gpu_data(), data_input,
			sizeof(float) * input_blobs->count(), cudaMemcpyHostToDevice);
		break;
	default:
		LOG(FATAL) << "Unknown Caffe mode.";
	}
	
	net->Forward();
	int index = get_blob_index(net, "score");
	boost::shared_ptr<Blob<float> > blob = net->blobs()[index];
	unsigned int num_data = blob->count();
	printf("output blob index:%d,  y count:%d\n", index, blob->count());

	int class_idx;
	Mat result(input_size, input_size, CV_8UC1, Scalar(0));
	const int CLASS_NUM = 2;
	const float *blob_ptr = (const float *)blob->cpu_data();
	for (height = 0; height < input_size; height++)
	{
		for (width = 0; width < input_size; width++)
		{
			int max = blob_ptr[0 * (input_size*input_size) + height * input_size + width];
			int max_idx = 0;
			for (class_idx = 1; class_idx < CLASS_NUM; ++class_idx)
			{
				int offset = class_idx * (input_size*input_size) + height * input_size + width;
				if (blob_ptr[offset] > max)
				{
					max = blob_ptr[offset];
					max_idx = class_idx;
				}
			}
			if (max_idx == 1)
			{
				result.at<uchar>(height, width) = 255;
			}

		}
	}
	namedWindow("Display window", WINDOW_AUTOSIZE);

	imshow("Display window", inputMat);                // Show our image inside it.
	waitKey(0); // Wait for a keystroke in the window

	imshow("Display window", result);                // Show our image inside it.
	waitKey(0); // Wait for a keystroke in the window
	
	return;
}
```

[详细的代码文件在这里](code/fcn/UseTrainedModel.cpp)

测试结果如下所示，并不令人满意：

![这里有张图片](img/FCN/result.jpg)

## 9、题外话

1. python真的很强大，同样是处理一个很大的json文件（来自coco数据集），我一开始用jsonc库（号称最好用的c语言下的json库），加载文件失败；改用java语言，提示虚拟机栈空间不够，调整虚拟机参数也没有解决。 python轻松搞定
2. jsoncpp/rapidjson/jsonc等好几个库，下载到windows下从源代码开始构建，步骤是
   1. 在库展开的目录下建一个build子目录
   2. cd到build子目录，执行cmake ..
   3. 然后就会看到build子目录里有个visualstudio解决方案，用vs2015打开它就能编译好
3. 但上面cmake这样搞出来的，默认是x86不是x64的，与x64的项目用不到一块。咋办？cmake的时候加一个选项即可： -G ' Visual Studio 14 win64'

